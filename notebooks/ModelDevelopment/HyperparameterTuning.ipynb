{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V6E1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "This notebook is a place for us to tune our the hyperparameters for both the lightgbm and catboost models for both the standard and complex name sets.\n",
    "\n",
    "We will tune each model on each data set and then do a final comparisson. So this will in part inform us of the best parameter choices and the best model choice."
   ],
   "metadata": {
    "id": "DjE0M1VsiF7a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install lightgbm\n",
    "!pip install sqlalchemy\n",
    "!pip install catboost\n",
    "!pip install optuna"
   ],
   "metadata": {
    "id": "yNrhVNxUiAcx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0a8a3b05-c37c-412b-bd9c-e8f872b7d775"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
      "Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Collecting sqlalchemy\n",
      "  Downloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy)\n",
      "  Downloading greenlet-3.2.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (4.14.1)\n",
      "Downloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.2.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (585 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.5/585.5 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.2.3 sqlalchemy-2.0.41\n",
      "Collecting catboost\n",
      "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
      "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\n",
      "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: graphviz, catboost\n",
      "Successfully installed catboost-1.2.8 graphviz-0.21\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
      "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, alembic, optuna\n",
      "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.4.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import gc\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import optuna\n",
    "from catboost import CatBoostRanker, Pool\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from sqlalchemy import select, func\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import ndcg_score"
   ],
   "metadata": {
    "id": "SugiQtxNPKd_"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "r-kXyYQfHGJk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "706de8f5-035b-4d72-ac58-c7319691d9d6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standard Nameset\n",
    "\n",
    "We will begin on the standard set."
   ],
   "metadata": {
    "id": "PV_O_OI67dcK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Paths to files in Drive\n",
    "db_path = \"/content/drive/MyDrive/Colab Notebooks/database.db\"\n",
    "train_ids_path = \"/content/train_std_ids.csv\"\n",
    "val_ids_path = \"/content/validation_std_ids.csv\""
   ],
   "metadata": {
    "id": "85sODZLeOz8Z"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pull whole standard dataset from SQL db into pandas frame."
   ],
   "metadata": {
    "id": "4ewxrbKKinbw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "conn = sqlite3.connect(db_path)  # or your local path\n",
    "full_df = pd.read_sql_query(\"SELECT * FROM feature_matrix\", conn)"
   ],
   "metadata": {
    "id": "4wNb5P0S4tEQ"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saved the training and validation IDs in a CSV so we can split the data set."
   ],
   "metadata": {
    "id": "Eh1_pW9kircr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_ids = pd.read_csv(train_ids_path)[\"train_ids\"].dropna().astype(int).tolist()\n",
    "val_ids = pd.read_csv(val_ids_path)[\"validation_ids\"].dropna().astype(int).tolist()"
   ],
   "metadata": {
    "id": "jiXxsfyUO6-s"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Validation dataset pulled from full set, remianing is training set. This is to ensure that we don't train on validation set."
   ],
   "metadata": {
    "id": "Ng6Bh82fizFD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "val_ids_set = set(val_ids)\n",
    "val_df = full_df[full_df[\"clean_row_id\"].isin(val_ids_set)]\n",
    "full_df = full_df[~full_df[\"clean_row_id\"].isin(val_ids_set)]"
   ],
   "metadata": {
    "id": "e-DWeeNh8gA3"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This was pulled from the initial model development notebook."
   ],
   "metadata": {
    "id": "8gT0AB51i6xZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_ranking_metrics(df, k=3):\n",
    "    # Group\n",
    "    grouped = df.groupby(\"clean_row_id\")\n",
    "\n",
    "    # Get top 1 and calculate accuracy\n",
    "    top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
    "    acc1 = (top1[\"label\"] == 1).mean()\n",
    "\n",
    "    # Same with recall\n",
    "    topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
    "    recall_k = topk.groupby(\"clean_row_id\")[\"label\"].max().mean()\n",
    "\n",
    "    # MMR\n",
    "    def reciprocal_rank(g):\n",
    "        sorted_g = g.sort_values(\"score\", ascending=False).reset_index()\n",
    "        match = sorted_g[sorted_g[\"label\"] == 1]\n",
    "        return 1.0 / (match.index[0] + 1) if not match.empty else 0.0\n",
    "\n",
    "    mrr = grouped.apply(reciprocal_rank).mean()\n",
    "    return acc1, recall_k, mrr"
   ],
   "metadata": {
    "id": "wHpoBtSfHSQT"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will define our train and val set here so we can reuse on LightGBM tuning."
   ],
   "metadata": {
    "id": "PSdfQOh750MU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Training set\n",
    "X_train = full_df.drop(\n",
    "    columns=[\"label\", \"clean_row_id\", \"investor\", \"firm\", \"template_id\"]\n",
    ")\n",
    "y_train = full_df[\"label\"]\n",
    "train_group_sizes = full_df.groupby(\"clean_row_id\").size().tolist()\n",
    "\n",
    "# Validation set\n",
    "X_val = val_df.drop(\n",
    "    columns=[\"label\", \"clean_row_id\", \"investor\", \"firm\", \"template_id\"]\n",
    ")\n",
    "y_val = val_df[\"label\"]\n",
    "val_group_sizes = val_df.groupby(\"clean_row_id\").size().tolist()"
   ],
   "metadata": {
    "id": "8tjiHScB5z4o"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a catboost objective function to tune our parameters. We want to do a full sweep on hyperparameters to make sure our model performs the best it can."
   ],
   "metadata": {
    "id": "jchAOpQrjIal"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def catboost_objective(trial):\n",
    "    params = {\n",
    "        \"loss_function\": \"YetiRank\",\n",
    "        \"eval_metric\": \"NDCG:top=3\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0, 10),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "        \"grow_policy\": trial.suggest_categorical(\n",
    "            \"grow_policy\", [\"SymmetricTree\", \"Lossguide\"]\n",
    "        ),\n",
    "        \"iterations\": 100,\n",
    "        \"early_stopping_rounds\": 5,\n",
    "        \"random_seed\": 42,\n",
    "        \"verbose\": False,\n",
    "        \"task_type\": \"CPU\",\n",
    "    }\n",
    "\n",
    "    # CatBoost pools\n",
    "    train_pool = Pool(\n",
    "        data=X_train,\n",
    "        label=y_train,\n",
    "        group_id=np.repeat(np.arange(len(train_group_sizes)), train_group_sizes),\n",
    "    )\n",
    "    val_pool = Pool(\n",
    "        data=X_val,\n",
    "        label=y_val,\n",
    "        group_id=np.repeat(np.arange(len(val_group_sizes)), val_group_sizes),\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    model = CatBoostRanker(**params)\n",
    "    model.fit(train_pool, eval_set=val_pool)\n",
    "\n",
    "    # Score\n",
    "    val_df_copy = val_df.copy()\n",
    "    val_df_copy[\"score\"] = model.predict(val_pool)\n",
    "    acc1, recall3, mrr = compute_ranking_metrics(val_df_copy, k=3)\n",
    "\n",
    "    return 1 - recall3  # minimize (maximize recall@3)"
   ],
   "metadata": {
    "id": "M5s6rQbbHPp1"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "And the same for LightGBM."
   ],
   "metadata": {
    "id": "-BQqHP5O6esV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def lightgbm_objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"lambdarank\",\n",
    "        \"metric\": \"ndcg\",\n",
    "        \"ndcg_eval_at\": [3],\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 128),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 100),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 5),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0, 5),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 5),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "\n",
    "    train_set = lgb.Dataset(X_train, label=y_train, group=train_group_sizes)\n",
    "    val_set = lgb.Dataset(X_val, label=y_val, group=val_group_sizes)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[val_set],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=30),\n",
    "            lgb.log_evaluation(period=0),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    val_df_copy = val_df.copy()\n",
    "    val_df_copy[\"score\"] = preds\n",
    "\n",
    "    acc1, recall3, mrr = compute_ranking_metrics(val_df_copy, k=3)\n",
    "    return 1 - recall3"
   ],
   "metadata": {
    "id": "diWTxNUM6ts_"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets start our sweep"
   ],
   "metadata": {
    "id": "ELxU5Qf16z8e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Run the sweep\n",
    "print(\"Running CatBoost tuning...\")\n",
    "catboost_study = optuna.create_study(direction=\"minimize\", study_name=\"catboost\")\n",
    "catboost_study.optimize(catboost_objective, n_trials=20)\n",
    "# Save to drive\n",
    "with open(\"/content/catboost_std_best.json\", \"w\") as f:\n",
    "    json.dump(catboost_study.best_params, f)"
   ],
   "metadata": {
    "id": "H2nuOB1MPUci",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3792ea81-524f-4977-8630-83fd9f161f7a"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-07-16 15:16:02,320] A new study created in memory with name: catboost\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running CatBoost tuning...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:16:51,027] Trial 0 finished with value: 0.006078809983551481 and parameters: {'learning_rate': 0.06275390956292835, 'depth': 7, 'l2_leaf_reg': 5.887087145752858, 'random_strength': 7.15393253729215, 'min_data_in_leaf': 74, 'subsample': 0.5917943183420644, 'colsample_bylevel': 0.533663660106312, 'grow_policy': 'SymmetricTree'}. Best is trial 0 with value: 0.006078809983551481.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:18:03,523] Trial 1 finished with value: 0.005649717514124242 and parameters: {'learning_rate': 0.02600340439667241, 'depth': 8, 'l2_leaf_reg': 2.8609555177546246, 'random_strength': 9.452171206512926, 'min_data_in_leaf': 92, 'subsample': 0.539739544044516, 'colsample_bylevel': 0.6061144761147864, 'grow_policy': 'SymmetricTree'}. Best is trial 1 with value: 0.005649717514124242.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:18:59,394] Trial 2 finished with value: 0.005721232925695485 and parameters: {'learning_rate': 0.02571035599479566, 'depth': 10, 'l2_leaf_reg': 8.854515795503294, 'random_strength': 1.544136025973809, 'min_data_in_leaf': 81, 'subsample': 0.5630889724275825, 'colsample_bylevel': 0.9314981062565846, 'grow_policy': 'SymmetricTree'}. Best is trial 1 with value: 0.005649717514124242.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:20:12,593] Trial 3 finished with value: 0.006007294571980237 and parameters: {'learning_rate': 0.07462694589941862, 'depth': 10, 'l2_leaf_reg': 2.154778514301853, 'random_strength': 0.27357947613068645, 'min_data_in_leaf': 89, 'subsample': 0.7368871639646191, 'colsample_bylevel': 0.8335719966581439, 'grow_policy': 'SymmetricTree'}. Best is trial 1 with value: 0.005649717514124242.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:22:51,028] Trial 4 finished with value: 0.003861832224844486 and parameters: {'learning_rate': 0.13275757957731918, 'depth': 6, 'l2_leaf_reg': 7.142519331365267, 'random_strength': 3.395785387976391, 'min_data_in_leaf': 84, 'subsample': 0.9048958560910838, 'colsample_bylevel': 0.511123337191838, 'grow_policy': 'Lossguide'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:23:38,582] Trial 5 finished with value: 0.006364871629836233 and parameters: {'learning_rate': 0.02232452833174663, 'depth': 6, 'l2_leaf_reg': 7.391680878401738, 'random_strength': 8.89726702594291, 'min_data_in_leaf': 20, 'subsample': 0.7348609568240634, 'colsample_bylevel': 0.6323902386092852, 'grow_policy': 'SymmetricTree'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:25:15,391] Trial 6 finished with value: 0.004934563398412362 and parameters: {'learning_rate': 0.18083804863392772, 'depth': 7, 'l2_leaf_reg': 5.043735267737906, 'random_strength': 9.772960611089946, 'min_data_in_leaf': 31, 'subsample': 0.7280152824858024, 'colsample_bylevel': 0.6894789129542873, 'grow_policy': 'SymmetricTree'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:27:09,448] Trial 7 finished with value: 0.004576986340556366 and parameters: {'learning_rate': 0.14495068228750044, 'depth': 7, 'l2_leaf_reg': 4.378424983194542, 'random_strength': 7.385437921919892, 'min_data_in_leaf': 38, 'subsample': 0.9514610613841548, 'colsample_bylevel': 0.7673187217912938, 'grow_policy': 'Lossguide'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:28:22,758] Trial 8 finished with value: 0.006150325395122613 and parameters: {'learning_rate': 0.11797990542262388, 'depth': 5, 'l2_leaf_reg': 5.51319601823983, 'random_strength': 5.7522445021081765, 'min_data_in_leaf': 82, 'subsample': 0.6428053403450817, 'colsample_bylevel': 0.8465483123784245, 'grow_policy': 'SymmetricTree'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:29:45,524] Trial 9 finished with value: 0.005435171279410733 and parameters: {'learning_rate': 0.06919363873436608, 'depth': 8, 'l2_leaf_reg': 2.3201607228994208, 'random_strength': 7.115601262579611, 'min_data_in_leaf': 63, 'subsample': 0.5784088451760294, 'colsample_bylevel': 0.9540117630362777, 'grow_policy': 'SymmetricTree'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:32:07,634] Trial 10 finished with value: 0.005721232925695485 and parameters: {'learning_rate': 0.18046209523596335, 'depth': 4, 'l2_leaf_reg': 9.906496618736249, 'random_strength': 3.1228639929830946, 'min_data_in_leaf': 50, 'subsample': 0.9552824587250898, 'colsample_bylevel': 0.5070964339326541, 'grow_policy': 'Lossguide'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:33:41,728] Trial 11 finished with value: 0.004576986340556366 and parameters: {'learning_rate': 0.13694630988988352, 'depth': 6, 'l2_leaf_reg': 4.086625849763388, 'random_strength': 3.976721826018532, 'min_data_in_leaf': 43, 'subsample': 0.9709791968377917, 'colsample_bylevel': 0.75574079252209, 'grow_policy': 'Lossguide'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:35:32,164] Trial 12 finished with value: 0.004076378459557994 and parameters: {'learning_rate': 0.15425345757205505, 'depth': 8, 'l2_leaf_reg': 7.2278897740052654, 'random_strength': 5.152651260067465, 'min_data_in_leaf': 63, 'subsample': 0.8735902834441678, 'colsample_bylevel': 0.7602468567957628, 'grow_policy': 'Lossguide'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:37:23,747] Trial 13 finished with value: 0.004147893871129238 and parameters: {'learning_rate': 0.1617225733016994, 'depth': 9, 'l2_leaf_reg': 7.413875307447629, 'random_strength': 4.374311946201069, 'min_data_in_leaf': 62, 'subsample': 0.8545920261255018, 'colsample_bylevel': 0.6603293646109619, 'grow_policy': 'Lossguide'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:39:39,276] Trial 14 finished with value: 0.005935779160409105 and parameters: {'learning_rate': 0.10694537716588079, 'depth': 5, 'l2_leaf_reg': 7.165487426356101, 'random_strength': 1.8661422032747355, 'min_data_in_leaf': 70, 'subsample': 0.8589874732022146, 'colsample_bylevel': 0.8427335369902766, 'grow_policy': 'Lossguide'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:41:16,622] Trial 15 finished with value: 0.004720017163698742 and parameters: {'learning_rate': 0.13358474711666768, 'depth': 8, 'l2_leaf_reg': 6.763218269571514, 'random_strength': 5.541799578407068, 'min_data_in_leaf': 99, 'subsample': 0.8576466407678314, 'colsample_bylevel': 0.568659037041511, 'grow_policy': 'Lossguide'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:43:19,364] Trial 16 finished with value: 0.004863047986841118 and parameters: {'learning_rate': 0.19447551701903043, 'depth': 6, 'l2_leaf_reg': 8.635274549011001, 'random_strength': 3.320124617307074, 'min_data_in_leaf': 56, 'subsample': 0.814393536028281, 'colsample_bylevel': 0.6916506646454088, 'grow_policy': 'Lossguide'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:45:33,487] Trial 17 finished with value: 0.004076378459557994 and parameters: {'learning_rate': 0.1628264348572751, 'depth': 9, 'l2_leaf_reg': 8.626987510907645, 'random_strength': 2.2680030962800135, 'min_data_in_leaf': 73, 'subsample': 0.9037724731392628, 'colsample_bylevel': 0.7683712603692795, 'grow_policy': 'Lossguide'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:48:03,097] Trial 18 finished with value: 0.006007294571980237 and parameters: {'learning_rate': 0.10234708216859703, 'depth': 4, 'l2_leaf_reg': 6.225435504792496, 'random_strength': 5.267527408458103, 'min_data_in_leaf': 15, 'subsample': 0.814591728799415, 'colsample_bylevel': 0.7194018367051533, 'grow_policy': 'Lossguide'}. Best is trial 4 with value: 0.003861832224844486.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:51:10,800] Trial 19 finished with value: 0.005721232925695485 and parameters: {'learning_rate': 0.09063553161497129, 'depth': 5, 'l2_leaf_reg': 8.313548228662597, 'random_strength': 6.379933158116214, 'min_data_in_leaf': 99, 'subsample': 0.9947249613522493, 'colsample_bylevel': 0.598098939925011, 'grow_policy': 'Lossguide'}. Best is trial 4 with value: 0.003861832224844486.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Running LightGBM tuning...\")\n",
    "lgbm_study = optuna.create_study(direction=\"minimize\", study_name=\"lightgbm\")\n",
    "lgbm_study.optimize(lightgbm_objective, n_trials=20)\n",
    "# Save to drive\n",
    "with open(\"/content/light_gbm_std_best.json\", \"w\") as f:\n",
    "    json.dump(lgbm_study.best_params, f)"
   ],
   "metadata": {
    "id": "XnBaBabV67kQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "66d840dd-30b7-4719-dd36-34e706f49fc4"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-07-16 15:51:10,804] A new study created in memory with name: lightgbm\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running LightGBM tuning...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@3: 0.960617\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:51:47,010] Trial 0 finished with value: 0.003575770578559734 and parameters: {'learning_rate': 0.1648331261312991, 'num_leaves': 22, 'min_data_in_leaf': 56, 'feature_fraction': 0.6220368308245395, 'bagging_fraction': 0.9675613645209333, 'lambda_l1': 1.88343488477213, 'lambda_l2': 4.302216020220545, 'bagging_freq': 4, 'max_depth': 8}. Best is trial 0 with value: 0.003575770578559734.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's ndcg@3: 0.942363\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:52:11,583] Trial 1 finished with value: 0.006364871629836233 and parameters: {'learning_rate': 0.06319651151842785, 'num_leaves': 91, 'min_data_in_leaf': 76, 'feature_fraction': 0.8801400758268227, 'bagging_fraction': 0.6902664883445727, 'lambda_l1': 0.8220476237823104, 'lambda_l2': 1.8966107727201336, 'bagging_freq': 3, 'max_depth': 5}. Best is trial 0 with value: 0.003575770578559734.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@3: 0.956018\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:52:45,931] Trial 2 finished with value: 0.003861832224844486 and parameters: {'learning_rate': 0.1628675204371491, 'num_leaves': 109, 'min_data_in_leaf': 54, 'feature_fraction': 0.808298131840267, 'bagging_fraction': 0.6816736027621615, 'lambda_l1': 2.719538148099195, 'lambda_l2': 0.008274120466558177, 'bagging_freq': 2, 'max_depth': 4}. Best is trial 0 with value: 0.003575770578559734.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@3: 0.960953\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:53:20,899] Trial 3 finished with value: 0.0035042551669884903 and parameters: {'learning_rate': 0.11780394667603233, 'num_leaves': 44, 'min_data_in_leaf': 28, 'feature_fraction': 0.5694367865340455, 'bagging_fraction': 0.8157898821760543, 'lambda_l1': 3.387242537447426, 'lambda_l2': 2.196643362517005, 'bagging_freq': 4, 'max_depth': 8}. Best is trial 3 with value: 0.0035042551669884903.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@3: 0.961518\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:53:50,949] Trial 4 finished with value: 0.0031466781091324947 and parameters: {'learning_rate': 0.06999205716538405, 'num_leaves': 72, 'min_data_in_leaf': 22, 'feature_fraction': 0.9357340112477351, 'bagging_fraction': 0.9800252910545488, 'lambda_l1': 2.8648804109739916, 'lambda_l2': 3.698302104252323, 'bagging_freq': 1, 'max_depth': 11}. Best is trial 4 with value: 0.0031466781091324947.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@3: 0.963902\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:54:32,836] Trial 5 finished with value: 0.002932131874418986 and parameters: {'learning_rate': 0.14738398322068144, 'num_leaves': 121, 'min_data_in_leaf': 64, 'feature_fraction': 0.6433135050247664, 'bagging_fraction': 0.8514804878194124, 'lambda_l1': 1.507404135359871, 'lambda_l2': 0.9794529738376201, 'bagging_freq': 2, 'max_depth': 9}. Best is trial 5 with value: 0.002932131874418986.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@3: 0.952907\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:55:08,277] Trial 6 finished with value: 0.00421940928270037 and parameters: {'learning_rate': 0.11180475399040998, 'num_leaves': 43, 'min_data_in_leaf': 96, 'feature_fraction': 0.5253208524365685, 'bagging_fraction': 0.9078280071557396, 'lambda_l1': 3.1083686627264444, 'lambda_l2': 4.543261230556205, 'bagging_freq': 4, 'max_depth': 5}. Best is trial 5 with value: 0.002932131874418986.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@3: 0.964107\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:55:45,732] Trial 7 finished with value: 0.002789101051276499 and parameters: {'learning_rate': 0.19426829578921662, 'num_leaves': 64, 'min_data_in_leaf': 75, 'feature_fraction': 0.9047338201190456, 'bagging_fraction': 0.9624860112209651, 'lambda_l1': 0.9753561860776749, 'lambda_l2': 4.367377782782343, 'bagging_freq': 3, 'max_depth': 10}. Best is trial 7 with value: 0.002789101051276499.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[90]\tvalid_0's ndcg@3: 0.940059\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:56:18,090] Trial 8 finished with value: 0.00536365586783949 and parameters: {'learning_rate': 0.04694578401285204, 'num_leaves': 91, 'min_data_in_leaf': 87, 'feature_fraction': 0.9853894028995265, 'bagging_fraction': 0.8310542061183517, 'lambda_l1': 3.474502993210868, 'lambda_l2': 3.679038815230335, 'bagging_freq': 3, 'max_depth': 4}. Best is trial 7 with value: 0.002789101051276499.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@3: 0.963886\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:56:53,658] Trial 9 finished with value: 0.0028606164628477426 and parameters: {'learning_rate': 0.17133439934835623, 'num_leaves': 71, 'min_data_in_leaf': 63, 'feature_fraction': 0.8935746135401298, 'bagging_fraction': 0.8359065031942383, 'lambda_l1': 1.9979525322747542, 'lambda_l2': 3.0203094561358634, 'bagging_freq': 4, 'max_depth': 11}. Best is trial 7 with value: 0.002789101051276499.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[84]\tvalid_0's ndcg@3: 0.962762\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:57:27,590] Trial 10 finished with value: 0.003218193520703738 and parameters: {'learning_rate': 0.19992341367008631, 'num_leaves': 60, 'min_data_in_leaf': 41, 'feature_fraction': 0.7304021151823121, 'bagging_fraction': 0.5325650750056574, 'lambda_l1': 4.696823988195911, 'lambda_l2': 4.8135067623116585, 'bagging_freq': 5, 'max_depth': 12}. Best is trial 7 with value: 0.002789101051276499.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[87]\tvalid_0's ndcg@3: 0.964408\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:58:02,162] Trial 11 finished with value: 0.002932131874418986 and parameters: {'learning_rate': 0.18648665553874924, 'num_leaves': 73, 'min_data_in_leaf': 74, 'feature_fraction': 0.8456555767294333, 'bagging_fraction': 0.7620069958993859, 'lambda_l1': 0.3229943801895545, 'lambda_l2': 2.8260905958204057, 'bagging_freq': 5, 'max_depth': 10}. Best is trial 7 with value: 0.002789101051276499.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\tvalid_0's ndcg@3: 0.954851\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:58:37,399] Trial 12 finished with value: 0.00443395551741399 and parameters: {'learning_rate': 0.013865012169456875, 'num_leaves': 48, 'min_data_in_leaf': 76, 'feature_fraction': 0.7419794180363664, 'bagging_fraction': 0.9167119842334155, 'lambda_l1': 1.6318670693130386, 'lambda_l2': 3.3796462837763004, 'bagging_freq': 3, 'max_depth': 12}. Best is trial 7 with value: 0.002789101051276499.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[86]\tvalid_0's ndcg@3: 0.964195\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:59:12,279] Trial 13 finished with value: 0.0030036472859901187 and parameters: {'learning_rate': 0.14315945630224736, 'num_leaves': 89, 'min_data_in_leaf': 43, 'feature_fraction': 0.9140660329991824, 'bagging_fraction': 0.8976669467954999, 'lambda_l1': 0.9166693506581738, 'lambda_l2': 2.880573092689219, 'bagging_freq': 4, 'max_depth': 10}. Best is trial 7 with value: 0.002789101051276499.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@3: 0.961243\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 15:59:49,451] Trial 14 finished with value: 0.0032897089322748707 and parameters: {'learning_rate': 0.17739359072451508, 'num_leaves': 24, 'min_data_in_leaf': 65, 'feature_fraction': 0.9883234001756489, 'bagging_fraction': 0.9986713339902848, 'lambda_l1': 0.05656406117400681, 'lambda_l2': 1.533955548723954, 'bagging_freq': 2, 'max_depth': 7}. Best is trial 7 with value: 0.002789101051276499.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@3: 0.963043\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 16:00:23,357] Trial 15 finished with value: 0.0031466781091324947 and parameters: {'learning_rate': 0.13663201070222775, 'num_leaves': 60, 'min_data_in_leaf': 98, 'feature_fraction': 0.8113741919550228, 'bagging_fraction': 0.7391807320124738, 'lambda_l1': 2.2025453459421973, 'lambda_l2': 3.85769969942106, 'bagging_freq': 3, 'max_depth': 10}. Best is trial 7 with value: 0.002789101051276499.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\tvalid_0's ndcg@3: 0.963903\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 16:00:57,416] Trial 16 finished with value: 0.0031466781091324947 and parameters: {'learning_rate': 0.19650698464977978, 'num_leaves': 78, 'min_data_in_leaf': 86, 'feature_fraction': 0.9173550312690267, 'bagging_fraction': 0.5850269953630121, 'lambda_l1': 1.0364090140900855, 'lambda_l2': 3.1627364893943533, 'bagging_freq': 5, 'max_depth': 11}. Best is trial 7 with value: 0.002789101051276499.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[90]\tvalid_0's ndcg@3: 0.96156\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 16:01:27,718] Trial 17 finished with value: 0.003075162697561362 and parameters: {'learning_rate': 0.081744877926514, 'num_leaves': 58, 'min_data_in_leaf': 11, 'feature_fraction': 0.7883771480365709, 'bagging_fraction': 0.791480212218783, 'lambda_l1': 4.123555981517363, 'lambda_l2': 4.02983994148207, 'bagging_freq': 1, 'max_depth': 9}. Best is trial 7 with value: 0.002789101051276499.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\tvalid_0's ndcg@3: 0.963846\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 16:02:04,973] Trial 18 finished with value: 0.0031466781091324947 and parameters: {'learning_rate': 0.17164432437650498, 'num_leaves': 104, 'min_data_in_leaf': 49, 'feature_fraction': 0.6884650431234792, 'bagging_fraction': 0.8709047214406931, 'lambda_l1': 2.282139761417465, 'lambda_l2': 4.991774985764998, 'bagging_freq': 4, 'max_depth': 11}. Best is trial 7 with value: 0.002789101051276499.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@3: 0.961642\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 16:02:42,787] Trial 19 finished with value: 0.003075162697561362 and parameters: {'learning_rate': 0.12476106886954541, 'num_leaves': 82, 'min_data_in_leaf': 64, 'feature_fraction': 0.8602645795424939, 'bagging_fraction': 0.9363487391805796, 'lambda_l1': 1.317271262416911, 'lambda_l2': 1.2829909720510708, 'bagging_freq': 2, 'max_depth': 7}. Best is trial 7 with value: 0.002789101051276499.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\nBest CatBoost:\")\n",
    "print(catboost_study.best_params)\n",
    "print(\"Recall@3:\", 1 - catboost_study.best_value)\n",
    "\n",
    "print(\"\\nBest LightGBM:\")\n",
    "print(lgbm_study.best_params)\n",
    "print(\"Recall@3:\", 1 - lgbm_study.best_value)"
   ],
   "metadata": {
    "id": "IYG8GxkM69Zq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0d432690-8d42-44e9-eabb-f6bcb3ffd28b"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Best CatBoost:\n",
      "{'learning_rate': 0.13275757957731918, 'depth': 6, 'l2_leaf_reg': 7.142519331365267, 'random_strength': 3.395785387976391, 'min_data_in_leaf': 84, 'subsample': 0.9048958560910838, 'colsample_bylevel': 0.511123337191838, 'grow_policy': 'Lossguide'}\n",
      "Recall@3: 0.9961381677751555\n",
      "\n",
      "Best LightGBM:\n",
      "{'learning_rate': 0.19426829578921662, 'num_leaves': 64, 'min_data_in_leaf': 75, 'feature_fraction': 0.9047338201190456, 'bagging_fraction': 0.9624860112209651, 'lambda_l1': 0.9753561860776749, 'lambda_l2': 4.367377782782343, 'bagging_freq': 3, 'max_depth': 10}\n",
      "Recall@3: 0.9972108989487235\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have our parameters, let's train both models and see which performs the best."
   ],
   "metadata": {
    "id": "TXsvWVhc8nEt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model_lightGBM(\n",
    "    parameters: dict,\n",
    "    n_rounds: int = 500,\n",
    "    lr_decay_gamma: float = 0.95,\n",
    "    val_df_input\n",
    "):\n",
    "    # Load validation set\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, group=val_group_sizes, free_raw_data=False)\n",
    "\n",
    "    # Load full training set\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train, group=train_group_sizes, free_raw_data=False)\n",
    "\n",
    "    # Learning rate schedule\n",
    "    def lr_decay(current_round):\n",
    "        return parameters[\"learning_rate\"] * (lr_decay_gamma ** current_round)\n",
    "\n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params=parameters,\n",
    "        train_set=lgb_train,\n",
    "        num_boost_round=n_rounds,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        valid_names=[\"train\", \"val\"],\n",
    "        callbacks=[\n",
    "            lgb.reset_parameter(learning_rate=lr_decay),\n",
    "            lgb.early_stopping(stopping_rounds=500),\n",
    "            lgb.log_evaluation(period=1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Predict and Evaluate\n",
    "    preds = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    val_df[\"score\"] = preds\n",
    "\n",
    "    acc1, recall3, mrr = compute_ranking_metrics(val_df, k=3)\n",
    "\n",
    "    print(\"\\Evaluation Metrics (Validation Set):\")\n",
    "    print(f\"Accuracy@1 : {acc1:.4f}\")\n",
    "    print(f\"Recall@3   : {recall3:.4f}\")\n",
    "    print(f\"MRR        : {mrr:.4f}\")\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "id": "DTe-GvYp8vA_"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_catboost_model(parameters: dict, n_rounds: int = 500, val_df_input=None):\n",
    "    # Training group_id\n",
    "    train_group_id = np.repeat(np.arange(len(train_group_sizes)), train_group_sizes)\n",
    "\n",
    "    # Validation group_id\n",
    "    val_group_id = np.repeat(np.arange(len(val_group_sizes)), val_group_sizes)\n",
    "    # Create Pools for CatBoost\n",
    "    train_pool = Pool(data=X_train, label=y_train, group_id=train_group_id)\n",
    "    val_pool = Pool(data=X_val, label=y_val, group_id=val_group_id)\n",
    "\n",
    "    # Train the model\n",
    "    model = CatBoostRanker(iterations=n_rounds, **parameters)\n",
    "\n",
    "    model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=50, verbose=True)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    preds = model.predict(val_pool)\n",
    "    val_df = val_df_input.copy()\n",
    "    val_df[\"score\"] = preds\n",
    "\n",
    "    acc1, recall3, mrr = compute_ranking_metrics(val_df, k=3)\n",
    "\n",
    "    print(\"\\nEvaluation Metrics (Validation Set):\")\n",
    "    print(f\"Accuracy@1 : {acc1:.4f}\")\n",
    "    print(f\"Recall@3   : {recall3:.4f}\")\n",
    "    print(f\"MRR        : {mrr:.4f}\")\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "id": "-xliLTqn87RX"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Static/default parameters that LightGBM expects\n",
    "base_params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": [\"ndcg\"],\n",
    "    \"eval_at\": [1, 3],\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"verbosity\": -1,\n",
    "    \"force_row_wise\": True,\n",
    "}\n",
    "\n",
    "# Load from file\n",
    "with open(\"light_gbm_std_best.json\", \"r\") as f:\n",
    "    best_params_from_json = json.load(f)\n",
    "\n",
    "# Merge base + Optuna best\n",
    "final_params = {**base_params, **best_params_from_json}\n",
    "\n",
    "train_model_lightGBM(parameters=final_params, n_rounds=1000, lr_decay_gamma=0.95)"
   ],
   "metadata": {
    "id": "8cqWngGu9feR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "902a767b-c03c-470f-b9eb-8d04417dc04a"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1]\ttrain's ndcg@1: 0.876358\ttrain's ndcg@3: 0.94871\tval's ndcg@1: 0.67675\tval's ndcg@3: 0.853679\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[2]\ttrain's ndcg@1: 0.924617\ttrain's ndcg@3: 0.970516\tval's ndcg@1: 0.852106\tval's ndcg@3: 0.940443\n",
      "[3]\ttrain's ndcg@1: 0.929556\ttrain's ndcg@3: 0.972374\tval's ndcg@1: 0.877566\tval's ndcg@3: 0.949844\n",
      "[4]\ttrain's ndcg@1: 0.929909\ttrain's ndcg@3: 0.972571\tval's ndcg@1: 0.879783\tval's ndcg@3: 0.951019\n",
      "[5]\ttrain's ndcg@1: 0.930351\ttrain's ndcg@3: 0.972744\tval's ndcg@1: 0.879282\tval's ndcg@3: 0.950731\n",
      "[6]\ttrain's ndcg@1: 0.930717\ttrain's ndcg@3: 0.972866\tval's ndcg@1: 0.881785\tval's ndcg@3: 0.951585\n",
      "[7]\ttrain's ndcg@1: 0.93112\ttrain's ndcg@3: 0.973051\tval's ndcg@1: 0.88393\tval's ndcg@3: 0.95257\n",
      "[8]\ttrain's ndcg@1: 0.93123\ttrain's ndcg@3: 0.973103\tval's ndcg@1: 0.884074\tval's ndcg@3: 0.952595\n",
      "[9]\ttrain's ndcg@1: 0.931496\ttrain's ndcg@3: 0.97322\tval's ndcg@1: 0.885003\tval's ndcg@3: 0.953002\n",
      "[10]\ttrain's ndcg@1: 0.931746\ttrain's ndcg@3: 0.973343\tval's ndcg@1: 0.886076\tval's ndcg@3: 0.953407\n",
      "[11]\ttrain's ndcg@1: 0.931829\ttrain's ndcg@3: 0.973388\tval's ndcg@1: 0.886219\tval's ndcg@3: 0.953515\n",
      "[12]\ttrain's ndcg@1: 0.932225\ttrain's ndcg@3: 0.973516\tval's ndcg@1: 0.886934\tval's ndcg@3: 0.953807\n",
      "[13]\ttrain's ndcg@1: 0.932528\ttrain's ndcg@3: 0.973655\tval's ndcg@1: 0.890009\tval's ndcg@3: 0.955077\n",
      "[14]\ttrain's ndcg@1: 0.932797\ttrain's ndcg@3: 0.973752\tval's ndcg@1: 0.891297\tval's ndcg@3: 0.955606\n",
      "[15]\ttrain's ndcg@1: 0.93321\ttrain's ndcg@3: 0.973923\tval's ndcg@1: 0.893657\tval's ndcg@3: 0.956568\n",
      "[16]\ttrain's ndcg@1: 0.93356\ttrain's ndcg@3: 0.974097\tval's ndcg@1: 0.8938\tval's ndcg@3: 0.956628\n",
      "[17]\ttrain's ndcg@1: 0.933476\ttrain's ndcg@3: 0.974089\tval's ndcg@1: 0.893871\tval's ndcg@3: 0.956762\n",
      "[18]\ttrain's ndcg@1: 0.933726\ttrain's ndcg@3: 0.974187\tval's ndcg@1: 0.894229\tval's ndcg@3: 0.956894\n",
      "[19]\ttrain's ndcg@1: 0.933636\ttrain's ndcg@3: 0.974179\tval's ndcg@1: 0.894014\tval's ndcg@3: 0.95676\n",
      "[20]\ttrain's ndcg@1: 0.934002\ttrain's ndcg@3: 0.974332\tval's ndcg@1: 0.894229\tval's ndcg@3: 0.956922\n",
      "[21]\ttrain's ndcg@1: 0.934002\ttrain's ndcg@3: 0.974329\tval's ndcg@1: 0.894443\tval's ndcg@3: 0.957027\n",
      "[22]\ttrain's ndcg@1: 0.934192\ttrain's ndcg@3: 0.974394\tval's ndcg@1: 0.894729\tval's ndcg@3: 0.957114\n",
      "[23]\ttrain's ndcg@1: 0.934229\ttrain's ndcg@3: 0.974414\tval's ndcg@1: 0.894872\tval's ndcg@3: 0.957195\n",
      "[24]\ttrain's ndcg@1: 0.934435\ttrain's ndcg@3: 0.974505\tval's ndcg@1: 0.894872\tval's ndcg@3: 0.957096\n",
      "[25]\ttrain's ndcg@1: 0.934385\ttrain's ndcg@3: 0.974481\tval's ndcg@1: 0.895087\tval's ndcg@3: 0.957212\n",
      "[26]\ttrain's ndcg@1: 0.934445\ttrain's ndcg@3: 0.974508\tval's ndcg@1: 0.894801\tval's ndcg@3: 0.957071\n",
      "[27]\ttrain's ndcg@1: 0.934445\ttrain's ndcg@3: 0.97451\tval's ndcg@1: 0.894801\tval's ndcg@3: 0.957097\n",
      "[28]\ttrain's ndcg@1: 0.934545\ttrain's ndcg@3: 0.974551\tval's ndcg@1: 0.894944\tval's ndcg@3: 0.95715\n",
      "[29]\ttrain's ndcg@1: 0.934571\ttrain's ndcg@3: 0.974559\tval's ndcg@1: 0.894586\tval's ndcg@3: 0.957018\n",
      "[30]\ttrain's ndcg@1: 0.934868\ttrain's ndcg@3: 0.974678\tval's ndcg@1: 0.8943\tval's ndcg@3: 0.956931\n",
      "[31]\ttrain's ndcg@1: 0.934908\ttrain's ndcg@3: 0.974705\tval's ndcg@1: 0.894372\tval's ndcg@3: 0.957065\n",
      "[32]\ttrain's ndcg@1: 0.935084\ttrain's ndcg@3: 0.974771\tval's ndcg@1: 0.894658\tval's ndcg@3: 0.95717\n",
      "[33]\ttrain's ndcg@1: 0.935187\ttrain's ndcg@3: 0.974809\tval's ndcg@1: 0.894944\tval's ndcg@3: 0.957285\n",
      "[34]\ttrain's ndcg@1: 0.935307\ttrain's ndcg@3: 0.974859\tval's ndcg@1: 0.895802\tval's ndcg@3: 0.957602\n",
      "[35]\ttrain's ndcg@1: 0.93528\ttrain's ndcg@3: 0.974857\tval's ndcg@1: 0.895731\tval's ndcg@3: 0.957657\n",
      "[36]\ttrain's ndcg@1: 0.935477\ttrain's ndcg@3: 0.97493\tval's ndcg@1: 0.897232\tval's ndcg@3: 0.958211\n",
      "[37]\ttrain's ndcg@1: 0.935447\ttrain's ndcg@3: 0.974922\tval's ndcg@1: 0.897232\tval's ndcg@3: 0.95822\n",
      "[38]\ttrain's ndcg@1: 0.93547\ttrain's ndcg@3: 0.974931\tval's ndcg@1: 0.897518\tval's ndcg@3: 0.958326\n",
      "[39]\ttrain's ndcg@1: 0.935523\ttrain's ndcg@3: 0.974951\tval's ndcg@1: 0.897375\tval's ndcg@3: 0.958273\n",
      "[40]\ttrain's ndcg@1: 0.93559\ttrain's ndcg@3: 0.974979\tval's ndcg@1: 0.897661\tval's ndcg@3: 0.958379\n",
      "[41]\ttrain's ndcg@1: 0.93547\ttrain's ndcg@3: 0.974936\tval's ndcg@1: 0.897661\tval's ndcg@3: 0.958369\n",
      "[42]\ttrain's ndcg@1: 0.93555\ttrain's ndcg@3: 0.974962\tval's ndcg@1: 0.897876\tval's ndcg@3: 0.958439\n",
      "[43]\ttrain's ndcg@1: 0.935563\ttrain's ndcg@3: 0.974966\tval's ndcg@1: 0.897876\tval's ndcg@3: 0.958448\n",
      "[44]\ttrain's ndcg@1: 0.935557\ttrain's ndcg@3: 0.974963\tval's ndcg@1: 0.898019\tval's ndcg@3: 0.958501\n",
      "[45]\ttrain's ndcg@1: 0.935613\ttrain's ndcg@3: 0.974983\tval's ndcg@1: 0.898019\tval's ndcg@3: 0.958492\n",
      "[46]\ttrain's ndcg@1: 0.935603\ttrain's ndcg@3: 0.974979\tval's ndcg@1: 0.898019\tval's ndcg@3: 0.958492\n",
      "[47]\ttrain's ndcg@1: 0.9356\ttrain's ndcg@3: 0.974978\tval's ndcg@1: 0.898019\tval's ndcg@3: 0.958492\n",
      "[48]\ttrain's ndcg@1: 0.9356\ttrain's ndcg@3: 0.974976\tval's ndcg@1: 0.898091\tval's ndcg@3: 0.958554\n",
      "[49]\ttrain's ndcg@1: 0.93567\ttrain's ndcg@3: 0.974997\tval's ndcg@1: 0.898091\tval's ndcg@3: 0.958545\n",
      "[50]\ttrain's ndcg@1: 0.93567\ttrain's ndcg@3: 0.974996\tval's ndcg@1: 0.898091\tval's ndcg@3: 0.958545\n",
      "[51]\ttrain's ndcg@1: 0.93567\ttrain's ndcg@3: 0.974996\tval's ndcg@1: 0.898091\tval's ndcg@3: 0.958545\n",
      "[52]\ttrain's ndcg@1: 0.935796\ttrain's ndcg@3: 0.975043\tval's ndcg@1: 0.898305\tval's ndcg@3: 0.958633\n",
      "[53]\ttrain's ndcg@1: 0.935853\ttrain's ndcg@3: 0.975067\tval's ndcg@1: 0.898591\tval's ndcg@3: 0.958739\n",
      "[54]\ttrain's ndcg@1: 0.935853\ttrain's ndcg@3: 0.975067\tval's ndcg@1: 0.898591\tval's ndcg@3: 0.958739\n",
      "[55]\ttrain's ndcg@1: 0.935853\ttrain's ndcg@3: 0.975067\tval's ndcg@1: 0.898591\tval's ndcg@3: 0.958739\n",
      "[56]\ttrain's ndcg@1: 0.935856\ttrain's ndcg@3: 0.975069\tval's ndcg@1: 0.898377\tval's ndcg@3: 0.95866\n",
      "[57]\ttrain's ndcg@1: 0.935879\ttrain's ndcg@3: 0.975075\tval's ndcg@1: 0.898448\tval's ndcg@3: 0.958686\n",
      "[58]\ttrain's ndcg@1: 0.935879\ttrain's ndcg@3: 0.975075\tval's ndcg@1: 0.898448\tval's ndcg@3: 0.958686\n",
      "[59]\ttrain's ndcg@1: 0.935879\ttrain's ndcg@3: 0.975075\tval's ndcg@1: 0.898448\tval's ndcg@3: 0.958686\n",
      "[60]\ttrain's ndcg@1: 0.935876\ttrain's ndcg@3: 0.975072\tval's ndcg@1: 0.898448\tval's ndcg@3: 0.958686\n",
      "[61]\ttrain's ndcg@1: 0.936069\ttrain's ndcg@3: 0.975145\tval's ndcg@1: 0.899521\tval's ndcg@3: 0.959082\n",
      "[62]\ttrain's ndcg@1: 0.936079\ttrain's ndcg@3: 0.975149\tval's ndcg@1: 0.899449\tval's ndcg@3: 0.959056\n",
      "[63]\ttrain's ndcg@1: 0.936079\ttrain's ndcg@3: 0.975149\tval's ndcg@1: 0.899449\tval's ndcg@3: 0.959056\n",
      "[64]\ttrain's ndcg@1: 0.936079\ttrain's ndcg@3: 0.975149\tval's ndcg@1: 0.899449\tval's ndcg@3: 0.959056\n",
      "[65]\ttrain's ndcg@1: 0.936079\ttrain's ndcg@3: 0.975149\tval's ndcg@1: 0.899449\tval's ndcg@3: 0.959056\n",
      "[66]\ttrain's ndcg@1: 0.936126\ttrain's ndcg@3: 0.975166\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[67]\ttrain's ndcg@1: 0.936126\ttrain's ndcg@3: 0.975166\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[68]\ttrain's ndcg@1: 0.936126\ttrain's ndcg@3: 0.975166\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[69]\ttrain's ndcg@1: 0.936126\ttrain's ndcg@3: 0.975166\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[70]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.97515\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[71]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.97515\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[72]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.97515\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[73]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.97515\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[74]\ttrain's ndcg@1: 0.936092\ttrain's ndcg@3: 0.975154\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[75]\ttrain's ndcg@1: 0.936092\ttrain's ndcg@3: 0.975155\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[76]\ttrain's ndcg@1: 0.936092\ttrain's ndcg@3: 0.975155\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[77]\ttrain's ndcg@1: 0.936092\ttrain's ndcg@3: 0.975155\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[78]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.97515\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[79]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[80]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.97515\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[81]\ttrain's ndcg@1: 0.936092\ttrain's ndcg@3: 0.975154\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[82]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[83]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[84]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[85]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[86]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[87]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[88]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[89]\ttrain's ndcg@1: 0.936092\ttrain's ndcg@3: 0.975155\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[90]\ttrain's ndcg@1: 0.936092\ttrain's ndcg@3: 0.975155\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[91]\ttrain's ndcg@1: 0.936092\ttrain's ndcg@3: 0.975155\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[92]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[93]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[94]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[95]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[96]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[97]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[98]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[99]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[100]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[101]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[102]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[103]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[104]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[105]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[106]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[107]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[108]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[109]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[110]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[111]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[112]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[113]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[114]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[115]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[116]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[117]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[118]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[119]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[120]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[121]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[122]\ttrain's ndcg@1: 0.936089\ttrain's ndcg@3: 0.975152\tval's ndcg@1: 0.899807\tval's ndcg@3: 0.959187\n",
      "[123]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[124]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[125]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[126]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[127]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[128]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[129]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[130]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[131]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[132]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[133]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[134]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[135]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[136]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[137]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[138]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[139]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[140]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[141]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[142]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[143]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[144]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[145]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[146]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[147]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[148]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[149]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[150]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[151]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[152]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[153]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[154]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[155]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[156]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[157]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[158]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[159]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[160]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[161]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[162]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[163]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[164]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[165]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[166]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[167]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[168]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[169]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[170]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[171]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[172]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[173]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[174]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[175]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[176]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[177]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[178]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[179]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[180]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[181]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[182]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[183]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[184]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[185]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[186]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[187]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[188]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[189]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[190]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[191]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[192]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[193]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[194]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[195]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[196]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[197]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[198]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[199]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[200]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[201]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[202]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[203]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[204]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[205]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[206]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[207]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[208]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[209]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[210]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[211]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[212]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[213]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[214]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[215]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[216]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[217]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[218]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[219]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[220]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[221]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[222]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[223]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[224]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[225]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[226]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[227]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[228]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[229]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[230]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[231]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[232]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[233]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[234]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[235]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[236]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[237]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[238]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[239]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[240]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[241]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[242]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[243]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[244]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[245]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[246]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[247]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[248]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[249]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[250]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[251]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[252]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[253]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[254]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[255]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[256]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[257]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[258]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[259]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[260]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[261]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[262]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[263]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[264]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[265]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[266]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[267]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[268]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[269]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[270]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[271]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[272]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[273]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[274]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[275]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[276]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[277]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[278]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[279]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[280]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[281]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[282]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[283]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[284]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[285]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[286]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[287]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[288]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[289]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[290]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[291]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[292]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[293]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[294]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[295]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[296]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[297]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[298]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[299]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[300]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[301]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[302]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[303]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[304]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[305]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[306]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[307]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[308]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[309]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[310]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[311]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[312]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[313]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[314]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[315]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[316]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[317]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[318]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[319]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[320]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[321]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[322]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[323]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[324]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[325]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[326]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[327]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[328]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[329]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[330]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[331]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[332]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[333]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[334]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[335]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[336]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[337]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[338]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[339]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[340]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[341]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[342]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[343]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[344]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[345]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[346]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[347]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[348]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[349]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[350]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[351]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[352]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[353]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[354]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[355]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[356]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[357]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[358]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[359]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[360]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[361]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[362]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[363]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[364]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[365]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[366]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[367]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[368]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[369]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[370]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[371]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[372]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[373]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[374]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[375]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[376]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[377]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[378]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[379]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[380]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[381]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[382]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[383]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[384]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[385]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[386]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[387]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[388]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[389]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[390]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[391]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[392]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[393]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[394]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[395]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[396]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[397]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[398]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[399]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[400]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[401]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[402]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[403]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[404]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[405]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[406]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[407]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[408]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[409]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[410]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[411]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[412]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[413]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[414]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[415]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[416]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[417]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[418]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[419]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[420]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[421]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[422]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[423]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[424]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[425]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[426]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[427]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[428]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[429]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[430]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[431]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[432]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[433]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[434]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[435]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[436]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[437]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[438]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[439]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[440]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[441]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[442]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[443]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[444]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[445]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[446]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[447]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[448]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[449]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[450]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[451]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[452]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[453]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[454]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[455]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[456]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[457]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[458]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[459]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[460]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[461]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[462]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[463]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[464]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[465]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[466]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[467]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[468]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[469]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[470]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[471]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[472]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[473]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[474]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[475]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[476]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[477]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[478]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[479]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[480]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[481]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[482]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[483]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[484]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[485]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[486]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[487]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[488]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[489]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[490]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[491]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[492]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[493]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[494]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[495]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[496]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[497]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[498]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[499]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[500]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[501]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[502]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[503]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[504]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[505]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[506]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[507]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[508]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[509]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[510]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[511]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[512]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[513]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[514]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[515]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[516]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[517]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[518]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[519]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[520]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[521]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[522]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[523]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[524]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[525]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[526]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[527]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[528]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[529]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[530]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[531]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[532]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[533]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[534]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[535]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[536]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[537]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[538]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[539]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[540]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[541]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[542]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[543]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[544]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[545]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[546]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[547]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[548]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[549]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[550]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[551]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[552]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[553]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[554]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[555]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[556]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[557]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[558]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[559]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[560]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[561]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[562]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[563]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[564]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[565]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[566]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[567]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[568]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[569]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[570]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[571]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[572]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[573]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[574]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[575]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[576]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[577]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[578]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[579]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[580]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[581]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[582]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[583]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[584]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[585]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[586]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[587]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[588]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[589]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[590]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[591]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[592]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[593]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[594]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[595]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[596]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[597]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[598]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[599]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[600]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[601]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[602]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[603]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[604]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[605]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[606]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[607]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[608]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[609]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[610]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[611]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[612]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[613]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[614]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[615]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[616]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[617]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[618]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[619]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[620]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[621]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[622]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "[623]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n",
      "Early stopping, best iteration is:\n",
      "[123]\ttrain's ndcg@1: 0.936206\ttrain's ndcg@3: 0.975195\tval's ndcg@1: 0.900451\tval's ndcg@3: 0.959425\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\Evaluation Metrics (Validation Set):\n",
      "Accuracy@1 : 0.9005\n",
      "Recall@3   : 0.9966\n",
      "MRR        : 0.9471\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7a8b2cbdfad0>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Strong results. The correct prediction is almost always in the top 3 ranked items and is in the top 1 stop 90% of the time."
   ],
   "metadata": {
    "id": "pKzkuaco7ppJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Base/defaults for CatBoost\n",
    "base_params = {\n",
    "    \"loss_function\": \"YetiRank\",\n",
    "    \"eval_metric\": \"NDCG:top=3\",\n",
    "    \"random_seed\": 42,\n",
    "    \"task_type\": \"CPU\",  # or \"GPU\"\n",
    "}\n",
    "\n",
    "# Load from file\n",
    "with open(\"catboost_std_best.json\", \"r\") as f:\n",
    "    best_params_from_json = json.load(f)\n",
    "\n",
    "# Merge base + Optuna best\n",
    "final_params = {**base_params, **best_params_from_json}\n",
    "\n",
    "best_catboost_model = train_catboost_model(\n",
    "    parameters=final_params, n_rounds=1000, val_df_input=val_df\n",
    ")"
   ],
   "metadata": {
    "id": "FPqTY2Ty-Grv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b3644dbb-7098-4d91-a0ee-c7f16f4af90b"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:\ttest: 0.8931327\tbest: 0.8931327 (0)\ttotal: 2.67s\tremaining: 44m 29s\n",
      "1:\ttest: 0.9371556\tbest: 0.9371556 (1)\ttotal: 5.34s\tremaining: 44m 22s\n",
      "2:\ttest: 0.9366829\tbest: 0.9371556 (1)\ttotal: 7.92s\tremaining: 43m 52s\n",
      "3:\ttest: 0.9369486\tbest: 0.9371556 (1)\ttotal: 11s\tremaining: 45m 29s\n",
      "4:\ttest: 0.9365407\tbest: 0.9371556 (1)\ttotal: 13.7s\tremaining: 45m 35s\n",
      "5:\ttest: 0.9394015\tbest: 0.9394015 (5)\ttotal: 16.8s\tremaining: 46m 27s\n",
      "6:\ttest: 0.9397957\tbest: 0.9397957 (6)\ttotal: 19.7s\tremaining: 46m 41s\n",
      "7:\ttest: 0.9400222\tbest: 0.9400222 (7)\ttotal: 22.4s\tremaining: 46m 12s\n",
      "8:\ttest: 0.9400545\tbest: 0.9400545 (8)\ttotal: 24.8s\tremaining: 45m 30s\n",
      "9:\ttest: 0.9403287\tbest: 0.9403287 (9)\ttotal: 27.6s\tremaining: 45m 29s\n",
      "10:\ttest: 0.9410855\tbest: 0.9410855 (10)\ttotal: 30.2s\tremaining: 45m 14s\n",
      "11:\ttest: 0.9444052\tbest: 0.9444052 (11)\ttotal: 32.5s\tremaining: 44m 38s\n",
      "12:\ttest: 0.9449433\tbest: 0.9449433 (12)\ttotal: 34.8s\tremaining: 44m 2s\n",
      "13:\ttest: 0.9430693\tbest: 0.9449433 (12)\ttotal: 37.5s\tremaining: 43m 58s\n",
      "14:\ttest: 0.9428471\tbest: 0.9449433 (12)\ttotal: 39.6s\tremaining: 43m 23s\n",
      "15:\ttest: 0.9448641\tbest: 0.9449433 (12)\ttotal: 42.4s\tremaining: 43m 27s\n",
      "16:\ttest: 0.9451519\tbest: 0.9451519 (16)\ttotal: 45s\tremaining: 43m 20s\n",
      "17:\ttest: 0.9452174\tbest: 0.9452174 (17)\ttotal: 47.5s\tremaining: 43m 12s\n",
      "18:\ttest: 0.9471987\tbest: 0.9471987 (18)\ttotal: 50.5s\tremaining: 43m 26s\n",
      "19:\ttest: 0.9472217\tbest: 0.9472217 (19)\ttotal: 53.2s\tremaining: 43m 25s\n",
      "20:\ttest: 0.9474269\tbest: 0.9474269 (20)\ttotal: 55.3s\tremaining: 42m 58s\n",
      "21:\ttest: 0.9489143\tbest: 0.9489143 (21)\ttotal: 57.9s\tremaining: 42m 52s\n",
      "22:\ttest: 0.9490386\tbest: 0.9490386 (22)\ttotal: 1m\tremaining: 42m 45s\n",
      "23:\ttest: 0.9488973\tbest: 0.9490386 (22)\ttotal: 1m 2s\tremaining: 42m 41s\n",
      "24:\ttest: 0.9491255\tbest: 0.9491255 (24)\ttotal: 1m 5s\tremaining: 42m 36s\n",
      "25:\ttest: 0.9491238\tbest: 0.9491255 (24)\ttotal: 1m 8s\tremaining: 42m 33s\n",
      "26:\ttest: 0.9493230\tbest: 0.9493230 (26)\ttotal: 1m 10s\tremaining: 42m 21s\n",
      "27:\ttest: 0.9493213\tbest: 0.9493230 (26)\ttotal: 1m 13s\tremaining: 42m 30s\n",
      "28:\ttest: 0.9494626\tbest: 0.9494626 (28)\ttotal: 1m 16s\tremaining: 42m 38s\n",
      "29:\ttest: 0.9495324\tbest: 0.9495324 (29)\ttotal: 1m 19s\tremaining: 42m 41s\n",
      "30:\ttest: 0.9495248\tbest: 0.9495324 (29)\ttotal: 1m 21s\tremaining: 42m 39s\n",
      "31:\ttest: 0.9497283\tbest: 0.9497283 (31)\ttotal: 1m 24s\tremaining: 42m 30s\n",
      "32:\ttest: 0.9498449\tbest: 0.9498449 (32)\ttotal: 1m 26s\tremaining: 42m 26s\n",
      "33:\ttest: 0.9499956\tbest: 0.9499956 (33)\ttotal: 1m 29s\tremaining: 42m 15s\n",
      "34:\ttest: 0.9499956\tbest: 0.9499956 (33)\ttotal: 1m 31s\tremaining: 42m 12s\n",
      "35:\ttest: 0.9499054\tbest: 0.9499956 (33)\ttotal: 1m 34s\tremaining: 42m 12s\n",
      "36:\ttest: 0.9499769\tbest: 0.9499956 (33)\ttotal: 1m 37s\tremaining: 42m 6s\n",
      "37:\ttest: 0.9499411\tbest: 0.9499956 (33)\ttotal: 1m 39s\tremaining: 41m 57s\n",
      "38:\ttest: 0.9500373\tbest: 0.9500373 (38)\ttotal: 1m 42s\tremaining: 41m 54s\n",
      "39:\ttest: 0.9500109\tbest: 0.9500373 (38)\ttotal: 1m 44s\tremaining: 41m 54s\n",
      "40:\ttest: 0.9502919\tbest: 0.9502919 (40)\ttotal: 1m 47s\tremaining: 41m 55s\n",
      "41:\ttest: 0.9503464\tbest: 0.9503464 (41)\ttotal: 1m 50s\tremaining: 42m 1s\n",
      "42:\ttest: 0.9503370\tbest: 0.9503464 (41)\ttotal: 1m 53s\tremaining: 41m 58s\n",
      "43:\ttest: 0.9502578\tbest: 0.9503464 (41)\ttotal: 1m 55s\tremaining: 41m 55s\n",
      "44:\ttest: 0.9502578\tbest: 0.9503464 (41)\ttotal: 1m 58s\tremaining: 41m 47s\n",
      "45:\ttest: 0.9502391\tbest: 0.9503464 (41)\ttotal: 2m\tremaining: 41m 40s\n",
      "46:\ttest: 0.9506895\tbest: 0.9506895 (46)\ttotal: 2m 3s\tremaining: 41m 36s\n",
      "47:\ttest: 0.9506895\tbest: 0.9506895 (46)\ttotal: 2m 6s\tremaining: 41m 42s\n",
      "48:\ttest: 0.9504801\tbest: 0.9506895 (46)\ttotal: 2m 9s\tremaining: 41m 45s\n",
      "49:\ttest: 0.9504801\tbest: 0.9506895 (46)\ttotal: 2m 11s\tremaining: 41m 35s\n",
      "50:\ttest: 0.9504630\tbest: 0.9506895 (46)\ttotal: 2m 13s\tremaining: 41m 31s\n",
      "51:\ttest: 0.9504630\tbest: 0.9506895 (46)\ttotal: 2m 16s\tremaining: 41m 24s\n",
      "52:\ttest: 0.9506401\tbest: 0.9506895 (46)\ttotal: 2m 18s\tremaining: 41m 17s\n",
      "53:\ttest: 0.9506742\tbest: 0.9506895 (46)\ttotal: 2m 21s\tremaining: 41m 17s\n",
      "54:\ttest: 0.9506307\tbest: 0.9506895 (46)\ttotal: 2m 24s\tremaining: 41m 21s\n",
      "55:\ttest: 0.9506835\tbest: 0.9506895 (46)\ttotal: 2m 27s\tremaining: 41m 18s\n",
      "56:\ttest: 0.9506027\tbest: 0.9506895 (46)\ttotal: 2m 29s\tremaining: 41m 20s\n",
      "57:\ttest: 0.9504707\tbest: 0.9506895 (46)\ttotal: 2m 32s\tremaining: 41m 17s\n",
      "58:\ttest: 0.9504537\tbest: 0.9506895 (46)\ttotal: 2m 35s\tremaining: 41m 13s\n",
      "59:\ttest: 0.9504179\tbest: 0.9506895 (46)\ttotal: 2m 38s\tremaining: 41m 16s\n",
      "60:\ttest: 0.9505499\tbest: 0.9506895 (46)\ttotal: 2m 40s\tremaining: 41m 10s\n",
      "61:\ttest: 0.9506648\tbest: 0.9506895 (46)\ttotal: 2m 43s\tremaining: 41m 7s\n",
      "62:\ttest: 0.9506648\tbest: 0.9506895 (46)\ttotal: 2m 46s\tremaining: 41m 11s\n",
      "63:\ttest: 0.9510607\tbest: 0.9510607 (63)\ttotal: 2m 48s\tremaining: 41m 9s\n",
      "64:\ttest: 0.9510437\tbest: 0.9510607 (63)\ttotal: 2m 51s\tremaining: 40m 59s\n",
      "65:\ttest: 0.9509892\tbest: 0.9510607 (63)\ttotal: 2m 53s\tremaining: 40m 57s\n",
      "66:\ttest: 0.9503745\tbest: 0.9510607 (63)\ttotal: 2m 56s\tremaining: 40m 53s\n",
      "67:\ttest: 0.9503387\tbest: 0.9510607 (63)\ttotal: 2m 58s\tremaining: 40m 52s\n",
      "68:\ttest: 0.9509815\tbest: 0.9510607 (63)\ttotal: 3m 1s\tremaining: 40m 49s\n",
      "69:\ttest: 0.9509815\tbest: 0.9510607 (63)\ttotal: 3m 4s\tremaining: 40m 47s\n",
      "70:\ttest: 0.9516150\tbest: 0.9516150 (70)\ttotal: 3m 7s\tremaining: 40m 47s\n",
      "71:\ttest: 0.9515716\tbest: 0.9516150 (70)\ttotal: 3m 9s\tremaining: 40m 42s\n",
      "72:\ttest: 0.9516431\tbest: 0.9516431 (72)\ttotal: 3m 12s\tremaining: 40m 41s\n",
      "73:\ttest: 0.9515809\tbest: 0.9516431 (72)\ttotal: 3m 14s\tremaining: 40m 36s\n",
      "74:\ttest: 0.9515809\tbest: 0.9516431 (72)\ttotal: 3m 17s\tremaining: 40m 36s\n",
      "75:\ttest: 0.9516431\tbest: 0.9516431 (72)\ttotal: 3m 19s\tremaining: 40m 30s\n",
      "76:\ttest: 0.9513000\tbest: 0.9516431 (72)\ttotal: 3m 22s\tremaining: 40m 27s\n",
      "77:\ttest: 0.9512532\tbest: 0.9516431 (72)\ttotal: 3m 25s\tremaining: 40m 23s\n",
      "78:\ttest: 0.9512268\tbest: 0.9516431 (72)\ttotal: 3m 27s\tremaining: 40m 17s\n",
      "79:\ttest: 0.9514055\tbest: 0.9516431 (72)\ttotal: 3m 30s\tremaining: 40m 17s\n",
      "80:\ttest: 0.9514319\tbest: 0.9516431 (72)\ttotal: 3m 32s\tremaining: 40m 12s\n",
      "81:\ttest: 0.9514055\tbest: 0.9516431 (72)\ttotal: 3m 35s\tremaining: 40m 9s\n",
      "82:\ttest: 0.9514413\tbest: 0.9516431 (72)\ttotal: 3m 38s\tremaining: 40m 10s\n",
      "83:\ttest: 0.9521276\tbest: 0.9521276 (83)\ttotal: 3m 40s\tremaining: 40m 5s\n",
      "84:\ttest: 0.9521369\tbest: 0.9521369 (84)\ttotal: 3m 43s\tremaining: 40m 5s\n",
      "85:\ttest: 0.9521633\tbest: 0.9521633 (85)\ttotal: 3m 46s\tremaining: 40m 2s\n",
      "86:\ttest: 0.9521633\tbest: 0.9521633 (85)\ttotal: 3m 48s\tremaining: 40m 1s\n",
      "87:\ttest: 0.9520049\tbest: 0.9521633 (85)\ttotal: 3m 51s\tremaining: 39m 56s\n",
      "88:\ttest: 0.9522161\tbest: 0.9522161 (88)\ttotal: 3m 53s\tremaining: 39m 53s\n",
      "89:\ttest: 0.9522238\tbest: 0.9522238 (89)\ttotal: 3m 56s\tremaining: 39m 50s\n",
      "90:\ttest: 0.9522238\tbest: 0.9522238 (89)\ttotal: 3m 59s\tremaining: 39m 48s\n",
      "91:\ttest: 0.9526461\tbest: 0.9526461 (91)\ttotal: 4m 1s\tremaining: 39m 45s\n",
      "92:\ttest: 0.9525482\tbest: 0.9526461 (91)\ttotal: 4m 4s\tremaining: 39m 42s\n",
      "93:\ttest: 0.9525482\tbest: 0.9526461 (91)\ttotal: 4m 6s\tremaining: 39m 39s\n",
      "94:\ttest: 0.9525907\tbest: 0.9526461 (91)\ttotal: 4m 9s\tremaining: 39m 36s\n",
      "95:\ttest: 0.9525737\tbest: 0.9526461 (91)\ttotal: 4m 12s\tremaining: 39m 33s\n",
      "96:\ttest: 0.9527303\tbest: 0.9527303 (96)\ttotal: 4m 14s\tremaining: 39m 29s\n",
      "97:\ttest: 0.9527227\tbest: 0.9527303 (96)\ttotal: 4m 17s\tremaining: 39m 27s\n",
      "98:\ttest: 0.9527491\tbest: 0.9527491 (98)\ttotal: 4m 19s\tremaining: 39m 20s\n",
      "99:\ttest: 0.9527755\tbest: 0.9527755 (99)\ttotal: 4m 21s\tremaining: 39m 17s\n",
      "100:\ttest: 0.9527755\tbest: 0.9527755 (99)\ttotal: 4m 24s\tremaining: 39m 15s\n",
      "101:\ttest: 0.9529074\tbest: 0.9529074 (101)\ttotal: 4m 27s\tremaining: 39m 12s\n",
      "102:\ttest: 0.9529245\tbest: 0.9529245 (102)\ttotal: 4m 29s\tremaining: 39m 9s\n",
      "103:\ttest: 0.9529074\tbest: 0.9529245 (102)\ttotal: 4m 32s\tremaining: 39m 5s\n",
      "104:\ttest: 0.9528981\tbest: 0.9529245 (102)\ttotal: 4m 34s\tremaining: 39m 2s\n",
      "105:\ttest: 0.9528453\tbest: 0.9529245 (102)\ttotal: 4m 37s\tremaining: 38m 57s\n",
      "106:\ttest: 0.9528734\tbest: 0.9529245 (102)\ttotal: 4m 39s\tremaining: 38m 55s\n",
      "107:\ttest: 0.9530411\tbest: 0.9530411 (107)\ttotal: 4m 42s\tremaining: 38m 52s\n",
      "108:\ttest: 0.9529602\tbest: 0.9530411 (107)\ttotal: 4m 44s\tremaining: 38m 47s\n",
      "109:\ttest: 0.9532318\tbest: 0.9532318 (109)\ttotal: 4m 46s\tremaining: 38m 41s\n",
      "110:\ttest: 0.9531961\tbest: 0.9532318 (109)\ttotal: 4m 50s\tremaining: 38m 42s\n",
      "111:\ttest: 0.9532148\tbest: 0.9532318 (109)\ttotal: 4m 52s\tremaining: 38m 40s\n",
      "112:\ttest: 0.9532148\tbest: 0.9532318 (109)\ttotal: 4m 55s\tremaining: 38m 39s\n",
      "113:\ttest: 0.9532148\tbest: 0.9532318 (109)\ttotal: 4m 57s\tremaining: 38m 32s\n",
      "114:\ttest: 0.9532148\tbest: 0.9532318 (109)\ttotal: 4m 59s\tremaining: 38m 28s\n",
      "115:\ttest: 0.9530224\tbest: 0.9532318 (109)\ttotal: 5m 2s\tremaining: 38m 24s\n",
      "116:\ttest: 0.9531995\tbest: 0.9532318 (109)\ttotal: 5m 5s\tremaining: 38m 21s\n",
      "117:\ttest: 0.9531280\tbest: 0.9532318 (109)\ttotal: 5m 7s\tremaining: 38m 20s\n",
      "118:\ttest: 0.9531373\tbest: 0.9532318 (109)\ttotal: 5m 10s\tremaining: 38m 15s\n",
      "119:\ttest: 0.9531280\tbest: 0.9532318 (109)\ttotal: 5m 12s\tremaining: 38m 13s\n",
      "120:\ttest: 0.9531280\tbest: 0.9532318 (109)\ttotal: 5m 15s\tremaining: 38m 9s\n",
      "121:\ttest: 0.9531543\tbest: 0.9532318 (109)\ttotal: 5m 17s\tremaining: 38m 5s\n",
      "122:\ttest: 0.9531543\tbest: 0.9532318 (109)\ttotal: 5m 19s\tremaining: 37m 56s\n",
      "123:\ttest: 0.9531637\tbest: 0.9532318 (109)\ttotal: 5m 22s\tremaining: 37m 57s\n",
      "124:\ttest: 0.9536916\tbest: 0.9536916 (124)\ttotal: 5m 24s\tremaining: 37m 53s\n",
      "125:\ttest: 0.9536746\tbest: 0.9536916 (124)\ttotal: 5m 27s\tremaining: 37m 50s\n",
      "126:\ttest: 0.9536933\tbest: 0.9536933 (126)\ttotal: 5m 29s\tremaining: 37m 47s\n",
      "127:\ttest: 0.9537631\tbest: 0.9537631 (127)\ttotal: 5m 32s\tremaining: 37m 43s\n",
      "128:\ttest: 0.9537103\tbest: 0.9537631 (127)\ttotal: 5m 34s\tremaining: 37m 41s\n",
      "129:\ttest: 0.9537103\tbest: 0.9537631 (127)\ttotal: 5m 37s\tremaining: 37m 40s\n",
      "130:\ttest: 0.9539402\tbest: 0.9539402 (130)\ttotal: 5m 40s\tremaining: 37m 37s\n",
      "131:\ttest: 0.9537989\tbest: 0.9539402 (130)\ttotal: 5m 42s\tremaining: 37m 31s\n",
      "132:\ttest: 0.9538176\tbest: 0.9539402 (130)\ttotal: 5m 44s\tremaining: 37m 28s\n",
      "133:\ttest: 0.9537912\tbest: 0.9539402 (130)\ttotal: 5m 47s\tremaining: 37m 25s\n",
      "134:\ttest: 0.9537648\tbest: 0.9539402 (130)\ttotal: 5m 50s\tremaining: 37m 23s\n",
      "135:\ttest: 0.9539061\tbest: 0.9539402 (130)\ttotal: 5m 52s\tremaining: 37m 20s\n",
      "136:\ttest: 0.9542663\tbest: 0.9542663 (136)\ttotal: 5m 55s\tremaining: 37m 16s\n",
      "137:\ttest: 0.9542663\tbest: 0.9542663 (136)\ttotal: 5m 57s\tremaining: 37m 12s\n",
      "138:\ttest: 0.9542569\tbest: 0.9542663 (136)\ttotal: 6m\tremaining: 37m 9s\n",
      "139:\ttest: 0.9542305\tbest: 0.9542663 (136)\ttotal: 6m 2s\tremaining: 37m 7s\n",
      "140:\ttest: 0.9541794\tbest: 0.9542663 (136)\ttotal: 6m 5s\tremaining: 37m 6s\n",
      "141:\ttest: 0.9542493\tbest: 0.9542663 (136)\ttotal: 6m 8s\tremaining: 37m 5s\n",
      "142:\ttest: 0.9542493\tbest: 0.9542663 (136)\ttotal: 6m 11s\tremaining: 37m 3s\n",
      "143:\ttest: 0.9542058\tbest: 0.9542663 (136)\ttotal: 6m 13s\tremaining: 36m 59s\n",
      "144:\ttest: 0.9542586\tbest: 0.9542663 (136)\ttotal: 6m 16s\tremaining: 36m 58s\n",
      "145:\ttest: 0.9542058\tbest: 0.9542663 (136)\ttotal: 6m 18s\tremaining: 36m 54s\n",
      "146:\ttest: 0.9539155\tbest: 0.9542663 (136)\ttotal: 6m 20s\tremaining: 36m 50s\n",
      "147:\ttest: 0.9539155\tbest: 0.9542663 (136)\ttotal: 6m 23s\tremaining: 36m 46s\n",
      "148:\ttest: 0.9540304\tbest: 0.9542663 (136)\ttotal: 6m 25s\tremaining: 36m 42s\n",
      "149:\ttest: 0.9541096\tbest: 0.9542663 (136)\ttotal: 6m 28s\tremaining: 36m 38s\n",
      "150:\ttest: 0.9541360\tbest: 0.9542663 (136)\ttotal: 6m 30s\tremaining: 36m 37s\n",
      "151:\ttest: 0.9541624\tbest: 0.9542663 (136)\ttotal: 6m 33s\tremaining: 36m 34s\n",
      "152:\ttest: 0.9541624\tbest: 0.9542663 (136)\ttotal: 6m 35s\tremaining: 36m 30s\n",
      "153:\ttest: 0.9540986\tbest: 0.9542663 (136)\ttotal: 6m 37s\tremaining: 36m 26s\n",
      "154:\ttest: 0.9541607\tbest: 0.9542663 (136)\ttotal: 6m 40s\tremaining: 36m 23s\n",
      "155:\ttest: 0.9541514\tbest: 0.9542663 (136)\ttotal: 6m 42s\tremaining: 36m 18s\n",
      "156:\ttest: 0.9543642\tbest: 0.9543642 (156)\ttotal: 6m 45s\tremaining: 36m 15s\n",
      "157:\ttest: 0.9543472\tbest: 0.9543642 (156)\ttotal: 6m 47s\tremaining: 36m 12s\n",
      "158:\ttest: 0.9542757\tbest: 0.9543642 (156)\ttotal: 6m 50s\tremaining: 36m 9s\n",
      "159:\ttest: 0.9543472\tbest: 0.9543642 (156)\ttotal: 6m 52s\tremaining: 36m 7s\n",
      "160:\ttest: 0.9543472\tbest: 0.9543642 (156)\ttotal: 6m 54s\tremaining: 36m 2s\n",
      "161:\ttest: 0.9543736\tbest: 0.9543736 (161)\ttotal: 6m 58s\tremaining: 36m 2s\n",
      "162:\ttest: 0.9543736\tbest: 0.9543736 (161)\ttotal: 7m\tremaining: 35m 59s\n",
      "163:\ttest: 0.9543736\tbest: 0.9543736 (161)\ttotal: 7m 3s\tremaining: 35m 57s\n",
      "164:\ttest: 0.9545847\tbest: 0.9545847 (164)\ttotal: 7m 5s\tremaining: 35m 54s\n",
      "165:\ttest: 0.9545754\tbest: 0.9545847 (164)\ttotal: 7m 8s\tremaining: 35m 51s\n",
      "166:\ttest: 0.9545847\tbest: 0.9545847 (164)\ttotal: 7m 10s\tremaining: 35m 48s\n",
      "167:\ttest: 0.9545847\tbest: 0.9545847 (164)\ttotal: 7m 13s\tremaining: 35m 48s\n",
      "168:\ttest: 0.9544076\tbest: 0.9545847 (164)\ttotal: 7m 16s\tremaining: 35m 45s\n",
      "169:\ttest: 0.9543812\tbest: 0.9545847 (164)\ttotal: 7m 18s\tremaining: 35m 42s\n",
      "170:\ttest: 0.9543284\tbest: 0.9545847 (164)\ttotal: 7m 21s\tremaining: 35m 40s\n",
      "171:\ttest: 0.9543284\tbest: 0.9545847 (164)\ttotal: 7m 24s\tremaining: 35m 39s\n",
      "172:\ttest: 0.9543548\tbest: 0.9545847 (164)\ttotal: 7m 27s\tremaining: 35m 37s\n",
      "173:\ttest: 0.9543548\tbest: 0.9545847 (164)\ttotal: 7m 29s\tremaining: 35m 34s\n",
      "174:\ttest: 0.9543812\tbest: 0.9545847 (164)\ttotal: 7m 32s\tremaining: 35m 31s\n",
      "175:\ttest: 0.9544775\tbest: 0.9545847 (164)\ttotal: 7m 35s\tremaining: 35m 30s\n",
      "176:\ttest: 0.9545830\tbest: 0.9545847 (164)\ttotal: 7m 38s\tremaining: 35m 29s\n",
      "177:\ttest: 0.9546886\tbest: 0.9546886 (177)\ttotal: 7m 40s\tremaining: 35m 27s\n",
      "178:\ttest: 0.9546622\tbest: 0.9546886 (177)\ttotal: 7m 43s\tremaining: 35m 24s\n",
      "179:\ttest: 0.9546358\tbest: 0.9546886 (177)\ttotal: 7m 45s\tremaining: 35m 21s\n",
      "180:\ttest: 0.9546980\tbest: 0.9546980 (180)\ttotal: 7m 47s\tremaining: 35m 17s\n",
      "181:\ttest: 0.9554106\tbest: 0.9554106 (181)\ttotal: 7m 50s\tremaining: 35m 15s\n",
      "182:\ttest: 0.9554106\tbest: 0.9554106 (181)\ttotal: 7m 53s\tremaining: 35m 12s\n",
      "183:\ttest: 0.9554557\tbest: 0.9554557 (183)\ttotal: 7m 55s\tremaining: 35m 7s\n",
      "184:\ttest: 0.9554821\tbest: 0.9554821 (184)\ttotal: 7m 57s\tremaining: 35m 5s\n",
      "185:\ttest: 0.9554821\tbest: 0.9554821 (184)\ttotal: 8m\tremaining: 35m 2s\n",
      "186:\ttest: 0.9555536\tbest: 0.9555536 (186)\ttotal: 8m 2s\tremaining: 34m 58s\n",
      "187:\ttest: 0.9555366\tbest: 0.9555536 (186)\ttotal: 8m 4s\tremaining: 34m 54s\n",
      "188:\ttest: 0.9550615\tbest: 0.9555536 (186)\ttotal: 8m 7s\tremaining: 34m 53s\n",
      "189:\ttest: 0.9550615\tbest: 0.9555536 (186)\ttotal: 8m 10s\tremaining: 34m 49s\n",
      "190:\ttest: 0.9552199\tbest: 0.9555536 (186)\ttotal: 8m 12s\tremaining: 34m 45s\n",
      "191:\ttest: 0.9556950\tbest: 0.9556950 (191)\ttotal: 8m 15s\tremaining: 34m 43s\n",
      "192:\ttest: 0.9556856\tbest: 0.9556950 (191)\ttotal: 8m 17s\tremaining: 34m 40s\n",
      "193:\ttest: 0.9556856\tbest: 0.9556950 (191)\ttotal: 8m 20s\tremaining: 34m 37s\n",
      "194:\ttest: 0.9556856\tbest: 0.9556950 (191)\ttotal: 8m 22s\tremaining: 34m 34s\n",
      "195:\ttest: 0.9558440\tbest: 0.9558440 (195)\ttotal: 8m 24s\tremaining: 34m 31s\n",
      "196:\ttest: 0.9558704\tbest: 0.9558704 (196)\ttotal: 8m 27s\tremaining: 34m 30s\n",
      "197:\ttest: 0.9558704\tbest: 0.9558704 (196)\ttotal: 8m 30s\tremaining: 34m 27s\n",
      "198:\ttest: 0.9558704\tbest: 0.9558704 (196)\ttotal: 8m 32s\tremaining: 34m 23s\n",
      "199:\ttest: 0.9558704\tbest: 0.9558704 (196)\ttotal: 8m 35s\tremaining: 34m 23s\n",
      "200:\ttest: 0.9558704\tbest: 0.9558704 (196)\ttotal: 8m 38s\tremaining: 34m 20s\n",
      "201:\ttest: 0.9558968\tbest: 0.9558968 (201)\ttotal: 8m 40s\tremaining: 34m 17s\n",
      "202:\ttest: 0.9558704\tbest: 0.9558968 (201)\ttotal: 8m 43s\tremaining: 34m 14s\n",
      "203:\ttest: 0.9561871\tbest: 0.9561871 (203)\ttotal: 8m 46s\tremaining: 34m 13s\n",
      "204:\ttest: 0.9562135\tbest: 0.9562135 (204)\ttotal: 8m 48s\tremaining: 34m 9s\n",
      "205:\ttest: 0.9562041\tbest: 0.9562135 (204)\ttotal: 8m 50s\tremaining: 34m 6s\n",
      "206:\ttest: 0.9561871\tbest: 0.9562135 (204)\ttotal: 8m 53s\tremaining: 34m 3s\n",
      "207:\ttest: 0.9561794\tbest: 0.9562135 (204)\ttotal: 8m 55s\tremaining: 34m\n",
      "208:\ttest: 0.9561794\tbest: 0.9562135 (204)\ttotal: 8m 58s\tremaining: 33m 57s\n",
      "209:\ttest: 0.9561266\tbest: 0.9562135 (204)\ttotal: 9m\tremaining: 33m 54s\n",
      "210:\ttest: 0.9561871\tbest: 0.9562135 (204)\ttotal: 9m 3s\tremaining: 33m 51s\n",
      "211:\ttest: 0.9561607\tbest: 0.9562135 (204)\ttotal: 9m 5s\tremaining: 33m 47s\n",
      "212:\ttest: 0.9561607\tbest: 0.9562135 (204)\ttotal: 9m 8s\tremaining: 33m 45s\n",
      "213:\ttest: 0.9561701\tbest: 0.9562135 (204)\ttotal: 9m 10s\tremaining: 33m 42s\n",
      "214:\ttest: 0.9562229\tbest: 0.9562229 (214)\ttotal: 9m 12s\tremaining: 33m 39s\n",
      "215:\ttest: 0.9562322\tbest: 0.9562322 (215)\ttotal: 9m 16s\tremaining: 33m 38s\n",
      "216:\ttest: 0.9562850\tbest: 0.9562850 (216)\ttotal: 9m 18s\tremaining: 33m 35s\n",
      "217:\ttest: 0.9563208\tbest: 0.9563208 (217)\ttotal: 9m 21s\tremaining: 33m 32s\n",
      "218:\ttest: 0.9563208\tbest: 0.9563208 (217)\ttotal: 9m 23s\tremaining: 33m 29s\n",
      "219:\ttest: 0.9563906\tbest: 0.9563906 (219)\ttotal: 9m 25s\tremaining: 33m 26s\n",
      "220:\ttest: 0.9563906\tbest: 0.9563906 (219)\ttotal: 9m 28s\tremaining: 33m 24s\n",
      "221:\ttest: 0.9563906\tbest: 0.9563906 (219)\ttotal: 9m 31s\tremaining: 33m 23s\n",
      "222:\ttest: 0.9563983\tbest: 0.9563983 (222)\ttotal: 9m 33s\tremaining: 33m 19s\n",
      "223:\ttest: 0.9565038\tbest: 0.9565038 (223)\ttotal: 9m 36s\tremaining: 33m 16s\n",
      "224:\ttest: 0.9565302\tbest: 0.9565302 (224)\ttotal: 9m 38s\tremaining: 33m 13s\n",
      "225:\ttest: 0.9565038\tbest: 0.9565302 (224)\ttotal: 9m 41s\tremaining: 33m 11s\n",
      "226:\ttest: 0.9565038\tbest: 0.9565302 (224)\ttotal: 9m 43s\tremaining: 33m 7s\n",
      "227:\ttest: 0.9565038\tbest: 0.9565302 (224)\ttotal: 9m 46s\tremaining: 33m 4s\n",
      "228:\ttest: 0.9565038\tbest: 0.9565302 (224)\ttotal: 9m 48s\tremaining: 33m 1s\n",
      "229:\ttest: 0.9565038\tbest: 0.9565302 (224)\ttotal: 9m 51s\tremaining: 32m 58s\n",
      "230:\ttest: 0.9565038\tbest: 0.9565302 (224)\ttotal: 9m 53s\tremaining: 32m 55s\n",
      "231:\ttest: 0.9565038\tbest: 0.9565302 (224)\ttotal: 9m 55s\tremaining: 32m 52s\n",
      "232:\ttest: 0.9565396\tbest: 0.9565396 (232)\ttotal: 9m 58s\tremaining: 32m 48s\n",
      "233:\ttest: 0.9565490\tbest: 0.9565490 (233)\ttotal: 10m\tremaining: 32m 45s\n",
      "234:\ttest: 0.9566017\tbest: 0.9566017 (234)\ttotal: 10m 3s\tremaining: 32m 43s\n",
      "235:\ttest: 0.9566545\tbest: 0.9566545 (235)\ttotal: 10m 5s\tremaining: 32m 40s\n",
      "236:\ttest: 0.9566545\tbest: 0.9566545 (235)\ttotal: 10m 8s\tremaining: 32m 37s\n",
      "237:\ttest: 0.9566545\tbest: 0.9566545 (235)\ttotal: 10m 10s\tremaining: 32m 35s\n",
      "238:\ttest: 0.9565566\tbest: 0.9566545 (235)\ttotal: 10m 13s\tremaining: 32m 34s\n",
      "239:\ttest: 0.9565302\tbest: 0.9566545 (235)\ttotal: 10m 16s\tremaining: 32m 31s\n",
      "240:\ttest: 0.9565643\tbest: 0.9566545 (235)\ttotal: 10m 18s\tremaining: 32m 27s\n",
      "241:\ttest: 0.9565115\tbest: 0.9566545 (235)\ttotal: 10m 20s\tremaining: 32m 24s\n",
      "242:\ttest: 0.9565115\tbest: 0.9566545 (235)\ttotal: 10m 23s\tremaining: 32m 21s\n",
      "243:\ttest: 0.9567167\tbest: 0.9567167 (243)\ttotal: 10m 25s\tremaining: 32m 19s\n",
      "244:\ttest: 0.9569806\tbest: 0.9569806 (244)\ttotal: 10m 28s\tremaining: 32m 16s\n",
      "245:\ttest: 0.9570070\tbest: 0.9570070 (245)\ttotal: 10m 31s\tremaining: 32m 14s\n",
      "246:\ttest: 0.9572897\tbest: 0.9572897 (246)\ttotal: 10m 33s\tremaining: 32m 12s\n",
      "247:\ttest: 0.9569994\tbest: 0.9572897 (246)\ttotal: 10m 36s\tremaining: 32m 10s\n",
      "248:\ttest: 0.9570164\tbest: 0.9572897 (246)\ttotal: 10m 38s\tremaining: 32m 6s\n",
      "249:\ttest: 0.9570070\tbest: 0.9572897 (246)\ttotal: 10m 41s\tremaining: 32m 5s\n",
      "250:\ttest: 0.9573161\tbest: 0.9573161 (250)\ttotal: 10m 44s\tremaining: 32m 3s\n",
      "251:\ttest: 0.9572275\tbest: 0.9573161 (250)\ttotal: 10m 46s\tremaining: 31m 59s\n",
      "252:\ttest: 0.9572539\tbest: 0.9573161 (250)\ttotal: 10m 49s\tremaining: 31m 56s\n",
      "253:\ttest: 0.9572633\tbest: 0.9573161 (250)\ttotal: 10m 52s\tremaining: 31m 55s\n",
      "254:\ttest: 0.9572633\tbest: 0.9573161 (250)\ttotal: 10m 54s\tremaining: 31m 52s\n",
      "255:\ttest: 0.9572369\tbest: 0.9573161 (250)\ttotal: 10m 57s\tremaining: 31m 50s\n",
      "256:\ttest: 0.9569994\tbest: 0.9573161 (250)\ttotal: 11m\tremaining: 31m 50s\n",
      "257:\ttest: 0.9570615\tbest: 0.9573161 (250)\ttotal: 11m 3s\tremaining: 31m 47s\n",
      "258:\ttest: 0.9570879\tbest: 0.9573161 (250)\ttotal: 11m 5s\tremaining: 31m 44s\n",
      "259:\ttest: 0.9573254\tbest: 0.9573254 (259)\ttotal: 11m 8s\tremaining: 31m 42s\n",
      "260:\ttest: 0.9573518\tbest: 0.9573518 (260)\ttotal: 11m 11s\tremaining: 31m 40s\n",
      "261:\ttest: 0.9574046\tbest: 0.9574046 (261)\ttotal: 11m 13s\tremaining: 31m 38s\n",
      "262:\ttest: 0.9572991\tbest: 0.9574046 (261)\ttotal: 11m 16s\tremaining: 31m 35s\n",
      "263:\ttest: 0.9572727\tbest: 0.9574046 (261)\ttotal: 11m 18s\tremaining: 31m 32s\n",
      "264:\ttest: 0.9572463\tbest: 0.9574046 (261)\ttotal: 11m 21s\tremaining: 31m 29s\n",
      "265:\ttest: 0.9572463\tbest: 0.9574046 (261)\ttotal: 11m 24s\tremaining: 31m 27s\n",
      "266:\ttest: 0.9574046\tbest: 0.9574046 (261)\ttotal: 11m 26s\tremaining: 31m 23s\n",
      "267:\ttest: 0.9574387\tbest: 0.9574387 (267)\ttotal: 11m 28s\tremaining: 31m 20s\n",
      "268:\ttest: 0.9574915\tbest: 0.9574915 (268)\ttotal: 11m 31s\tremaining: 31m 18s\n",
      "269:\ttest: 0.9575443\tbest: 0.9575443 (269)\ttotal: 11m 33s\tremaining: 31m 15s\n",
      "270:\ttest: 0.9575179\tbest: 0.9575443 (269)\ttotal: 11m 35s\tremaining: 31m 11s\n",
      "271:\ttest: 0.9576158\tbest: 0.9576158 (271)\ttotal: 11m 38s\tremaining: 31m 8s\n",
      "272:\ttest: 0.9576158\tbest: 0.9576158 (271)\ttotal: 11m 41s\tremaining: 31m 6s\n",
      "273:\ttest: 0.9579061\tbest: 0.9579061 (273)\ttotal: 11m 43s\tremaining: 31m 3s\n",
      "274:\ttest: 0.9579061\tbest: 0.9579061 (273)\ttotal: 11m 45s\tremaining: 31m 1s\n",
      "275:\ttest: 0.9579061\tbest: 0.9579061 (273)\ttotal: 11m 48s\tremaining: 30m 58s\n",
      "276:\ttest: 0.9577478\tbest: 0.9579061 (273)\ttotal: 11m 51s\tremaining: 30m 57s\n",
      "277:\ttest: 0.9577384\tbest: 0.9579061 (273)\ttotal: 11m 53s\tremaining: 30m 54s\n",
      "278:\ttest: 0.9578005\tbest: 0.9579061 (273)\ttotal: 11m 56s\tremaining: 30m 51s\n",
      "279:\ttest: 0.9578005\tbest: 0.9579061 (273)\ttotal: 11m 59s\tremaining: 30m 48s\n",
      "280:\ttest: 0.9581437\tbest: 0.9581437 (280)\ttotal: 12m 1s\tremaining: 30m 45s\n",
      "281:\ttest: 0.9580287\tbest: 0.9581437 (280)\ttotal: 12m 3s\tremaining: 30m 42s\n",
      "282:\ttest: 0.9579402\tbest: 0.9581437 (280)\ttotal: 12m 6s\tremaining: 30m 39s\n",
      "283:\ttest: 0.9579402\tbest: 0.9581437 (280)\ttotal: 12m 8s\tremaining: 30m 36s\n",
      "284:\ttest: 0.9579402\tbest: 0.9581437 (280)\ttotal: 12m 11s\tremaining: 30m 35s\n",
      "285:\ttest: 0.9580909\tbest: 0.9581437 (280)\ttotal: 12m 13s\tremaining: 30m 32s\n",
      "286:\ttest: 0.9581530\tbest: 0.9581530 (286)\ttotal: 12m 16s\tremaining: 30m 29s\n",
      "287:\ttest: 0.9580909\tbest: 0.9581530 (286)\ttotal: 12m 18s\tremaining: 30m 26s\n",
      "288:\ttest: 0.9581343\tbest: 0.9581530 (286)\ttotal: 12m 21s\tremaining: 30m 23s\n",
      "289:\ttest: 0.9581343\tbest: 0.9581530 (286)\ttotal: 12m 23s\tremaining: 30m 21s\n",
      "290:\ttest: 0.9581607\tbest: 0.9581607 (290)\ttotal: 12m 26s\tremaining: 30m 18s\n",
      "291:\ttest: 0.9581343\tbest: 0.9581607 (290)\ttotal: 12m 28s\tremaining: 30m 15s\n",
      "292:\ttest: 0.9581607\tbest: 0.9581607 (290)\ttotal: 12m 31s\tremaining: 30m 13s\n",
      "293:\ttest: 0.9581513\tbest: 0.9581607 (290)\ttotal: 12m 34s\tremaining: 30m 11s\n",
      "294:\ttest: 0.9581420\tbest: 0.9581607 (290)\ttotal: 12m 36s\tremaining: 30m 8s\n",
      "295:\ttest: 0.9583267\tbest: 0.9583267 (295)\ttotal: 12m 39s\tremaining: 30m 5s\n",
      "296:\ttest: 0.9585643\tbest: 0.9585643 (296)\ttotal: 12m 41s\tremaining: 30m 2s\n",
      "297:\ttest: 0.9585643\tbest: 0.9585643 (296)\ttotal: 12m 44s\tremaining: 29m 59s\n",
      "298:\ttest: 0.9585549\tbest: 0.9585643 (296)\ttotal: 12m 46s\tremaining: 29m 57s\n",
      "299:\ttest: 0.9586077\tbest: 0.9586077 (299)\ttotal: 12m 49s\tremaining: 29m 55s\n",
      "300:\ttest: 0.9586171\tbest: 0.9586171 (300)\ttotal: 12m 51s\tremaining: 29m 51s\n",
      "301:\ttest: 0.9585907\tbest: 0.9586171 (300)\ttotal: 12m 54s\tremaining: 29m 49s\n",
      "302:\ttest: 0.9587320\tbest: 0.9587320 (302)\ttotal: 12m 56s\tremaining: 29m 46s\n",
      "303:\ttest: 0.9586699\tbest: 0.9587320 (302)\ttotal: 12m 59s\tremaining: 29m 44s\n",
      "304:\ttest: 0.9588299\tbest: 0.9588299 (304)\ttotal: 13m 1s\tremaining: 29m 41s\n",
      "305:\ttest: 0.9588299\tbest: 0.9588299 (304)\ttotal: 13m 4s\tremaining: 29m 39s\n",
      "306:\ttest: 0.9586979\tbest: 0.9588299 (304)\ttotal: 13m 7s\tremaining: 29m 37s\n",
      "307:\ttest: 0.9586979\tbest: 0.9588299 (304)\ttotal: 13m 9s\tremaining: 29m 34s\n",
      "308:\ttest: 0.9588410\tbest: 0.9588410 (308)\ttotal: 13m 12s\tremaining: 29m 31s\n",
      "309:\ttest: 0.9588410\tbest: 0.9588410 (308)\ttotal: 13m 15s\tremaining: 29m 29s\n",
      "310:\ttest: 0.9588844\tbest: 0.9588844 (310)\ttotal: 13m 17s\tremaining: 29m 26s\n",
      "311:\ttest: 0.9589559\tbest: 0.9589559 (311)\ttotal: 13m 19s\tremaining: 29m 23s\n",
      "312:\ttest: 0.9589202\tbest: 0.9589559 (311)\ttotal: 13m 22s\tremaining: 29m 20s\n",
      "313:\ttest: 0.9589466\tbest: 0.9589559 (311)\ttotal: 13m 24s\tremaining: 29m 17s\n",
      "314:\ttest: 0.9589559\tbest: 0.9589559 (311)\ttotal: 13m 26s\tremaining: 29m 14s\n",
      "315:\ttest: 0.9589823\tbest: 0.9589823 (315)\ttotal: 13m 29s\tremaining: 29m 11s\n",
      "316:\ttest: 0.9589823\tbest: 0.9589823 (315)\ttotal: 13m 31s\tremaining: 29m 8s\n",
      "317:\ttest: 0.9589917\tbest: 0.9589917 (317)\ttotal: 13m 33s\tremaining: 29m 5s\n",
      "318:\ttest: 0.9590181\tbest: 0.9590181 (318)\ttotal: 13m 36s\tremaining: 29m 2s\n",
      "319:\ttest: 0.9590181\tbest: 0.9590181 (318)\ttotal: 13m 38s\tremaining: 28m 59s\n",
      "320:\ttest: 0.9591330\tbest: 0.9591330 (320)\ttotal: 13m 40s\tremaining: 28m 56s\n",
      "321:\ttest: 0.9588861\tbest: 0.9591330 (320)\ttotal: 13m 43s\tremaining: 28m 53s\n",
      "322:\ttest: 0.9588691\tbest: 0.9591330 (320)\ttotal: 13m 45s\tremaining: 28m 50s\n",
      "323:\ttest: 0.9588427\tbest: 0.9591330 (320)\ttotal: 13m 48s\tremaining: 28m 47s\n",
      "324:\ttest: 0.9588691\tbest: 0.9591330 (320)\ttotal: 13m 50s\tremaining: 28m 44s\n",
      "325:\ttest: 0.9588333\tbest: 0.9591330 (320)\ttotal: 13m 52s\tremaining: 28m 41s\n",
      "326:\ttest: 0.9588503\tbest: 0.9591330 (320)\ttotal: 13m 55s\tremaining: 28m 39s\n",
      "327:\ttest: 0.9588503\tbest: 0.9591330 (320)\ttotal: 13m 57s\tremaining: 28m 36s\n",
      "328:\ttest: 0.9588427\tbest: 0.9591330 (320)\ttotal: 14m\tremaining: 28m 33s\n",
      "329:\ttest: 0.9588427\tbest: 0.9591330 (320)\ttotal: 14m 3s\tremaining: 28m 32s\n",
      "330:\ttest: 0.9591066\tbest: 0.9591330 (320)\ttotal: 14m 5s\tremaining: 28m 29s\n",
      "331:\ttest: 0.9590879\tbest: 0.9591330 (320)\ttotal: 14m 7s\tremaining: 28m 26s\n",
      "332:\ttest: 0.9591424\tbest: 0.9591424 (332)\ttotal: 14m 10s\tremaining: 28m 24s\n",
      "333:\ttest: 0.9591066\tbest: 0.9591424 (332)\ttotal: 14m 13s\tremaining: 28m 21s\n",
      "334:\ttest: 0.9590802\tbest: 0.9591424 (332)\ttotal: 14m 15s\tremaining: 28m 18s\n",
      "335:\ttest: 0.9590879\tbest: 0.9591424 (332)\ttotal: 14m 18s\tremaining: 28m 16s\n",
      "336:\ttest: 0.9590879\tbest: 0.9591424 (332)\ttotal: 14m 20s\tremaining: 28m 13s\n",
      "337:\ttest: 0.9590879\tbest: 0.9591424 (332)\ttotal: 14m 23s\tremaining: 28m 11s\n",
      "338:\ttest: 0.9590973\tbest: 0.9591424 (332)\ttotal: 14m 25s\tremaining: 28m 8s\n",
      "339:\ttest: 0.9591671\tbest: 0.9591671 (339)\ttotal: 14m 27s\tremaining: 28m 4s\n",
      "340:\ttest: 0.9591407\tbest: 0.9591671 (339)\ttotal: 14m 30s\tremaining: 28m 2s\n",
      "341:\ttest: 0.9591143\tbest: 0.9591671 (339)\ttotal: 14m 33s\tremaining: 27m 59s\n",
      "342:\ttest: 0.9591935\tbest: 0.9591935 (342)\ttotal: 14m 35s\tremaining: 27m 57s\n",
      "343:\ttest: 0.9591841\tbest: 0.9591935 (342)\ttotal: 14m 38s\tremaining: 27m 54s\n",
      "344:\ttest: 0.9591841\tbest: 0.9591935 (342)\ttotal: 14m 41s\tremaining: 27m 53s\n",
      "345:\ttest: 0.9591313\tbest: 0.9591935 (342)\ttotal: 14m 43s\tremaining: 27m 50s\n",
      "346:\ttest: 0.9591577\tbest: 0.9591935 (342)\ttotal: 14m 46s\tremaining: 27m 47s\n",
      "347:\ttest: 0.9591671\tbest: 0.9591935 (342)\ttotal: 14m 48s\tremaining: 27m 45s\n",
      "348:\ttest: 0.9591671\tbest: 0.9591935 (342)\ttotal: 14m 51s\tremaining: 27m 42s\n",
      "349:\ttest: 0.9591407\tbest: 0.9591935 (342)\ttotal: 14m 54s\tremaining: 27m 41s\n",
      "350:\ttest: 0.9591407\tbest: 0.9591935 (342)\ttotal: 14m 56s\tremaining: 27m 38s\n",
      "351:\ttest: 0.9591407\tbest: 0.9591935 (342)\ttotal: 14m 59s\tremaining: 27m 35s\n",
      "352:\ttest: 0.9591841\tbest: 0.9591935 (342)\ttotal: 15m 1s\tremaining: 27m 32s\n",
      "353:\ttest: 0.9591577\tbest: 0.9591935 (342)\ttotal: 15m 4s\tremaining: 27m 29s\n",
      "354:\ttest: 0.9591577\tbest: 0.9591935 (342)\ttotal: 15m 6s\tremaining: 27m 27s\n",
      "355:\ttest: 0.9590428\tbest: 0.9591935 (342)\ttotal: 15m 9s\tremaining: 27m 24s\n",
      "356:\ttest: 0.9590521\tbest: 0.9591935 (342)\ttotal: 15m 11s\tremaining: 27m 21s\n",
      "357:\ttest: 0.9590785\tbest: 0.9591935 (342)\ttotal: 15m 14s\tremaining: 27m 19s\n",
      "358:\ttest: 0.9591841\tbest: 0.9591935 (342)\ttotal: 15m 16s\tremaining: 27m 16s\n",
      "359:\ttest: 0.9593254\tbest: 0.9593254 (359)\ttotal: 15m 18s\tremaining: 27m 13s\n",
      "360:\ttest: 0.9593254\tbest: 0.9593254 (359)\ttotal: 15m 21s\tremaining: 27m 10s\n",
      "361:\ttest: 0.9593518\tbest: 0.9593518 (361)\ttotal: 15m 24s\tremaining: 27m 8s\n",
      "362:\ttest: 0.9593782\tbest: 0.9593782 (362)\ttotal: 15m 26s\tremaining: 27m 6s\n",
      "363:\ttest: 0.9593782\tbest: 0.9593782 (362)\ttotal: 15m 29s\tremaining: 27m 3s\n",
      "364:\ttest: 0.9592897\tbest: 0.9593782 (362)\ttotal: 15m 32s\tremaining: 27m 1s\n",
      "365:\ttest: 0.9592897\tbest: 0.9593782 (362)\ttotal: 15m 34s\tremaining: 26m 58s\n",
      "366:\ttest: 0.9592726\tbest: 0.9593782 (362)\ttotal: 15m 36s\tremaining: 26m 55s\n",
      "367:\ttest: 0.9592633\tbest: 0.9593782 (362)\ttotal: 15m 39s\tremaining: 26m 52s\n",
      "368:\ttest: 0.9592633\tbest: 0.9593782 (362)\ttotal: 15m 41s\tremaining: 26m 50s\n",
      "369:\ttest: 0.9592633\tbest: 0.9593782 (362)\ttotal: 15m 44s\tremaining: 26m 48s\n",
      "370:\ttest: 0.9592633\tbest: 0.9593782 (362)\ttotal: 15m 47s\tremaining: 26m 46s\n",
      "371:\ttest: 0.9592897\tbest: 0.9593782 (362)\ttotal: 15m 49s\tremaining: 26m 43s\n",
      "372:\ttest: 0.9592897\tbest: 0.9593782 (362)\ttotal: 15m 52s\tremaining: 26m 41s\n",
      "373:\ttest: 0.9591577\tbest: 0.9593782 (362)\ttotal: 15m 55s\tremaining: 26m 39s\n",
      "374:\ttest: 0.9590862\tbest: 0.9593782 (362)\ttotal: 15m 57s\tremaining: 26m 36s\n",
      "375:\ttest: 0.9589806\tbest: 0.9593782 (362)\ttotal: 16m\tremaining: 26m 34s\n",
      "376:\ttest: 0.9589900\tbest: 0.9593782 (362)\ttotal: 16m 3s\tremaining: 26m 31s\n",
      "377:\ttest: 0.9589900\tbest: 0.9593782 (362)\ttotal: 16m 5s\tremaining: 26m 28s\n",
      "378:\ttest: 0.9590428\tbest: 0.9593782 (362)\ttotal: 16m 7s\tremaining: 26m 25s\n",
      "379:\ttest: 0.9590428\tbest: 0.9593782 (362)\ttotal: 16m 10s\tremaining: 26m 23s\n",
      "380:\ttest: 0.9590428\tbest: 0.9593782 (362)\ttotal: 16m 12s\tremaining: 26m 20s\n",
      "381:\ttest: 0.9590521\tbest: 0.9593782 (362)\ttotal: 16m 15s\tremaining: 26m 18s\n",
      "382:\ttest: 0.9590785\tbest: 0.9593782 (362)\ttotal: 16m 17s\tremaining: 26m 15s\n",
      "383:\ttest: 0.9590070\tbest: 0.9593782 (362)\ttotal: 16m 20s\tremaining: 26m 12s\n",
      "384:\ttest: 0.9589976\tbest: 0.9593782 (362)\ttotal: 16m 22s\tremaining: 26m 9s\n",
      "385:\ttest: 0.9589976\tbest: 0.9593782 (362)\ttotal: 16m 25s\tremaining: 26m 6s\n",
      "386:\ttest: 0.9589976\tbest: 0.9593782 (362)\ttotal: 16m 27s\tremaining: 26m 3s\n",
      "387:\ttest: 0.9589976\tbest: 0.9593782 (362)\ttotal: 16m 29s\tremaining: 26m 1s\n",
      "388:\ttest: 0.9589976\tbest: 0.9593782 (362)\ttotal: 16m 32s\tremaining: 25m 59s\n",
      "389:\ttest: 0.9589976\tbest: 0.9593782 (362)\ttotal: 16m 35s\tremaining: 25m 56s\n",
      "390:\ttest: 0.9589976\tbest: 0.9593782 (362)\ttotal: 16m 38s\tremaining: 25m 54s\n",
      "391:\ttest: 0.9590257\tbest: 0.9593782 (362)\ttotal: 16m 40s\tremaining: 25m 52s\n",
      "392:\ttest: 0.9590087\tbest: 0.9593782 (362)\ttotal: 16m 43s\tremaining: 25m 50s\n",
      "393:\ttest: 0.9589823\tbest: 0.9593782 (362)\ttotal: 16m 46s\tremaining: 25m 48s\n",
      "394:\ttest: 0.9589823\tbest: 0.9593782 (362)\ttotal: 16m 49s\tremaining: 25m 46s\n",
      "395:\ttest: 0.9589823\tbest: 0.9593782 (362)\ttotal: 16m 51s\tremaining: 25m 42s\n",
      "396:\ttest: 0.9589823\tbest: 0.9593782 (362)\ttotal: 16m 54s\tremaining: 25m 40s\n",
      "397:\ttest: 0.9589031\tbest: 0.9593782 (362)\ttotal: 16m 56s\tremaining: 25m 38s\n",
      "398:\ttest: 0.9589202\tbest: 0.9593782 (362)\ttotal: 17m\tremaining: 25m 36s\n",
      "399:\ttest: 0.9590521\tbest: 0.9593782 (362)\ttotal: 17m 2s\tremaining: 25m 34s\n",
      "400:\ttest: 0.9590521\tbest: 0.9593782 (362)\ttotal: 17m 5s\tremaining: 25m 31s\n",
      "401:\ttest: 0.9591313\tbest: 0.9593782 (362)\ttotal: 17m 7s\tremaining: 25m 28s\n",
      "402:\ttest: 0.9591577\tbest: 0.9593782 (362)\ttotal: 17m 10s\tremaining: 25m 26s\n",
      "403:\ttest: 0.9591577\tbest: 0.9593782 (362)\ttotal: 17m 12s\tremaining: 25m 23s\n",
      "404:\ttest: 0.9590521\tbest: 0.9593782 (362)\ttotal: 17m 14s\tremaining: 25m 20s\n",
      "405:\ttest: 0.9590257\tbest: 0.9593782 (362)\ttotal: 17m 17s\tremaining: 25m 17s\n",
      "406:\ttest: 0.9590257\tbest: 0.9593782 (362)\ttotal: 17m 19s\tremaining: 25m 14s\n",
      "407:\ttest: 0.9590692\tbest: 0.9593782 (362)\ttotal: 17m 22s\tremaining: 25m 11s\n",
      "408:\ttest: 0.9590692\tbest: 0.9593782 (362)\ttotal: 17m 24s\tremaining: 25m 9s\n",
      "409:\ttest: 0.9590692\tbest: 0.9593782 (362)\ttotal: 17m 27s\tremaining: 25m 7s\n",
      "410:\ttest: 0.9590428\tbest: 0.9593782 (362)\ttotal: 17m 30s\tremaining: 25m 5s\n",
      "411:\ttest: 0.9590615\tbest: 0.9593782 (362)\ttotal: 17m 32s\tremaining: 25m 2s\n",
      "412:\ttest: 0.9590615\tbest: 0.9593782 (362)\ttotal: 17m 35s\tremaining: 24m 59s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.9593782253\n",
      "bestIteration = 362\n",
      "\n",
      "Shrink model to first 363 iterations.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluation Metrics (Validation Set):\n",
      "Accuracy@1 : 0.9010\n",
      "Recall@3   : 0.9961\n",
      "MRR        : 0.9473\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Catboost has a slight edge here. Lets see how it performs on the complex set."
   ],
   "metadata": {
    "id": "i5xIJuHr76e1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Complex Set\n",
    "\n",
    "Now we will tune the complex model as well. We will delete the previous data to save on RAM first."
   ],
   "metadata": {
    "id": "KxNwI54-7Z4y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Delete training-related objects\n",
    "del full_df, X_train, y_train, train_group_sizes\n",
    "del val_df, X_val, y_val, val_group_sizes\n",
    "\n",
    "# Run garbage collection\n",
    "gc.collect()"
   ],
   "metadata": {
    "id": "t1xt9MMoJUrr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Get new ids\n",
    "train_ids_path = \"/content/train_com_ids.csv\"\n",
    "val_ids_path = \"/content/validation_com_ids.csv\"\n",
    "train_ids = pd.read_csv(train_ids_path)[\"train_ids\"].dropna().astype(int).tolist()\n",
    "val_ids = pd.read_csv(val_ids_path)[\"validation_ids\"].dropna().astype(int).tolist()\n",
    "\n",
    "# Reconnect and load the second dataset\n",
    "conn = sqlite3.connect(db_path)\n",
    "full_df = pd.read_sql_query(\"SELECT * FROM feature_matrix_complex\", conn)\n",
    "val_ids_set = set(val_ids)\n",
    "val_df = full_df[full_df[\"clean_row_id\"].isin(val_ids_set)]\n",
    "full_df = full_df[~full_df[\"clean_row_id\"].isin(val_ids_set)]\n",
    "\n",
    "\n",
    "# Rebuild train/val splits\n",
    "X_train = full_df.drop(\n",
    "    columns=[\"label\", \"clean_row_id\", \"investor\", \"firm\", \"template_id\"]\n",
    ")\n",
    "y_train = full_df[\"label\"]\n",
    "train_group_sizes = full_df.groupby(\"clean_row_id\").size().tolist()\n",
    "X_val = val_df.drop(\n",
    "    columns=[\"label\", \"clean_row_id\", \"investor\", \"firm\", \"template_id\"]\n",
    ")\n",
    "y_val = val_df[\"label\"]\n",
    "val_group_sizes = val_df.groupby(\"clean_row_id\").size().tolist()"
   ],
   "metadata": {
    "id": "La1zyOWq7d-l"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets re run the full sweep."
   ],
   "metadata": {
    "id": "B7CnraKw8AMo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Run the sweep\n",
    "print(\"Running CatBoost tuning...\")\n",
    "catboost_study = optuna.create_study(direction=\"minimize\", study_name=\"catboost\")\n",
    "catboost_study.optimize(catboost_objective, n_trials=50)\n",
    "# Save to drive\n",
    "with open(\"/content/catboost_com_best.json\", \"w\") as f:\n",
    "    json.dump(catboost_study.best_params, f)"
   ],
   "metadata": {
    "id": "fj1kbEWU8Dae",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8541cd1a-612d-4c5d-e652-9476524cd04f"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-07-16 17:29:25,081] A new study created in memory with name: catboost\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running CatBoost tuning...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 17:36:20,396] Trial 0 finished with value: 0.016249999999999987 and parameters: {'learning_rate': 0.16997943397292264, 'depth': 9, 'l2_leaf_reg': 4.325054229205765, 'random_strength': 3.195199898203583, 'min_data_in_leaf': 70, 'subsample': 0.8460831562701309, 'colsample_bylevel': 0.9935706982846684, 'grow_policy': 'Lossguide'}. Best is trial 0 with value: 0.016249999999999987.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 17:46:37,382] Trial 1 finished with value: 0.018750000000000044 and parameters: {'learning_rate': 0.1337634400131964, 'depth': 9, 'l2_leaf_reg': 1.8495819339681328, 'random_strength': 9.296981036666262, 'min_data_in_leaf': 46, 'subsample': 0.6839336909945267, 'colsample_bylevel': 0.6510233092709485, 'grow_policy': 'Lossguide'}. Best is trial 0 with value: 0.016249999999999987.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 17:48:21,035] Trial 2 finished with value: 0.06374999999999997 and parameters: {'learning_rate': 0.1669698644218351, 'depth': 4, 'l2_leaf_reg': 5.729087340586366, 'random_strength': 9.025774084434339, 'min_data_in_leaf': 63, 'subsample': 0.5658213591578048, 'colsample_bylevel': 0.9824575791889256, 'grow_policy': 'SymmetricTree'}. Best is trial 0 with value: 0.016249999999999987.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 17:51:23,038] Trial 3 finished with value: 0.043749999999999956 and parameters: {'learning_rate': 0.027648927180806943, 'depth': 5, 'l2_leaf_reg': 1.481196110764879, 'random_strength': 8.304217383795272, 'min_data_in_leaf': 80, 'subsample': 0.6136562354464528, 'colsample_bylevel': 0.7402133860854778, 'grow_policy': 'SymmetricTree'}. Best is trial 0 with value: 0.016249999999999987.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 18:06:09,626] Trial 4 finished with value: 0.016249999999999987 and parameters: {'learning_rate': 0.0903812901843815, 'depth': 8, 'l2_leaf_reg': 4.276664458101944, 'random_strength': 2.3676741563931127, 'min_data_in_leaf': 84, 'subsample': 0.8872009295229518, 'colsample_bylevel': 0.6119569037806711, 'grow_policy': 'Lossguide'}. Best is trial 0 with value: 0.016249999999999987.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 18:11:21,867] Trial 5 finished with value: 0.02749999999999997 and parameters: {'learning_rate': 0.18678942831537096, 'depth': 4, 'l2_leaf_reg': 2.7964303619456126, 'random_strength': 0.4562559347322792, 'min_data_in_leaf': 89, 'subsample': 0.5099689052949166, 'colsample_bylevel': 0.5590287896999044, 'grow_policy': 'SymmetricTree'}. Best is trial 0 with value: 0.016249999999999987.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 18:19:16,388] Trial 6 finished with value: 0.016249999999999987 and parameters: {'learning_rate': 0.17075457326508622, 'depth': 9, 'l2_leaf_reg': 7.5553868333979946, 'random_strength': 8.35225469580508, 'min_data_in_leaf': 42, 'subsample': 0.9565476575505109, 'colsample_bylevel': 0.8998264333516923, 'grow_policy': 'Lossguide'}. Best is trial 0 with value: 0.016249999999999987.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 18:24:31,614] Trial 7 finished with value: 0.03249999999999997 and parameters: {'learning_rate': 0.028285871889386166, 'depth': 9, 'l2_leaf_reg': 1.6871344019966314, 'random_strength': 3.4372916769719577, 'min_data_in_leaf': 65, 'subsample': 0.808336699379898, 'colsample_bylevel': 0.76199762385597, 'grow_policy': 'SymmetricTree'}. Best is trial 0 with value: 0.016249999999999987.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 18:27:53,027] Trial 8 finished with value: 0.03749999999999998 and parameters: {'learning_rate': 0.03289172494198587, 'depth': 8, 'l2_leaf_reg': 7.9475470986258125, 'random_strength': 6.137717485777675, 'min_data_in_leaf': 16, 'subsample': 0.9187031365444658, 'colsample_bylevel': 0.5942517386850497, 'grow_policy': 'SymmetricTree'}. Best is trial 0 with value: 0.016249999999999987.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 18:32:27,692] Trial 9 finished with value: 0.04125000000000001 and parameters: {'learning_rate': 0.03756458848248146, 'depth': 10, 'l2_leaf_reg': 8.305182943508752, 'random_strength': 9.657380307302985, 'min_data_in_leaf': 12, 'subsample': 0.9740533622840615, 'colsample_bylevel': 0.7472588369577438, 'grow_policy': 'SymmetricTree'}. Best is trial 0 with value: 0.016249999999999987.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 18:40:42,372] Trial 10 finished with value: 0.025000000000000022 and parameters: {'learning_rate': 0.11208285415455121, 'depth': 6, 'l2_leaf_reg': 5.239302165756724, 'random_strength': 4.9626808008353045, 'min_data_in_leaf': 98, 'subsample': 0.7856052030676686, 'colsample_bylevel': 0.9978129225289238, 'grow_policy': 'Lossguide'}. Best is trial 0 with value: 0.016249999999999987.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 18:42:15,372] Trial 11 finished with value: 0.08499999999999996 and parameters: {'learning_rate': 0.07250928133904695, 'depth': 7, 'l2_leaf_reg': 4.21640186137329, 'random_strength': 1.7857374557188477, 'min_data_in_leaf': 76, 'subsample': 0.871178628143268, 'colsample_bylevel': 0.8358930317907277, 'grow_policy': 'Lossguide'}. Best is trial 0 with value: 0.016249999999999987.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 18:51:22,491] Trial 12 finished with value: 0.02124999999999999 and parameters: {'learning_rate': 0.08118911122943571, 'depth': 7, 'l2_leaf_reg': 4.446533168825782, 'random_strength': 2.902877610031589, 'min_data_in_leaf': 75, 'subsample': 0.8647578457585376, 'colsample_bylevel': 0.5411036741676964, 'grow_policy': 'Lossguide'}. Best is trial 0 with value: 0.016249999999999987.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 18:58:56,071] Trial 13 finished with value: 0.015000000000000013 and parameters: {'learning_rate': 0.1329416174119581, 'depth': 8, 'l2_leaf_reg': 6.628807908190987, 'random_strength': 4.881288964388853, 'min_data_in_leaf': 100, 'subsample': 0.708915364734102, 'colsample_bylevel': 0.6444718826631752, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 19:01:08,413] Trial 14 finished with value: 0.10124999999999995 and parameters: {'learning_rate': 0.1437899463010917, 'depth': 10, 'l2_leaf_reg': 9.684293986066876, 'random_strength': 5.775314061279012, 'min_data_in_leaf': 98, 'subsample': 0.6980803182634413, 'colsample_bylevel': 0.6777237524553046, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 19:08:46,787] Trial 15 finished with value: 0.020000000000000018 and parameters: {'learning_rate': 0.13298809821775684, 'depth': 8, 'l2_leaf_reg': 6.565774692784751, 'random_strength': 4.008739507007489, 'min_data_in_leaf': 27, 'subsample': 0.7235065044751605, 'colsample_bylevel': 0.8896354634513152, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 19:16:37,568] Trial 16 finished with value: 0.015000000000000013 and parameters: {'learning_rate': 0.19845196062507392, 'depth': 8, 'l2_leaf_reg': 3.047048725320599, 'random_strength': 6.640148829311792, 'min_data_in_leaf': 54, 'subsample': 0.8031847800758722, 'colsample_bylevel': 0.5019690050178204, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 19:20:58,373] Trial 17 finished with value: 0.02375000000000005 and parameters: {'learning_rate': 0.18804539345967222, 'depth': 7, 'l2_leaf_reg': 2.933979404387594, 'random_strength': 6.862142132228012, 'min_data_in_leaf': 44, 'subsample': 0.640282868518409, 'colsample_bylevel': 0.5121411595402561, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 19:27:16,805] Trial 18 finished with value: 0.028750000000000053 and parameters: {'learning_rate': 0.1101457518990312, 'depth': 6, 'l2_leaf_reg': 6.4535365234324455, 'random_strength': 7.2405934689578615, 'min_data_in_leaf': 54, 'subsample': 0.7676862863738039, 'colsample_bylevel': 0.6794649676671648, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 19:37:46,310] Trial 19 finished with value: 0.025000000000000022 and parameters: {'learning_rate': 0.06159400323309709, 'depth': 6, 'l2_leaf_reg': 3.0769147102191847, 'random_strength': 4.88039026946018, 'min_data_in_leaf': 34, 'subsample': 0.6635315361605596, 'colsample_bylevel': 0.5053555098489295, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 19:44:10,693] Trial 20 finished with value: 0.018750000000000044 and parameters: {'learning_rate': 0.19676877486207567, 'depth': 8, 'l2_leaf_reg': 9.526453571399351, 'random_strength': 7.317076658477312, 'min_data_in_leaf': 58, 'subsample': 0.73669446116934, 'colsample_bylevel': 0.6158859115408809, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 19:54:09,214] Trial 21 finished with value: 0.016249999999999987 and parameters: {'learning_rate': 0.15831741556835827, 'depth': 9, 'l2_leaf_reg': 3.807536388933876, 'random_strength': 4.344643166305948, 'min_data_in_leaf': 69, 'subsample': 0.818807073931413, 'colsample_bylevel': 0.8283840293843212, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 20:00:33,515] Trial 22 finished with value: 0.022499999999999964 and parameters: {'learning_rate': 0.15134423446694717, 'depth': 10, 'l2_leaf_reg': 5.231538741363011, 'random_strength': 5.998008576906265, 'min_data_in_leaf': 53, 'subsample': 0.8339443141912178, 'colsample_bylevel': 0.9420045418303832, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 20:06:25,156] Trial 23 finished with value: 0.022499999999999964 and parameters: {'learning_rate': 0.17784709324562403, 'depth': 8, 'l2_leaf_reg': 6.626142414609591, 'random_strength': 1.706385563957249, 'min_data_in_leaf': 89, 'subsample': 0.7540476203180968, 'colsample_bylevel': 0.6982481039391963, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 20:13:35,240] Trial 24 finished with value: 0.018750000000000044 and parameters: {'learning_rate': 0.1992129892934677, 'depth': 9, 'l2_leaf_reg': 3.463193785408993, 'random_strength': 3.59694851714651, 'min_data_in_leaf': 69, 'subsample': 0.9297650018805467, 'colsample_bylevel': 0.7963456119889593, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 20:19:15,120] Trial 25 finished with value: 0.025000000000000022 and parameters: {'learning_rate': 0.14190140146376884, 'depth': 7, 'l2_leaf_reg': 2.488924519773674, 'random_strength': 5.306203987656905, 'min_data_in_leaf': 33, 'subsample': 0.839331809407632, 'colsample_bylevel': 0.5513657299093746, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 20:25:14,710] Trial 26 finished with value: 0.02124999999999999 and parameters: {'learning_rate': 0.12224339125577581, 'depth': 8, 'l2_leaf_reg': 5.916202353416509, 'random_strength': 0.14479678281650532, 'min_data_in_leaf': 91, 'subsample': 0.7932325049065627, 'colsample_bylevel': 0.5865849863632192, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 20:34:41,018] Trial 27 finished with value: 0.01749999999999996 and parameters: {'learning_rate': 0.15902683332715645, 'depth': 9, 'l2_leaf_reg': 4.670390018240075, 'random_strength': 6.5625208010808755, 'min_data_in_leaf': 60, 'subsample': 0.7155655130476399, 'colsample_bylevel': 0.6423382249883346, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 20:43:12,013] Trial 28 finished with value: 0.018750000000000044 and parameters: {'learning_rate': 0.17839870880468972, 'depth': 10, 'l2_leaf_reg': 2.4078069001238194, 'random_strength': 7.966628251304437, 'min_data_in_leaf': 49, 'subsample': 0.6211999823302238, 'colsample_bylevel': 0.7141141328949283, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 20:44:42,774] Trial 29 finished with value: 0.08750000000000002 and parameters: {'learning_rate': 0.12272589576878876, 'depth': 9, 'l2_leaf_reg': 7.2790077559026605, 'random_strength': 4.395050420171177, 'min_data_in_leaf': 37, 'subsample': 0.6638698137462474, 'colsample_bylevel': 0.6571923402205813, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 20:52:22,367] Trial 30 finished with value: 0.020000000000000018 and parameters: {'learning_rate': 0.1319681919119058, 'depth': 7, 'l2_leaf_reg': 8.904823069799626, 'random_strength': 2.828194587833783, 'min_data_in_leaf': 70, 'subsample': 0.8969814883756564, 'colsample_bylevel': 0.864927237099795, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 21:00:41,625] Trial 31 finished with value: 0.022499999999999964 and parameters: {'learning_rate': 0.09073749045373579, 'depth': 8, 'l2_leaf_reg': 3.73464874749676, 'random_strength': 2.8836212274725996, 'min_data_in_leaf': 83, 'subsample': 0.8684439555437664, 'colsample_bylevel': 0.6222562169863813, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 21:03:23,746] Trial 32 finished with value: 0.05249999999999999 and parameters: {'learning_rate': 0.09756294450632219, 'depth': 8, 'l2_leaf_reg': 4.923903948033927, 'random_strength': 1.7195262957161286, 'min_data_in_leaf': 83, 'subsample': 0.907883804382575, 'colsample_bylevel': 0.5831936097505986, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 21:04:56,936] Trial 33 finished with value: 0.10250000000000004 and parameters: {'learning_rate': 0.04710855716264213, 'depth': 9, 'l2_leaf_reg': 1.0396132325250353, 'random_strength': 2.3505207375393815, 'min_data_in_leaf': 100, 'subsample': 0.771472032104086, 'colsample_bylevel': 0.957529159610082, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 21:10:25,997] Trial 34 finished with value: 0.02375000000000005 and parameters: {'learning_rate': 0.1684575760180427, 'depth': 8, 'l2_leaf_reg': 5.569291679183943, 'random_strength': 0.5933663685477695, 'min_data_in_leaf': 93, 'subsample': 0.8545087226200869, 'colsample_bylevel': 0.5372164873268269, 'grow_policy': 'SymmetricTree'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 21:16:14,797] Trial 35 finished with value: 0.020000000000000018 and parameters: {'learning_rate': 0.18654084532987564, 'depth': 7, 'l2_leaf_reg': 5.917392060115683, 'random_strength': 3.606724399426806, 'min_data_in_leaf': 78, 'subsample': 0.9496639384400926, 'colsample_bylevel': 0.632391257348677, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 21:20:14,835] Trial 36 finished with value: 0.03125 and parameters: {'learning_rate': 0.012105363142883213, 'depth': 8, 'l2_leaf_reg': 4.1001847910196165, 'random_strength': 1.0095609477614973, 'min_data_in_leaf': 83, 'subsample': 0.9961923084525511, 'colsample_bylevel': 0.790037160583857, 'grow_policy': 'SymmetricTree'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 21:28:38,666] Trial 37 finished with value: 0.015000000000000013 and parameters: {'learning_rate': 0.10014647568834933, 'depth': 9, 'l2_leaf_reg': 3.3146633934057133, 'random_strength': 5.540247468169484, 'min_data_in_leaf': 88, 'subsample': 0.5305599017517209, 'colsample_bylevel': 0.5946931609815392, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 21:36:18,671] Trial 38 finished with value: 0.02124999999999999 and parameters: {'learning_rate': 0.16123782308868068, 'depth': 10, 'l2_leaf_reg': 2.385960260214877, 'random_strength': 5.366609931714747, 'min_data_in_leaf': 95, 'subsample': 0.5555163775165846, 'colsample_bylevel': 0.5248830367826037, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 21:41:42,314] Trial 39 finished with value: 0.025000000000000022 and parameters: {'learning_rate': 0.11767088178936412, 'depth': 9, 'l2_leaf_reg': 3.473841151420566, 'random_strength': 7.841970679715235, 'min_data_in_leaf': 87, 'subsample': 0.5110986269639535, 'colsample_bylevel': 0.581777255723324, 'grow_policy': 'SymmetricTree'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 21:54:41,898] Trial 40 finished with value: 0.020000000000000018 and parameters: {'learning_rate': 0.10284534610159342, 'depth': 4, 'l2_leaf_reg': 1.8311766264382174, 'random_strength': 8.893911397109939, 'min_data_in_leaf': 62, 'subsample': 0.5626328905961246, 'colsample_bylevel': 0.5634608110141399, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 21:56:31,872] Trial 41 finished with value: 0.07374999999999998 and parameters: {'learning_rate': 0.0872639531611695, 'depth': 8, 'l2_leaf_reg': 3.2654884998261413, 'random_strength': 6.369941775046787, 'min_data_in_leaf': 86, 'subsample': 0.8083540857710483, 'colsample_bylevel': 0.6095916581942283, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:11:24,403] Trial 42 finished with value: 0.020000000000000018 and parameters: {'learning_rate': 0.07432551680394103, 'depth': 9, 'l2_leaf_reg': 4.6952778973045906, 'random_strength': 4.553173228482615, 'min_data_in_leaf': 75, 'subsample': 0.8944407469207548, 'colsample_bylevel': 0.7297678889443948, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:12:47,502] Trial 43 finished with value: 0.04249999999999998 and parameters: {'learning_rate': 0.06428167257680474, 'depth': 9, 'l2_leaf_reg': 4.072527330451358, 'random_strength': 5.48871666243431, 'min_data_in_leaf': 95, 'subsample': 0.8855558927362973, 'colsample_bylevel': 0.6679456105861908, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:18:57,609] Trial 44 finished with value: 0.02749999999999997 and parameters: {'learning_rate': 0.09787378427350894, 'depth': 9, 'l2_leaf_reg': 7.366629220290943, 'random_strength': 2.3104800615943213, 'min_data_in_leaf': 65, 'subsample': 0.598191226243577, 'colsample_bylevel': 0.5644964878123196, 'grow_policy': 'SymmetricTree'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:25:05,744] Trial 45 finished with value: 0.018750000000000044 and parameters: {'learning_rate': 0.18135376289543884, 'depth': 8, 'l2_leaf_reg': 2.685939017732163, 'random_strength': 3.88514632229983, 'min_data_in_leaf': 50, 'subsample': 0.5129229205232568, 'colsample_bylevel': 0.6036481819545267, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:33:12,517] Trial 46 finished with value: 0.01749999999999996 and parameters: {'learning_rate': 0.14318968654567638, 'depth': 7, 'l2_leaf_reg': 5.160500278056838, 'random_strength': 1.2119788439249084, 'min_data_in_leaf': 72, 'subsample': 0.9324017999734644, 'colsample_bylevel': 0.7714516775848355, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:34:37,674] Trial 47 finished with value: 0.09499999999999997 and parameters: {'learning_rate': 0.11038991020284, 'depth': 9, 'l2_leaf_reg': 4.438958514091594, 'random_strength': 4.837859530175684, 'min_data_in_leaf': 78, 'subsample': 0.837590701695287, 'colsample_bylevel': 0.6428885370653349, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:40:22,747] Trial 48 finished with value: 0.02749999999999997 and parameters: {'learning_rate': 0.12610707187317663, 'depth': 8, 'l2_leaf_reg': 6.255842356501569, 'random_strength': 3.2834021256368517, 'min_data_in_leaf': 97, 'subsample': 0.7941054378260608, 'colsample_bylevel': 0.6974435845271694, 'grow_policy': 'SymmetricTree'}. Best is trial 13 with value: 0.015000000000000013.\n",
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:46:54,486] Trial 49 finished with value: 0.018750000000000044 and parameters: {'learning_rate': 0.19309193815439332, 'depth': 10, 'l2_leaf_reg': 2.025955922698504, 'random_strength': 7.227423252924508, 'min_data_in_leaf': 90, 'subsample': 0.5846690714443061, 'colsample_bylevel': 0.5216527428478228, 'grow_policy': 'Lossguide'}. Best is trial 13 with value: 0.015000000000000013.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Running LightGBM tuning...\")\n",
    "lgbm_study = optuna.create_study(direction=\"minimize\", study_name=\"lightgbm\")\n",
    "lgbm_study.optimize(lightgbm_objective, n_trials=50)\n",
    "# Save to drive\n",
    "with open(\"/content/light_gbm_com_best.json\", \"w\") as f:\n",
    "    json.dump(lgbm_study.best_params, f)"
   ],
   "metadata": {
    "id": "TLc6QQkd8HAE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d222a5db-4cc0-461f-c613-b8d18222d6ad"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I 2025-07-16 22:46:54,491] A new study created in memory with name: lightgbm\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running LightGBM tuning...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@3: 0.939641\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:47:34,869] Trial 0 finished with value: 0.01375000000000004 and parameters: {'learning_rate': 0.16225298188131157, 'num_leaves': 55, 'min_data_in_leaf': 30, 'feature_fraction': 0.8866622471040231, 'bagging_fraction': 0.9882933452901567, 'lambda_l1': 2.9888525353614726, 'lambda_l2': 3.345021012693289, 'bagging_freq': 1, 'max_depth': 9}. Best is trial 0 with value: 0.01375000000000004.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@3: 0.912125\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:48:16,395] Trial 1 finished with value: 0.02749999999999997 and parameters: {'learning_rate': 0.011889350993384286, 'num_leaves': 63, 'min_data_in_leaf': 27, 'feature_fraction': 0.6743691753707486, 'bagging_fraction': 0.6859109687414484, 'lambda_l1': 2.3088736630587126, 'lambda_l2': 3.8311251377579225, 'bagging_freq': 4, 'max_depth': 7}. Best is trial 0 with value: 0.01375000000000004.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's ndcg@3: 0.906366\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:48:58,020] Trial 2 finished with value: 0.030000000000000027 and parameters: {'learning_rate': 0.04098924000068181, 'num_leaves': 115, 'min_data_in_leaf': 96, 'feature_fraction': 0.9016092904066846, 'bagging_fraction': 0.833344685517506, 'lambda_l1': 0.6205022354544598, 'lambda_l2': 4.89630222632678, 'bagging_freq': 3, 'max_depth': 6}. Best is trial 0 with value: 0.01375000000000004.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@3: 0.918152\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:49:35,533] Trial 3 finished with value: 0.02749999999999997 and parameters: {'learning_rate': 0.02553754553002321, 'num_leaves': 114, 'min_data_in_leaf': 15, 'feature_fraction': 0.8543081550302887, 'bagging_fraction': 0.9228072050173419, 'lambda_l1': 2.046699148140832, 'lambda_l2': 2.0743399615844424, 'bagging_freq': 1, 'max_depth': 11}. Best is trial 0 with value: 0.01375000000000004.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@3: 0.931352\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:50:21,183] Trial 4 finished with value: 0.01749999999999996 and parameters: {'learning_rate': 0.06481569434502592, 'num_leaves': 89, 'min_data_in_leaf': 77, 'feature_fraction': 0.8042858567059298, 'bagging_fraction': 0.9204961297607742, 'lambda_l1': 1.9533492636279455, 'lambda_l2': 3.255927604510748, 'bagging_freq': 3, 'max_depth': 9}. Best is trial 0 with value: 0.01375000000000004.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\tvalid_0's ndcg@3: 0.929804\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:51:08,588] Trial 5 finished with value: 0.02124999999999999 and parameters: {'learning_rate': 0.06210022987018839, 'num_leaves': 36, 'min_data_in_leaf': 68, 'feature_fraction': 0.8020633392588385, 'bagging_fraction': 0.7795796403250379, 'lambda_l1': 3.4526360059127597, 'lambda_l2': 2.1377737992159007, 'bagging_freq': 2, 'max_depth': 8}. Best is trial 0 with value: 0.01375000000000004.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@3: 0.934209\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:51:56,189] Trial 6 finished with value: 0.016249999999999987 and parameters: {'learning_rate': 0.06876502637976797, 'num_leaves': 105, 'min_data_in_leaf': 60, 'feature_fraction': 0.9241846931925701, 'bagging_fraction': 0.7291122028578276, 'lambda_l1': 2.35395165380886, 'lambda_l2': 3.8512691134746184, 'bagging_freq': 2, 'max_depth': 12}. Best is trial 0 with value: 0.01375000000000004.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[92]\tvalid_0's ndcg@3: 0.934507\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:52:47,864] Trial 7 finished with value: 0.016249999999999987 and parameters: {'learning_rate': 0.17709852985123783, 'num_leaves': 97, 'min_data_in_leaf': 33, 'feature_fraction': 0.612721123167651, 'bagging_fraction': 0.5557109274524021, 'lambda_l1': 1.9343561858566076, 'lambda_l2': 3.78579226355027, 'bagging_freq': 2, 'max_depth': 5}. Best is trial 0 with value: 0.01375000000000004.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@3: 0.941843\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:53:32,380] Trial 8 finished with value: 0.010000000000000009 and parameters: {'learning_rate': 0.19508897815396264, 'num_leaves': 47, 'min_data_in_leaf': 97, 'feature_fraction': 0.8200412888498705, 'bagging_fraction': 0.8738167254478661, 'lambda_l1': 4.906485872313661, 'lambda_l2': 2.254967729989295, 'bagging_freq': 1, 'max_depth': 9}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's ndcg@3: 0.94409\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:54:13,186] Trial 9 finished with value: 0.01375000000000004 and parameters: {'learning_rate': 0.14904209602614502, 'num_leaves': 54, 'min_data_in_leaf': 96, 'feature_fraction': 0.5427647720005709, 'bagging_fraction': 0.6123402354869276, 'lambda_l1': 1.0812031780603892, 'lambda_l2': 0.6656882010689114, 'bagging_freq': 4, 'max_depth': 10}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@3: 0.935429\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:54:56,643] Trial 10 finished with value: 0.01749999999999996 and parameters: {'learning_rate': 0.11928112721387439, 'num_leaves': 29, 'min_data_in_leaf': 49, 'feature_fraction': 0.9894268770049781, 'bagging_fraction': 0.841159884745378, 'lambda_l1': 4.922302457945405, 'lambda_l2': 0.7814828519650374, 'bagging_freq': 5, 'max_depth': 5}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[71]\tvalid_0's ndcg@3: 0.93467\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:55:43,015] Trial 11 finished with value: 0.01375000000000004 and parameters: {'learning_rate': 0.1962460465842539, 'num_leaves': 47, 'min_data_in_leaf': 44, 'feature_fraction': 0.7189137856733783, 'bagging_fraction': 0.9976259614979406, 'lambda_l1': 4.913435869812661, 'lambda_l2': 2.791527958008163, 'bagging_freq': 1, 'max_depth': 9}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[89]\tvalid_0's ndcg@3: 0.940563\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:56:25,023] Trial 12 finished with value: 0.015000000000000013 and parameters: {'learning_rate': 0.15139431102915868, 'num_leaves': 74, 'min_data_in_leaf': 11, 'feature_fraction': 0.780771100583937, 'bagging_fraction': 0.9798090645663569, 'lambda_l1': 3.5946212472731034, 'lambda_l2': 1.4290245876316787, 'bagging_freq': 1, 'max_depth': 8}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[93]\tvalid_0's ndcg@3: 0.937632\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:57:05,739] Trial 13 finished with value: 0.015000000000000013 and parameters: {'learning_rate': 0.19833253244313423, 'num_leaves': 19, 'min_data_in_leaf': 83, 'feature_fraction': 0.9933682094570295, 'bagging_fraction': 0.8706904734900003, 'lambda_l1': 3.916259130668703, 'lambda_l2': 2.798212143910034, 'bagging_freq': 1, 'max_depth': 10}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@3: 0.936575\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:58:04,112] Trial 14 finished with value: 0.012499999999999956 and parameters: {'learning_rate': 0.1599052142021365, 'num_leaves': 76, 'min_data_in_leaf': 36, 'feature_fraction': 0.8787419833805229, 'bagging_fraction': 0.9238614963374626, 'lambda_l1': 4.175421044410306, 'lambda_l2': 4.8869882768727795, 'bagging_freq': 2, 'max_depth': 7}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@3: 0.930295\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:58:56,992] Trial 15 finished with value: 0.02124999999999999 and parameters: {'learning_rate': 0.12756143084742066, 'num_leaves': 79, 'min_data_in_leaf': 41, 'feature_fraction': 0.7175878226940691, 'bagging_fraction': 0.9049485599768822, 'lambda_l1': 4.298492044978823, 'lambda_l2': 4.92794017528262, 'bagging_freq': 2, 'max_depth': 4}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@3: 0.939209\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 22:59:43,744] Trial 16 finished with value: 0.01375000000000004 and parameters: {'learning_rate': 0.0949618528996377, 'num_leaves': 127, 'min_data_in_leaf': 58, 'feature_fraction': 0.8389486508963309, 'bagging_fraction': 0.7872005943127132, 'lambda_l1': 4.254797994942413, 'lambda_l2': 1.4801837735893928, 'bagging_freq': 2, 'max_depth': 7}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@3: 0.941486\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:00:33,775] Trial 17 finished with value: 0.01375000000000004 and parameters: {'learning_rate': 0.1743635560041369, 'num_leaves': 40, 'min_data_in_leaf': 82, 'feature_fraction': 0.9152685827365797, 'bagging_fraction': 0.7056249015539376, 'lambda_l1': 4.456682586307752, 'lambda_l2': 1.4840102213200277, 'bagging_freq': 3, 'max_depth': 7}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@3: 0.935161\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:01:28,034] Trial 18 finished with value: 0.015000000000000013 and parameters: {'learning_rate': 0.13252642968021977, 'num_leaves': 65, 'min_data_in_leaf': 71, 'feature_fraction': 0.6361021465236331, 'bagging_fraction': 0.8202276746354825, 'lambda_l1': 3.1622001859919635, 'lambda_l2': 0.05885210828274001, 'bagging_freq': 2, 'max_depth': 6}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[91]\tvalid_0's ndcg@3: 0.937304\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:02:22,779] Trial 19 finished with value: 0.015000000000000013 and parameters: {'learning_rate': 0.17963218827054106, 'num_leaves': 86, 'min_data_in_leaf': 100, 'feature_fraction': 0.7614512405557169, 'bagging_fraction': 0.9382556285823048, 'lambda_l1': 4.945006656437579, 'lambda_l2': 4.449022646545192, 'bagging_freq': 4, 'max_depth': 10}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's ndcg@3: 0.936545\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:02:57,082] Trial 20 finished with value: 0.01749999999999996 and parameters: {'learning_rate': 0.08885438738620009, 'num_leaves': 20, 'min_data_in_leaf': 22, 'feature_fraction': 0.8518542189751281, 'bagging_fraction': 0.8678903564286913, 'lambda_l1': 0.019014596147159946, 'lambda_l2': 2.364152273806752, 'bagging_freq': 1, 'max_depth': 11}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\tvalid_0's ndcg@3: 0.939507\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:03:37,618] Trial 21 finished with value: 0.011249999999999982 and parameters: {'learning_rate': 0.15443003246253595, 'num_leaves': 55, 'min_data_in_leaf': 34, 'feature_fraction': 0.8906204483986593, 'bagging_fraction': 0.9606154968288049, 'lambda_l1': 2.9190643419362012, 'lambda_l2': 4.3159832531077855, 'bagging_freq': 1, 'max_depth': 9}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[80]\tvalid_0's ndcg@3: 0.940891\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:04:18,074] Trial 22 finished with value: 0.011249999999999982 and parameters: {'learning_rate': 0.14567594267518513, 'num_leaves': 65, 'min_data_in_leaf': 39, 'feature_fraction': 0.9559526643712262, 'bagging_fraction': 0.9524075487702776, 'lambda_l1': 2.815110863422298, 'lambda_l2': 4.347384733837286, 'bagging_freq': 1, 'max_depth': 8}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[75]\tvalid_0's ndcg@3: 0.938286\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:04:57,821] Trial 23 finished with value: 0.011249999999999982 and parameters: {'learning_rate': 0.14055743783564806, 'num_leaves': 62, 'min_data_in_leaf': 52, 'feature_fraction': 0.9541589054490743, 'bagging_fraction': 0.9449249109907848, 'lambda_l1': 2.748300168052951, 'lambda_l2': 4.33866770740528, 'bagging_freq': 1, 'max_depth': 9}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@3: 0.94162\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:05:35,432] Trial 24 finished with value: 0.015000000000000013 and parameters: {'learning_rate': 0.11549364295239463, 'num_leaves': 48, 'min_data_in_leaf': 23, 'feature_fraction': 0.9366890381526735, 'bagging_fraction': 0.8768434426919739, 'lambda_l1': 1.5538529502016452, 'lambda_l2': 3.2742957692194055, 'bagging_freq': 1, 'max_depth': 8}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[87]\tvalid_0's ndcg@3: 0.943525\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:06:18,436] Trial 25 finished with value: 0.010000000000000009 and parameters: {'learning_rate': 0.18614718595949256, 'num_leaves': 39, 'min_data_in_leaf': 41, 'feature_fraction': 0.9573862765250418, 'bagging_fraction': 0.9599362734881016, 'lambda_l1': 2.7872874637913254, 'lambda_l2': 4.359440999222507, 'bagging_freq': 1, 'max_depth': 11}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[89]\tvalid_0's ndcg@3: 0.943391\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:07:10,055] Trial 26 finished with value: 0.012499999999999956 and parameters: {'learning_rate': 0.18866637689452367, 'num_leaves': 31, 'min_data_in_leaf': 47, 'feature_fraction': 0.8365048733461136, 'bagging_fraction': 0.8902895870077736, 'lambda_l1': 3.44515958029183, 'lambda_l2': 2.849062049752076, 'bagging_freq': 3, 'max_depth': 12}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[79]\tvalid_0's ndcg@3: 0.940697\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:08:06,381] Trial 27 finished with value: 0.012499999999999956 and parameters: {'learning_rate': 0.16726255783424782, 'num_leaves': 42, 'min_data_in_leaf': 62, 'feature_fraction': 0.9706960189158425, 'bagging_fraction': 0.9629722870184746, 'lambda_l1': 1.3880439418367205, 'lambda_l2': 3.5506997937044327, 'bagging_freq': 2, 'max_depth': 11}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\tvalid_0's ndcg@3: 0.943361\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:08:53,213] Trial 28 finished with value: 0.011249999999999982 and parameters: {'learning_rate': 0.1892292463764014, 'num_leaves': 29, 'min_data_in_leaf': 54, 'feature_fraction': 0.805150003502603, 'bagging_fraction': 0.7966402745943224, 'lambda_l1': 3.6916021767385185, 'lambda_l2': 1.883199850442399, 'bagging_freq': 5, 'max_depth': 10}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[89]\tvalid_0's ndcg@3: 0.936188\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:09:34,214] Trial 29 finished with value: 0.016249999999999987 and parameters: {'learning_rate': 0.16123033277872845, 'num_leaves': 54, 'min_data_in_leaf': 31, 'feature_fraction': 0.884749437175844, 'bagging_fraction': 0.995962792258265, 'lambda_l1': 3.02883697709224, 'lambda_l2': 4.22526140512565, 'bagging_freq': 1, 'max_depth': 11}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's ndcg@3: 0.941486\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:10:16,574] Trial 30 finished with value: 0.015000000000000013 and parameters: {'learning_rate': 0.1845576000109755, 'num_leaves': 50, 'min_data_in_leaf': 22, 'feature_fraction': 0.874122724524548, 'bagging_fraction': 0.9703034864132998, 'lambda_l1': 3.287234299549743, 'lambda_l2': 3.066627525216757, 'bagging_freq': 1, 'max_depth': 12}. Best is trial 8 with value: 0.010000000000000009.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's ndcg@3: 0.941545\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:10:57,533] Trial 31 finished with value: 0.008750000000000036 and parameters: {'learning_rate': 0.14925362110050383, 'num_leaves': 61, 'min_data_in_leaf': 39, 'feature_fraction': 0.9509281191383269, 'bagging_fraction': 0.9584197752403462, 'lambda_l1': 2.766186105296192, 'lambda_l2': 4.511482480952715, 'bagging_freq': 1, 'max_depth': 9}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\tvalid_0's ndcg@3: 0.940861\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:11:40,440] Trial 32 finished with value: 0.011249999999999982 and parameters: {'learning_rate': 0.16783013495969817, 'num_leaves': 60, 'min_data_in_leaf': 37, 'feature_fraction': 0.9137916596078073, 'bagging_fraction': 0.90275577201779, 'lambda_l1': 2.609900132385227, 'lambda_l2': 4.593855612770385, 'bagging_freq': 1, 'max_depth': 9}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[88]\tvalid_0's ndcg@3: 0.940132\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:12:22,028] Trial 33 finished with value: 0.011249999999999982 and parameters: {'learning_rate': 0.16254184310081038, 'num_leaves': 42, 'min_data_in_leaf': 28, 'feature_fraction': 0.9997472017608418, 'bagging_fraction': 0.8459185705923274, 'lambda_l1': 2.381944175630537, 'lambda_l2': 4.048083396781808, 'bagging_freq': 1, 'max_depth': 10}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[71]\tvalid_0's ndcg@3: 0.940861\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:13:06,520] Trial 34 finished with value: 0.01375000000000004 and parameters: {'learning_rate': 0.19543111881522443, 'num_leaves': 58, 'min_data_in_leaf': 44, 'feature_fraction': 0.9450001174540787, 'bagging_fraction': 0.638318839628841, 'lambda_l1': 2.8264204162081366, 'lambda_l2': 4.640119267602978, 'bagging_freq': 1, 'max_depth': 9}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[83]\tvalid_0's ndcg@3: 0.939507\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:14:05,264] Trial 35 finished with value: 0.012499999999999956 and parameters: {'learning_rate': 0.15510397647680255, 'num_leaves': 67, 'min_data_in_leaf': 89, 'feature_fraction': 0.9052510025852627, 'bagging_fraction': 0.9671532227604275, 'lambda_l1': 2.12276800370513, 'lambda_l2': 3.609260117083212, 'bagging_freq': 2, 'max_depth': 8}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[88]\tvalid_0's ndcg@3: 0.939804\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:14:45,813] Trial 36 finished with value: 0.012499999999999956 and parameters: {'learning_rate': 0.1729146589589571, 'num_leaves': 34, 'min_data_in_leaf': 67, 'feature_fraction': 0.8738291453596493, 'bagging_fraction': 0.933104194437236, 'lambda_l1': 1.7452123655747371, 'lambda_l2': 4.100988681995403, 'bagging_freq': 1, 'max_depth': 11}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[80]\tvalid_0's ndcg@3: 0.93912\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:15:34,699] Trial 37 finished with value: 0.015000000000000013 and parameters: {'learning_rate': 0.1378088797534821, 'num_leaves': 70, 'min_data_in_leaf': 16, 'feature_fraction': 0.8375204907626204, 'bagging_fraction': 0.5052126086954803, 'lambda_l1': 1.0382978438342283, 'lambda_l2': 2.4039823096092396, 'bagging_freq': 2, 'max_depth': 9}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's ndcg@3: 0.937795\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:16:11,400] Trial 38 finished with value: 0.015000000000000013 and parameters: {'learning_rate': 0.10395664183191303, 'num_leaves': 46, 'min_data_in_leaf': 31, 'feature_fraction': 0.9732807552814717, 'bagging_fraction': 0.7509592471715375, 'lambda_l1': 3.8851040810040542, 'lambda_l2': 3.9125192457634723, 'bagging_freq': 1, 'max_depth': 10}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[90]\tvalid_0's ndcg@3: 0.942304\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:17:08,852] Trial 39 finished with value: 0.010000000000000009 and parameters: {'learning_rate': 0.18319687641379984, 'num_leaves': 82, 'min_data_in_leaf': 50, 'feature_fraction': 0.8155794224356111, 'bagging_fraction': 0.9011573915090634, 'lambda_l1': 4.623771755117341, 'lambda_l2': 3.5639370175706917, 'bagging_freq': 3, 'max_depth': 7}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's ndcg@3: 0.901157\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:17:41,792] Trial 40 finished with value: 0.02749999999999997 and parameters: {'learning_rate': 0.010193776878215655, 'num_leaves': 87, 'min_data_in_leaf': 49, 'feature_fraction': 0.7313728058835619, 'bagging_fraction': 0.9125536786878152, 'lambda_l1': 4.541988319071369, 'lambda_l2': 3.5641396398322494, 'bagging_freq': 4, 'max_depth': 6}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[81]\tvalid_0's ndcg@3: 0.938227\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:18:38,431] Trial 41 finished with value: 0.01375000000000004 and parameters: {'learning_rate': 0.18293981755221142, 'num_leaves': 81, 'min_data_in_leaf': 42, 'feature_fraction': 0.819832463580882, 'bagging_fraction': 0.9496359280878136, 'lambda_l1': 4.506561428624602, 'lambda_l2': 4.718057612245007, 'bagging_freq': 3, 'max_depth': 8}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[88]\tvalid_0's ndcg@3: 0.939209\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:19:22,853] Trial 42 finished with value: 0.011249999999999982 and parameters: {'learning_rate': 0.17445507050793407, 'num_leaves': 99, 'min_data_in_leaf': 34, 'feature_fraction': 0.7818950053446081, 'bagging_fraction': 0.8494002620518861, 'lambda_l1': 4.719090880724252, 'lambda_l2': 3.751167389586577, 'bagging_freq': 1, 'max_depth': 7}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's ndcg@3: 0.936873\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:19:58,787] Trial 43 finished with value: 0.01375000000000004 and parameters: {'learning_rate': 0.19997363640478644, 'num_leaves': 57, 'min_data_in_leaf': 57, 'feature_fraction': 0.8943885152018101, 'bagging_fraction': 0.8884870320838483, 'lambda_l1': 2.3951700023052833, 'lambda_l2': 3.0894041574816185, 'bagging_freq': 5, 'max_depth': 9}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[93]\tvalid_0's ndcg@3: 0.938911\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:20:57,736] Trial 44 finished with value: 0.01375000000000004 and parameters: {'learning_rate': 0.1870962356424833, 'num_leaves': 93, 'min_data_in_leaf': 64, 'feature_fraction': 0.9311317257995595, 'bagging_fraction': 0.9992055575055298, 'lambda_l1': 3.8509618782939454, 'lambda_l2': 1.962968239339415, 'bagging_freq': 3, 'max_depth': 8}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[78]\tvalid_0's ndcg@3: 0.941813\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:21:48,402] Trial 45 finished with value: 0.012499999999999956 and parameters: {'learning_rate': 0.1539296225549537, 'num_leaves': 72, 'min_data_in_leaf': 51, 'feature_fraction': 0.5176712977913858, 'bagging_fraction': 0.9253309233844215, 'lambda_l1': 2.1712913995347476, 'lambda_l2': 4.996140795239308, 'bagging_freq': 1, 'max_depth': 9}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[72]\tvalid_0's ndcg@3: 0.913241\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:22:36,015] Trial 46 finished with value: 0.025000000000000022 and parameters: {'learning_rate': 0.03930525801832191, 'num_leaves': 26, 'min_data_in_leaf': 47, 'feature_fraction': 0.7844098511370348, 'bagging_fraction': 0.8240999271118816, 'lambda_l1': 4.108114208147697, 'lambda_l2': 4.1022087957852875, 'bagging_freq': 2, 'max_depth': 6}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[83]\tvalid_0's ndcg@3: 0.934507\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:23:30,666] Trial 47 finished with value: 0.015000000000000013 and parameters: {'learning_rate': 0.12637347009795435, 'num_leaves': 38, 'min_data_in_leaf': 75, 'feature_fraction': 0.8680593754551115, 'bagging_fraction': 0.9834351606367228, 'lambda_l1': 4.7381986552629645, 'lambda_l2': 4.519630002082221, 'bagging_freq': 2, 'max_depth': 7}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[84]\tvalid_0's ndcg@3: 0.938882\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:24:22,234] Trial 48 finished with value: 0.012499999999999956 and parameters: {'learning_rate': 0.1920428649594978, 'num_leaves': 51, 'min_data_in_leaf': 26, 'feature_fraction': 0.7560982715103031, 'bagging_fraction': 0.9051172767594667, 'lambda_l1': 3.308447166112485, 'lambda_l2': 3.3993554590269572, 'bagging_freq': 4, 'max_depth': 5}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's ndcg@3: 0.936843\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n",
      "[I 2025-07-16 23:25:03,535] Trial 49 finished with value: 0.015000000000000013 and parameters: {'learning_rate': 0.16975654380712957, 'num_leaves': 109, 'min_data_in_leaf': 35, 'feature_fraction': 0.8182234916546454, 'bagging_fraction': 0.8655903725582945, 'lambda_l1': 3.0090595304292993, 'lambda_l2': 3.907284352997599, 'bagging_freq': 1, 'max_depth': 10}. Best is trial 31 with value: 0.008750000000000036.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\nBest CatBoost:\")\n",
    "print(catboost_study.best_params)\n",
    "print(\"Recall@3:\", 1 - catboost_study.best_value)\n",
    "\n",
    "print(\"\\nBest LightGBM:\")\n",
    "print(lgbm_study.best_params)\n",
    "print(\"Recall@3:\", 1 - lgbm_study.best_value)"
   ],
   "metadata": {
    "id": "GhMkyu5F8HSy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b9716ac4-b2b3-4afb-94b4-d9f3f08a709e"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Best CatBoost:\n",
      "{'learning_rate': 0.1329416174119581, 'depth': 8, 'l2_leaf_reg': 6.628807908190987, 'random_strength': 4.881288964388853, 'min_data_in_leaf': 100, 'subsample': 0.708915364734102, 'colsample_bylevel': 0.6444718826631752, 'grow_policy': 'Lossguide'}\n",
      "Recall@3: 0.985\n",
      "\n",
      "Best LightGBM:\n",
      "{'learning_rate': 0.14925362110050383, 'num_leaves': 61, 'min_data_in_leaf': 39, 'feature_fraction': 0.9509281191383269, 'bagging_fraction': 0.9584197752403462, 'lambda_l1': 2.766186105296192, 'lambda_l2': 4.511482480952715, 'bagging_freq': 1, 'max_depth': 9}\n",
      "Recall@3: 0.99125\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Static/default parameters that LightGBM expects\n",
    "base_params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": [\"ndcg\"],\n",
    "    \"eval_at\": [1, 3],\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"verbosity\": -1,\n",
    "    \"force_row_wise\": True,\n",
    "}\n",
    "\n",
    "# Load from file\n",
    "with open(\"light_gbm_com_best.json\", \"r\") as f:\n",
    "    best_params_from_json = json.load(f)\n",
    "\n",
    "# Merge base + Optuna best\n",
    "final_params = {**base_params, **best_params_from_json}\n",
    "\n",
    "train_model_lightGBM(parameters=final_params, n_rounds=1000, lr_decay_gamma=0.95)"
   ],
   "metadata": {
    "id": "foJ5K30c-SR2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3380b065-08eb-4894-f38f-314a43a7b9ea"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1]\ttrain's ndcg@1: 0.712771\ttrain's ndcg@3: 0.881039\tval's ndcg@1: 0.49875\tval's ndcg@3: 0.697153\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[2]\ttrain's ndcg@1: 0.916362\ttrain's ndcg@3: 0.966712\tval's ndcg@1: 0.69125\tval's ndcg@3: 0.83077\n",
      "[3]\ttrain's ndcg@1: 0.934003\ttrain's ndcg@3: 0.973692\tval's ndcg@1: 0.7825\tval's ndcg@3: 0.894476\n",
      "[4]\ttrain's ndcg@1: 0.936357\ttrain's ndcg@3: 0.974752\tval's ndcg@1: 0.79125\tval's ndcg@3: 0.898494\n",
      "[5]\ttrain's ndcg@1: 0.938536\ttrain's ndcg@3: 0.975715\tval's ndcg@1: 0.7925\tval's ndcg@3: 0.898792\n",
      "[6]\ttrain's ndcg@1: 0.942472\ttrain's ndcg@3: 0.977236\tval's ndcg@1: 0.79375\tval's ndcg@3: 0.899714\n",
      "[7]\ttrain's ndcg@1: 0.943334\ttrain's ndcg@3: 0.977538\tval's ndcg@1: 0.795\tval's ndcg@3: 0.899848\n",
      "[8]\ttrain's ndcg@1: 0.945068\ttrain's ndcg@3: 0.978197\tval's ndcg@1: 0.795\tval's ndcg@3: 0.900176\n",
      "[9]\ttrain's ndcg@1: 0.946764\ttrain's ndcg@3: 0.978825\tval's ndcg@1: 0.8025\tval's ndcg@3: 0.903107\n",
      "[10]\ttrain's ndcg@1: 0.948822\ttrain's ndcg@3: 0.979593\tval's ndcg@1: 0.805\tval's ndcg@3: 0.903569\n",
      "[11]\ttrain's ndcg@1: 0.94908\ttrain's ndcg@3: 0.979692\tval's ndcg@1: 0.80375\tval's ndcg@3: 0.903107\n",
      "[12]\ttrain's ndcg@1: 0.949816\ttrain's ndcg@3: 0.979946\tval's ndcg@1: 0.80125\tval's ndcg@3: 0.902646\n",
      "[13]\ttrain's ndcg@1: 0.95093\ttrain's ndcg@3: 0.980452\tval's ndcg@1: 0.8025\tval's ndcg@3: 0.903732\n",
      "[14]\ttrain's ndcg@1: 0.951402\ttrain's ndcg@3: 0.980594\tval's ndcg@1: 0.80625\tval's ndcg@3: 0.904819\n",
      "[15]\ttrain's ndcg@1: 0.952527\ttrain's ndcg@3: 0.98103\tval's ndcg@1: 0.80375\tval's ndcg@3: 0.903569\n",
      "[16]\ttrain's ndcg@1: 0.952637\ttrain's ndcg@3: 0.981064\tval's ndcg@1: 0.805\tval's ndcg@3: 0.90531\n",
      "[17]\ttrain's ndcg@1: 0.952714\ttrain's ndcg@3: 0.981106\tval's ndcg@1: 0.8075\tval's ndcg@3: 0.907482\n",
      "[18]\ttrain's ndcg@1: 0.952549\ttrain's ndcg@3: 0.981047\tval's ndcg@1: 0.805\tval's ndcg@3: 0.905771\n",
      "[19]\ttrain's ndcg@1: 0.952467\ttrain's ndcg@3: 0.981082\tval's ndcg@1: 0.805\tval's ndcg@3: 0.905146\n",
      "[20]\ttrain's ndcg@1: 0.952637\ttrain's ndcg@3: 0.981255\tval's ndcg@1: 0.80875\tval's ndcg@3: 0.907155\n",
      "[21]\ttrain's ndcg@1: 0.953647\ttrain's ndcg@3: 0.981621\tval's ndcg@1: 0.80875\tval's ndcg@3: 0.906664\n",
      "[22]\ttrain's ndcg@1: 0.954454\ttrain's ndcg@3: 0.981982\tval's ndcg@1: 0.80875\tval's ndcg@3: 0.906039\n",
      "[23]\ttrain's ndcg@1: 0.955387\ttrain's ndcg@3: 0.982327\tval's ndcg@1: 0.8075\tval's ndcg@3: 0.905741\n",
      "[24]\ttrain's ndcg@1: 0.955519\ttrain's ndcg@3: 0.982375\tval's ndcg@1: 0.80875\tval's ndcg@3: 0.90653\n",
      "[25]\ttrain's ndcg@1: 0.955579\ttrain's ndcg@3: 0.982396\tval's ndcg@1: 0.81\tval's ndcg@3: 0.907155\n",
      "[26]\ttrain's ndcg@1: 0.95565\ttrain's ndcg@3: 0.982449\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907616\n",
      "[27]\ttrain's ndcg@1: 0.956968\ttrain's ndcg@3: 0.982915\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.90778\n",
      "[28]\ttrain's ndcg@1: 0.956852\ttrain's ndcg@3: 0.982901\tval's ndcg@1: 0.8125\tval's ndcg@3: 0.908078\n",
      "[29]\ttrain's ndcg@1: 0.95699\ttrain's ndcg@3: 0.982956\tval's ndcg@1: 0.81375\tval's ndcg@3: 0.908539\n",
      "[30]\ttrain's ndcg@1: 0.957149\ttrain's ndcg@3: 0.983032\tval's ndcg@1: 0.815\tval's ndcg@3: 0.908375\n",
      "[31]\ttrain's ndcg@1: 0.957067\ttrain's ndcg@3: 0.98302\tval's ndcg@1: 0.81375\tval's ndcg@3: 0.908078\n",
      "[32]\ttrain's ndcg@1: 0.957198\ttrain's ndcg@3: 0.983095\tval's ndcg@1: 0.81375\tval's ndcg@3: 0.907914\n",
      "[33]\ttrain's ndcg@1: 0.957253\ttrain's ndcg@3: 0.983144\tval's ndcg@1: 0.81375\tval's ndcg@3: 0.907453\n",
      "[34]\ttrain's ndcg@1: 0.957292\ttrain's ndcg@3: 0.983135\tval's ndcg@1: 0.8125\tval's ndcg@3: 0.906991\n",
      "[35]\ttrain's ndcg@1: 0.957242\ttrain's ndcg@3: 0.983116\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.90653\n",
      "[36]\ttrain's ndcg@1: 0.957242\ttrain's ndcg@3: 0.983133\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.90653\n",
      "[37]\ttrain's ndcg@1: 0.957209\ttrain's ndcg@3: 0.983121\tval's ndcg@1: 0.81\tval's ndcg@3: 0.906069\n",
      "[38]\ttrain's ndcg@1: 0.95722\ttrain's ndcg@3: 0.983128\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906694\n",
      "[39]\ttrain's ndcg@1: 0.957242\ttrain's ndcg@3: 0.983136\tval's ndcg@1: 0.8125\tval's ndcg@3: 0.907155\n",
      "[40]\ttrain's ndcg@1: 0.957308\ttrain's ndcg@3: 0.983161\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906694\n",
      "[41]\ttrain's ndcg@1: 0.957308\ttrain's ndcg@3: 0.983156\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907319\n",
      "[42]\ttrain's ndcg@1: 0.957308\ttrain's ndcg@3: 0.983157\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907319\n",
      "[43]\ttrain's ndcg@1: 0.957314\ttrain's ndcg@3: 0.98316\tval's ndcg@1: 0.81\tval's ndcg@3: 0.906694\n",
      "[44]\ttrain's ndcg@1: 0.957495\ttrain's ndcg@3: 0.983249\tval's ndcg@1: 0.81\tval's ndcg@3: 0.906694\n",
      "[45]\ttrain's ndcg@1: 0.957506\ttrain's ndcg@3: 0.983253\tval's ndcg@1: 0.81\tval's ndcg@3: 0.906694\n",
      "[46]\ttrain's ndcg@1: 0.957506\ttrain's ndcg@3: 0.983253\tval's ndcg@1: 0.81\tval's ndcg@3: 0.906694\n",
      "[47]\ttrain's ndcg@1: 0.957506\ttrain's ndcg@3: 0.983253\tval's ndcg@1: 0.81\tval's ndcg@3: 0.906694\n",
      "[48]\ttrain's ndcg@1: 0.957506\ttrain's ndcg@3: 0.983253\tval's ndcg@1: 0.81\tval's ndcg@3: 0.906694\n",
      "[49]\ttrain's ndcg@1: 0.957506\ttrain's ndcg@3: 0.983256\tval's ndcg@1: 0.80875\tval's ndcg@3: 0.906232\n",
      "[50]\ttrain's ndcg@1: 0.957495\ttrain's ndcg@3: 0.983252\tval's ndcg@1: 0.80875\tval's ndcg@3: 0.906232\n",
      "[51]\ttrain's ndcg@1: 0.957495\ttrain's ndcg@3: 0.983254\tval's ndcg@1: 0.80875\tval's ndcg@3: 0.906396\n",
      "[52]\ttrain's ndcg@1: 0.95772\ttrain's ndcg@3: 0.983311\tval's ndcg@1: 0.80875\tval's ndcg@3: 0.906232\n",
      "[53]\ttrain's ndcg@1: 0.957725\ttrain's ndcg@3: 0.983337\tval's ndcg@1: 0.80875\tval's ndcg@3: 0.906232\n",
      "[54]\ttrain's ndcg@1: 0.95772\ttrain's ndcg@3: 0.983335\tval's ndcg@1: 0.80875\tval's ndcg@3: 0.906232\n",
      "[55]\ttrain's ndcg@1: 0.957725\ttrain's ndcg@3: 0.983342\tval's ndcg@1: 0.80875\tval's ndcg@3: 0.906396\n",
      "[56]\ttrain's ndcg@1: 0.957736\ttrain's ndcg@3: 0.983346\tval's ndcg@1: 0.81\tval's ndcg@3: 0.906857\n",
      "[57]\ttrain's ndcg@1: 0.957736\ttrain's ndcg@3: 0.983346\tval's ndcg@1: 0.81\tval's ndcg@3: 0.906857\n",
      "[58]\ttrain's ndcg@1: 0.957736\ttrain's ndcg@3: 0.98332\tval's ndcg@1: 0.80875\tval's ndcg@3: 0.906232\n",
      "[59]\ttrain's ndcg@1: 0.958285\ttrain's ndcg@3: 0.983522\tval's ndcg@1: 0.80875\tval's ndcg@3: 0.906232\n",
      "[60]\ttrain's ndcg@1: 0.958285\ttrain's ndcg@3: 0.983522\tval's ndcg@1: 0.81\tval's ndcg@3: 0.906694\n",
      "[61]\ttrain's ndcg@1: 0.958225\ttrain's ndcg@3: 0.9835\tval's ndcg@1: 0.8125\tval's ndcg@3: 0.907616\n",
      "[62]\ttrain's ndcg@1: 0.958225\ttrain's ndcg@3: 0.9835\tval's ndcg@1: 0.8125\tval's ndcg@3: 0.907616\n",
      "[63]\ttrain's ndcg@1: 0.958285\ttrain's ndcg@3: 0.983522\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[64]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.983593\tval's ndcg@1: 0.8125\tval's ndcg@3: 0.907616\n",
      "[65]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.983593\tval's ndcg@1: 0.8125\tval's ndcg@3: 0.907616\n",
      "[66]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.983593\tval's ndcg@1: 0.8125\tval's ndcg@3: 0.907616\n",
      "[67]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.983593\tval's ndcg@1: 0.8125\tval's ndcg@3: 0.907616\n",
      "[68]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.983593\tval's ndcg@1: 0.8125\tval's ndcg@3: 0.907616\n",
      "[69]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983591\tval's ndcg@1: 0.8125\tval's ndcg@3: 0.907616\n",
      "[70]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983591\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[71]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983591\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[72]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983591\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[73]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983591\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[74]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983591\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[75]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983591\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[76]\ttrain's ndcg@1: 0.958307\ttrain's ndcg@3: 0.983531\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[77]\ttrain's ndcg@1: 0.958302\ttrain's ndcg@3: 0.983529\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[78]\ttrain's ndcg@1: 0.958466\ttrain's ndcg@3: 0.983589\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[79]\ttrain's ndcg@1: 0.958466\ttrain's ndcg@3: 0.983589\tval's ndcg@1: 0.81\tval's ndcg@3: 0.906694\n",
      "[80]\ttrain's ndcg@1: 0.958466\ttrain's ndcg@3: 0.983589\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[81]\ttrain's ndcg@1: 0.958466\ttrain's ndcg@3: 0.983589\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[82]\ttrain's ndcg@1: 0.958466\ttrain's ndcg@3: 0.983589\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[83]\ttrain's ndcg@1: 0.958466\ttrain's ndcg@3: 0.983589\tval's ndcg@1: 0.81\tval's ndcg@3: 0.906694\n",
      "[84]\ttrain's ndcg@1: 0.958302\ttrain's ndcg@3: 0.983529\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[85]\ttrain's ndcg@1: 0.958466\ttrain's ndcg@3: 0.983589\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[86]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983591\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[87]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983591\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[88]\ttrain's ndcg@1: 0.958307\ttrain's ndcg@3: 0.983531\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[89]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[90]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[91]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[92]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[93]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[94]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[95]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[96]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[97]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[98]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[99]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[100]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[101]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[102]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[103]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[104]\ttrain's ndcg@1: 0.958307\ttrain's ndcg@3: 0.983556\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[105]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[106]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[107]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[108]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[109]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[110]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[111]\ttrain's ndcg@1: 0.958307\ttrain's ndcg@3: 0.983556\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[112]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[113]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[114]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[115]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[116]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983617\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[117]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[118]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[119]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[120]\ttrain's ndcg@1: 0.958307\ttrain's ndcg@3: 0.983557\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[121]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[122]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[123]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[124]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[125]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[126]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[127]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[128]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[129]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[130]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[131]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[132]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[133]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[134]\ttrain's ndcg@1: 0.958307\ttrain's ndcg@3: 0.983557\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[135]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[136]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[137]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[138]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[139]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[140]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[141]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[142]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[143]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[144]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[145]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[146]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[147]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[148]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[149]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[150]\ttrain's ndcg@1: 0.958472\ttrain's ndcg@3: 0.983618\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.907155\n",
      "[151]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[152]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[153]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[154]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[155]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[156]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[157]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[158]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[159]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[160]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[161]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[162]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[163]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[164]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[165]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[166]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[167]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[168]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[169]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[170]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[171]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[172]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[173]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[174]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[175]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[176]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[177]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[178]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[179]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[180]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[181]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[182]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[183]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[184]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[185]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[186]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[187]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[188]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[189]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[190]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[191]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[192]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[193]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[194]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[195]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[196]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[197]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[198]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[199]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[200]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[201]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[202]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[203]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[204]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[205]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[206]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[207]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[208]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[209]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[210]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[211]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[212]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[213]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[214]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[215]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[216]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[217]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[218]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[219]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[220]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[221]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[222]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[223]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[224]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[225]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[226]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[227]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[228]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[229]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[230]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[231]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[232]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[233]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[234]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[235]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[236]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[237]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[238]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[239]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[240]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[241]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[242]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[243]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[244]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[245]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[246]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[247]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[248]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[249]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[250]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[251]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[252]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[253]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[254]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[255]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[256]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[257]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[258]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[259]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[260]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[261]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[262]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[263]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[264]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[265]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[266]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[267]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[268]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[269]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[270]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[271]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[272]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[273]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[274]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[275]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[276]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[277]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[278]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[279]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[280]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[281]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[282]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[283]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[284]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[285]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[286]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[287]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[288]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[289]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[290]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[291]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[292]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[293]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[294]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[295]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[296]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[297]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[298]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[299]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[300]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[301]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[302]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[303]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[304]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[305]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[306]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[307]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[308]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[309]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[310]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[311]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[312]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[313]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[314]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[315]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[316]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[317]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[318]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[319]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[320]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[321]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[322]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[323]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[324]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[325]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[326]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[327]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[328]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[329]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[330]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[331]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[332]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[333]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[334]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[335]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[336]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[337]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[338]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[339]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[340]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[341]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[342]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[343]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[344]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[345]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[346]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[347]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[348]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[349]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[350]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[351]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[352]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[353]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[354]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[355]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[356]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[357]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[358]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[359]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[360]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[361]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[362]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[363]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[364]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[365]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[366]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[367]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[368]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[369]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[370]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[371]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[372]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[373]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[374]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[375]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[376]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[377]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[378]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[379]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[380]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[381]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[382]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[383]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[384]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[385]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[386]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[387]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[388]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[389]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[390]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[391]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[392]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[393]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[394]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[395]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[396]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[397]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[398]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[399]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[400]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[401]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[402]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[403]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[404]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[405]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[406]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[407]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[408]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[409]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[410]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[411]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[412]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[413]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[414]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[415]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[416]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[417]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[418]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[419]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[420]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[421]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[422]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[423]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[424]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[425]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[426]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[427]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[428]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[429]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[430]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[431]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[432]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[433]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[434]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[435]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[436]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[437]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[438]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[439]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[440]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[441]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[442]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[443]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[444]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[445]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[446]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[447]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[448]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[449]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[450]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[451]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[452]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[453]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[454]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[455]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[456]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[457]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[458]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[459]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[460]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[461]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[462]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[463]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[464]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[465]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[466]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[467]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[468]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[469]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[470]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[471]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[472]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[473]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[474]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[475]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[476]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[477]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[478]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[479]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[480]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[481]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[482]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[483]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[484]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[485]\ttrain's ndcg@1: 0.958313\ttrain's ndcg@3: 0.983559\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[486]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[487]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[488]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[489]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[490]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[491]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[492]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[493]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[494]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[495]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[496]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[497]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[498]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[499]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[500]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[501]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[502]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[503]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[504]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[505]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[506]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[507]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[508]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[509]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[510]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[511]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[512]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[513]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[514]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[515]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[516]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[517]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[518]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[519]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[520]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[521]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[522]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[523]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[524]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[525]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[526]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[527]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[528]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "[529]\ttrain's ndcg@1: 0.958477\ttrain's ndcg@3: 0.98362\tval's ndcg@1: 0.81125\tval's ndcg@3: 0.906991\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttrain's ndcg@1: 0.95699\ttrain's ndcg@3: 0.982956\tval's ndcg@1: 0.81375\tval's ndcg@3: 0.908539\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\Evaluation Metrics (Validation Set):\n",
      "Accuracy@1 : 0.8137\n",
      "Recall@3   : 0.9712\n",
      "MRR        : 0.8911\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7a8a37fa4250>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Base/defaults for CatBoost\n",
    "base_params = {\n",
    "    \"loss_function\": \"YetiRank\",\n",
    "    \"eval_metric\": \"NDCG:top=3\",\n",
    "    \"random_seed\": 42,\n",
    "    \"task_type\": \"CPU\",  # or \"GPU\"\n",
    "}\n",
    "\n",
    "# Load from file\n",
    "best_params_from_json = {\n",
    "    \"learning_rate\": 0.1329416174119581,\n",
    "    \"depth\": 8,\n",
    "    \"l2_leaf_reg\": 6.628807908190987,\n",
    "    \"random_strength\": 4.881288964388853,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"subsample\": 0.708915364734102,\n",
    "    \"colsample_bylevel\": 0.6444718826631752,\n",
    "    \"grow_policy\": \"Lossguide\",\n",
    "}\n",
    "\n",
    "# Merge base + Optuna best\n",
    "final_params = {**base_params, **best_params_from_json}\n",
    "\n",
    "best_catboost_model = train_catboost_model(\n",
    "    parameters=final_params, n_rounds=1000, val_df_input=val_df\n",
    ")"
   ],
   "metadata": {
    "id": "ddCswPBU-TVC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8ab37ae5-201f-4195-cde9-72f44521cd6f"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:\ttest: 0.7810507\tbest: 0.7810507 (0)\ttotal: 8.58s\tremaining: 2h 22m 50s\n",
      "1:\ttest: 0.8003522\tbest: 0.8003522 (1)\ttotal: 17.3s\tremaining: 2h 24m 9s\n",
      "2:\ttest: 0.8017659\tbest: 0.8017659 (2)\ttotal: 26.1s\tremaining: 2h 24m 43s\n",
      "3:\ttest: 0.8056795\tbest: 0.8056795 (3)\ttotal: 34.4s\tremaining: 2h 22m 39s\n",
      "4:\ttest: 0.8048909\tbest: 0.8056795 (3)\ttotal: 43.3s\tremaining: 2h 23m 32s\n",
      "5:\ttest: 0.8048909\tbest: 0.8056795 (3)\ttotal: 52s\tremaining: 2h 23m 42s\n",
      "6:\ttest: 0.8073612\tbest: 0.8073612 (6)\ttotal: 1m\tremaining: 2h 23m 45s\n",
      "7:\ttest: 0.8083136\tbest: 0.8083136 (7)\ttotal: 1m 9s\tremaining: 2h 22m 35s\n",
      "8:\ttest: 0.8084772\tbest: 0.8084772 (8)\ttotal: 1m 17s\tremaining: 2h 22m 33s\n",
      "9:\ttest: 0.8093999\tbest: 0.8093999 (9)\ttotal: 1m 25s\tremaining: 2h 21m 40s\n",
      "10:\ttest: 0.8109476\tbest: 0.8109476 (10)\ttotal: 1m 34s\tremaining: 2h 21m 36s\n",
      "11:\ttest: 0.8120339\tbest: 0.8120339 (11)\ttotal: 1m 43s\tremaining: 2h 21m 27s\n",
      "12:\ttest: 0.8200403\tbest: 0.8200403 (12)\ttotal: 1m 51s\tremaining: 2h 21m 1s\n",
      "13:\ttest: 0.8527336\tbest: 0.8527336 (13)\ttotal: 2m\tremaining: 2h 21m 7s\n",
      "14:\ttest: 0.8567220\tbest: 0.8567220 (14)\ttotal: 2m 8s\tremaining: 2h 21m 5s\n",
      "15:\ttest: 0.8568857\tbest: 0.8568857 (15)\ttotal: 2m 17s\tremaining: 2h 20m 38s\n",
      "16:\ttest: 0.8579720\tbest: 0.8579720 (16)\ttotal: 2m 25s\tremaining: 2h 20m 9s\n",
      "17:\ttest: 0.8642517\tbest: 0.8642517 (17)\ttotal: 2m 34s\tremaining: 2h 20m 16s\n",
      "18:\ttest: 0.8653380\tbest: 0.8653380 (18)\ttotal: 2m 42s\tremaining: 2h 19m 53s\n",
      "19:\ttest: 0.8807554\tbest: 0.8807554 (19)\ttotal: 2m 50s\tremaining: 2h 19m 26s\n",
      "20:\ttest: 0.8801304\tbest: 0.8807554 (19)\ttotal: 2m 58s\tremaining: 2h 19m 1s\n",
      "21:\ttest: 0.8813508\tbest: 0.8813508 (21)\ttotal: 3m 7s\tremaining: 2h 18m 39s\n",
      "22:\ttest: 0.8853985\tbest: 0.8853985 (22)\ttotal: 3m 15s\tremaining: 2h 18m 18s\n",
      "23:\ttest: 0.8874075\tbest: 0.8874075 (23)\ttotal: 3m 23s\tremaining: 2h 17m 56s\n",
      "24:\ttest: 0.8939552\tbest: 0.8939552 (24)\ttotal: 3m 31s\tremaining: 2h 17m 38s\n",
      "25:\ttest: 0.8936575\tbest: 0.8939552 (24)\ttotal: 3m 40s\tremaining: 2h 17m 24s\n",
      "26:\ttest: 0.8946098\tbest: 0.8946098 (26)\ttotal: 3m 48s\tremaining: 2h 17m 27s\n",
      "27:\ttest: 0.9024371\tbest: 0.9024371 (27)\ttotal: 3m 57s\tremaining: 2h 17m 8s\n",
      "28:\ttest: 0.9052348\tbest: 0.9052348 (28)\ttotal: 4m 5s\tremaining: 2h 16m 52s\n",
      "29:\ttest: 0.9061575\tbest: 0.9061575 (29)\ttotal: 4m 13s\tremaining: 2h 16m 39s\n",
      "30:\ttest: 0.9069462\tbest: 0.9069462 (30)\ttotal: 4m 21s\tremaining: 2h 16m 23s\n",
      "31:\ttest: 0.9064848\tbest: 0.9069462 (30)\ttotal: 4m 30s\tremaining: 2h 16m 8s\n",
      "32:\ttest: 0.9067825\tbest: 0.9069462 (30)\ttotal: 4m 38s\tremaining: 2h 16m 12s\n",
      "33:\ttest: 0.9101755\tbest: 0.9101755 (33)\ttotal: 4m 47s\tremaining: 2h 15m 57s\n",
      "34:\ttest: 0.9101755\tbest: 0.9101755 (33)\ttotal: 4m 55s\tremaining: 2h 15m 41s\n",
      "35:\ttest: 0.9112915\tbest: 0.9112915 (35)\ttotal: 5m 3s\tremaining: 2h 15m 27s\n",
      "36:\ttest: 0.9122142\tbest: 0.9122142 (36)\ttotal: 5m 12s\tremaining: 2h 15m 24s\n",
      "37:\ttest: 0.9117825\tbest: 0.9122142 (36)\ttotal: 5m 20s\tremaining: 2h 15m 11s\n",
      "38:\ttest: 0.9197142\tbest: 0.9197142 (38)\ttotal: 5m 28s\tremaining: 2h 14m 56s\n",
      "39:\ttest: 0.9198778\tbest: 0.9198778 (39)\ttotal: 5m 36s\tremaining: 2h 14m 43s\n",
      "40:\ttest: 0.9201755\tbest: 0.9201755 (40)\ttotal: 5m 45s\tremaining: 2h 14m 38s\n",
      "41:\ttest: 0.9211278\tbest: 0.9211278 (41)\ttotal: 5m 53s\tremaining: 2h 14m 27s\n",
      "42:\ttest: 0.9211278\tbest: 0.9211278 (41)\ttotal: 6m 2s\tremaining: 2h 14m 38s\n",
      "43:\ttest: 0.9202052\tbest: 0.9211278 (41)\ttotal: 6m 11s\tremaining: 2h 14m 36s\n",
      "44:\ttest: 0.9215892\tbest: 0.9215892 (44)\ttotal: 6m 20s\tremaining: 2h 14m 24s\n",
      "45:\ttest: 0.9228392\tbest: 0.9228392 (45)\ttotal: 6m 28s\tremaining: 2h 14m 20s\n",
      "46:\ttest: 0.9213662\tbest: 0.9228392 (45)\ttotal: 6m 36s\tremaining: 2h 14m 8s\n",
      "47:\ttest: 0.9215299\tbest: 0.9228392 (45)\ttotal: 6m 45s\tremaining: 2h 13m 57s\n",
      "48:\ttest: 0.9219912\tbest: 0.9228392 (45)\ttotal: 6m 54s\tremaining: 2h 13m 56s\n",
      "49:\ttest: 0.9223185\tbest: 0.9228392 (45)\ttotal: 7m 2s\tremaining: 2h 13m 45s\n",
      "50:\ttest: 0.9223185\tbest: 0.9228392 (45)\ttotal: 7m 10s\tremaining: 2h 13m 33s\n",
      "51:\ttest: 0.9216935\tbest: 0.9228392 (45)\ttotal: 7m 18s\tremaining: 2h 13m 19s\n",
      "52:\ttest: 0.9218572\tbest: 0.9228392 (45)\ttotal: 7m 27s\tremaining: 2h 13m 7s\n",
      "53:\ttest: 0.9218572\tbest: 0.9228392 (45)\ttotal: 7m 35s\tremaining: 2h 12m 54s\n",
      "54:\ttest: 0.9218572\tbest: 0.9228392 (45)\ttotal: 7m 43s\tremaining: 2h 12m 50s\n",
      "55:\ttest: 0.9242979\tbest: 0.9242979 (55)\ttotal: 7m 53s\tremaining: 2h 12m 53s\n",
      "56:\ttest: 0.9242979\tbest: 0.9242979 (55)\ttotal: 8m 2s\tremaining: 2h 12m 55s\n",
      "57:\ttest: 0.9244616\tbest: 0.9244616 (57)\ttotal: 8m 10s\tremaining: 2h 12m 43s\n",
      "58:\ttest: 0.9246252\tbest: 0.9246252 (58)\ttotal: 8m 18s\tremaining: 2h 12m 32s\n",
      "59:\ttest: 0.9244616\tbest: 0.9246252 (58)\ttotal: 8m 26s\tremaining: 2h 12m 19s\n",
      "60:\ttest: 0.9240002\tbest: 0.9246252 (58)\ttotal: 8m 35s\tremaining: 2h 12m 13s\n",
      "61:\ttest: 0.9243276\tbest: 0.9246252 (58)\ttotal: 8m 43s\tremaining: 2h 12m\n",
      "62:\ttest: 0.9221845\tbest: 0.9246252 (58)\ttotal: 8m 51s\tremaining: 2h 11m 48s\n",
      "63:\ttest: 0.9221845\tbest: 0.9246252 (58)\ttotal: 9m\tremaining: 2h 11m 45s\n",
      "64:\ttest: 0.9221845\tbest: 0.9246252 (58)\ttotal: 9m 8s\tremaining: 2h 11m 34s\n",
      "65:\ttest: 0.9221845\tbest: 0.9246252 (58)\ttotal: 9m 17s\tremaining: 2h 11m 22s\n",
      "66:\ttest: 0.9226459\tbest: 0.9246252 (58)\ttotal: 9m 25s\tremaining: 2h 11m 17s\n",
      "67:\ttest: 0.9224822\tbest: 0.9246252 (58)\ttotal: 9m 34s\tremaining: 2h 11m 13s\n",
      "68:\ttest: 0.9224822\tbest: 0.9246252 (58)\ttotal: 9m 43s\tremaining: 2h 11m 7s\n",
      "69:\ttest: 0.9207709\tbest: 0.9246252 (58)\ttotal: 9m 51s\tremaining: 2h 10m 56s\n",
      "70:\ttest: 0.9231072\tbest: 0.9246252 (58)\ttotal: 9m 59s\tremaining: 2h 10m 44s\n",
      "71:\ttest: 0.9237322\tbest: 0.9246252 (58)\ttotal: 10m 8s\tremaining: 2h 10m 42s\n",
      "72:\ttest: 0.9231072\tbest: 0.9246252 (58)\ttotal: 10m 17s\tremaining: 2h 10m 36s\n",
      "73:\ttest: 0.9231072\tbest: 0.9246252 (58)\ttotal: 10m 25s\tremaining: 2h 10m 24s\n",
      "74:\ttest: 0.9226162\tbest: 0.9246252 (58)\ttotal: 10m 33s\tremaining: 2h 10m 18s\n",
      "75:\ttest: 0.9226162\tbest: 0.9246252 (58)\ttotal: 10m 42s\tremaining: 2h 10m 7s\n",
      "76:\ttest: 0.9226162\tbest: 0.9246252 (58)\ttotal: 10m 50s\tremaining: 2h 9m 56s\n",
      "77:\ttest: 0.9230776\tbest: 0.9246252 (58)\ttotal: 10m 58s\tremaining: 2h 9m 46s\n",
      "78:\ttest: 0.9230776\tbest: 0.9246252 (58)\ttotal: 11m 6s\tremaining: 2h 9m 35s\n",
      "79:\ttest: 0.9232412\tbest: 0.9246252 (58)\ttotal: 11m 15s\tremaining: 2h 9m 24s\n",
      "80:\ttest: 0.9232412\tbest: 0.9246252 (58)\ttotal: 11m 23s\tremaining: 2h 9m 12s\n",
      "81:\ttest: 0.9227799\tbest: 0.9246252 (58)\ttotal: 11m 31s\tremaining: 2h 9m 1s\n",
      "82:\ttest: 0.9234049\tbest: 0.9246252 (58)\ttotal: 11m 39s\tremaining: 2h 8m 51s\n",
      "83:\ttest: 0.9234049\tbest: 0.9246252 (58)\ttotal: 11m 48s\tremaining: 2h 8m 41s\n",
      "84:\ttest: 0.9238662\tbest: 0.9246252 (58)\ttotal: 11m 57s\tremaining: 2h 8m 43s\n",
      "85:\ttest: 0.9238662\tbest: 0.9246252 (58)\ttotal: 12m 5s\tremaining: 2h 8m 35s\n",
      "86:\ttest: 0.9238662\tbest: 0.9246252 (58)\ttotal: 12m 14s\tremaining: 2h 8m 25s\n",
      "87:\ttest: 0.9251162\tbest: 0.9251162 (87)\ttotal: 12m 22s\tremaining: 2h 8m 14s\n",
      "88:\ttest: 0.9246549\tbest: 0.9251162 (87)\ttotal: 12m 30s\tremaining: 2h 8m 4s\n",
      "89:\ttest: 0.9251162\tbest: 0.9251162 (87)\ttotal: 12m 39s\tremaining: 2h 8m 3s\n",
      "90:\ttest: 0.9251162\tbest: 0.9251162 (87)\ttotal: 12m 48s\tremaining: 2h 7m 52s\n",
      "91:\ttest: 0.9210982\tbest: 0.9251162 (87)\ttotal: 12m 56s\tremaining: 2h 7m 42s\n",
      "92:\ttest: 0.9209345\tbest: 0.9251162 (87)\ttotal: 13m 4s\tremaining: 2h 7m 34s\n",
      "93:\ttest: 0.9206369\tbest: 0.9251162 (87)\ttotal: 13m 13s\tremaining: 2h 7m 29s\n",
      "94:\ttest: 0.9212619\tbest: 0.9251162 (87)\ttotal: 13m 21s\tremaining: 2h 7m 19s\n",
      "95:\ttest: 0.9208005\tbest: 0.9251162 (87)\ttotal: 13m 30s\tremaining: 2h 7m 9s\n",
      "96:\ttest: 0.9208005\tbest: 0.9251162 (87)\ttotal: 13m 38s\tremaining: 2h 7m 1s\n",
      "97:\ttest: 0.9217232\tbest: 0.9251162 (87)\ttotal: 13m 47s\tremaining: 2h 6m 55s\n",
      "98:\ttest: 0.9209345\tbest: 0.9251162 (87)\ttotal: 13m 55s\tremaining: 2h 6m 44s\n",
      "99:\ttest: 0.9206369\tbest: 0.9251162 (87)\ttotal: 14m 4s\tremaining: 2h 6m 40s\n",
      "100:\ttest: 0.9215595\tbest: 0.9251162 (87)\ttotal: 14m 12s\tremaining: 2h 6m 31s\n",
      "101:\ttest: 0.9215595\tbest: 0.9251162 (87)\ttotal: 14m 21s\tremaining: 2h 6m 24s\n",
      "102:\ttest: 0.9246549\tbest: 0.9251162 (87)\ttotal: 14m 29s\tremaining: 2h 6m 14s\n",
      "103:\ttest: 0.9243572\tbest: 0.9251162 (87)\ttotal: 14m 37s\tremaining: 2h 6m 3s\n",
      "104:\ttest: 0.9242232\tbest: 0.9251162 (87)\ttotal: 14m 46s\tremaining: 2h 5m 59s\n",
      "105:\ttest: 0.9240595\tbest: 0.9251162 (87)\ttotal: 14m 55s\tremaining: 2h 5m 55s\n",
      "106:\ttest: 0.9240595\tbest: 0.9251162 (87)\ttotal: 15m 4s\tremaining: 2h 5m 51s\n",
      "107:\ttest: 0.9251459\tbest: 0.9251459 (107)\ttotal: 15m 13s\tremaining: 2h 5m 41s\n",
      "108:\ttest: 0.9269912\tbest: 0.9269912 (108)\ttotal: 15m 21s\tremaining: 2h 5m 36s\n",
      "109:\ttest: 0.9268276\tbest: 0.9269912 (108)\ttotal: 15m 30s\tremaining: 2h 5m 25s\n",
      "110:\ttest: 0.9259049\tbest: 0.9269912 (108)\ttotal: 15m 38s\tremaining: 2h 5m 17s\n",
      "111:\ttest: 0.9263662\tbest: 0.9269912 (108)\ttotal: 15m 47s\tremaining: 2h 5m 8s\n",
      "112:\ttest: 0.9263662\tbest: 0.9269912 (108)\ttotal: 15m 55s\tremaining: 2h 4m 58s\n",
      "113:\ttest: 0.9240595\tbest: 0.9269912 (108)\ttotal: 16m 3s\tremaining: 2h 4m 48s\n",
      "114:\ttest: 0.9259049\tbest: 0.9269912 (108)\ttotal: 16m 12s\tremaining: 2h 4m 41s\n",
      "115:\ttest: 0.9259049\tbest: 0.9269912 (108)\ttotal: 16m 20s\tremaining: 2h 4m 34s\n",
      "116:\ttest: 0.9259049\tbest: 0.9269912 (108)\ttotal: 16m 29s\tremaining: 2h 4m 28s\n",
      "117:\ttest: 0.9277502\tbest: 0.9277502 (117)\ttotal: 16m 38s\tremaining: 2h 4m 20s\n",
      "118:\ttest: 0.9282116\tbest: 0.9282116 (118)\ttotal: 16m 46s\tremaining: 2h 4m 14s\n",
      "119:\ttest: 0.9288366\tbest: 0.9288366 (119)\ttotal: 16m 55s\tremaining: 2h 4m 4s\n",
      "120:\ttest: 0.9285389\tbest: 0.9288366 (119)\ttotal: 17m 4s\tremaining: 2h 4m 2s\n",
      "121:\ttest: 0.9280776\tbest: 0.9288366 (119)\ttotal: 17m 12s\tremaining: 2h 3m 51s\n",
      "122:\ttest: 0.9293276\tbest: 0.9293276 (122)\ttotal: 17m 20s\tremaining: 2h 3m 41s\n",
      "123:\ttest: 0.9288662\tbest: 0.9293276 (122)\ttotal: 17m 29s\tremaining: 2h 3m 34s\n",
      "124:\ttest: 0.9305776\tbest: 0.9305776 (124)\ttotal: 17m 38s\tremaining: 2h 3m 26s\n",
      "125:\ttest: 0.9301162\tbest: 0.9305776 (124)\ttotal: 17m 46s\tremaining: 2h 3m 15s\n",
      "126:\ttest: 0.9301162\tbest: 0.9305776 (124)\ttotal: 17m 54s\tremaining: 2h 3m 6s\n",
      "127:\ttest: 0.9301162\tbest: 0.9305776 (124)\ttotal: 18m 2s\tremaining: 2h 2m 56s\n",
      "128:\ttest: 0.9305776\tbest: 0.9305776 (128)\ttotal: 18m 11s\tremaining: 2h 2m 46s\n",
      "129:\ttest: 0.9302799\tbest: 0.9305776 (128)\ttotal: 18m 19s\tremaining: 2h 2m 37s\n",
      "130:\ttest: 0.9307412\tbest: 0.9307412 (130)\ttotal: 18m 27s\tremaining: 2h 2m 27s\n",
      "131:\ttest: 0.9302799\tbest: 0.9307412 (130)\ttotal: 18m 35s\tremaining: 2h 2m 17s\n",
      "132:\ttest: 0.9307412\tbest: 0.9307412 (130)\ttotal: 18m 44s\tremaining: 2h 2m 7s\n",
      "133:\ttest: 0.9307412\tbest: 0.9307412 (130)\ttotal: 18m 52s\tremaining: 2h 2m\n",
      "134:\ttest: 0.9309049\tbest: 0.9309049 (134)\ttotal: 19m 1s\tremaining: 2h 1m 51s\n",
      "135:\ttest: 0.9318276\tbest: 0.9318276 (135)\ttotal: 19m 9s\tremaining: 2h 1m 40s\n",
      "136:\ttest: 0.9318276\tbest: 0.9318276 (135)\ttotal: 19m 17s\tremaining: 2h 1m 33s\n",
      "137:\ttest: 0.9319616\tbest: 0.9319616 (137)\ttotal: 19m 26s\tremaining: 2h 1m 23s\n",
      "138:\ttest: 0.9315002\tbest: 0.9319616 (137)\ttotal: 19m 34s\tremaining: 2h 1m 14s\n",
      "139:\ttest: 0.9315002\tbest: 0.9319616 (137)\ttotal: 19m 43s\tremaining: 2h 1m 7s\n",
      "140:\ttest: 0.9315002\tbest: 0.9319616 (137)\ttotal: 19m 51s\tremaining: 2h 59s\n",
      "141:\ttest: 0.9319616\tbest: 0.9319616 (137)\ttotal: 20m\tremaining: 2h 50s\n",
      "142:\ttest: 0.9319616\tbest: 0.9319616 (137)\ttotal: 20m 8s\tremaining: 2h 43s\n",
      "143:\ttest: 0.9319616\tbest: 0.9319616 (137)\ttotal: 20m 16s\tremaining: 2h 33s\n",
      "144:\ttest: 0.9315002\tbest: 0.9319616 (137)\ttotal: 20m 25s\tremaining: 2h 24s\n",
      "145:\ttest: 0.9319616\tbest: 0.9319616 (137)\ttotal: 20m 34s\tremaining: 2h 18s\n",
      "146:\ttest: 0.9324229\tbest: 0.9324229 (146)\ttotal: 20m 42s\tremaining: 2h 9s\n",
      "147:\ttest: 0.9322592\tbest: 0.9324229 (146)\ttotal: 20m 50s\tremaining: 1h 59m 59s\n",
      "148:\ttest: 0.9317979\tbest: 0.9324229 (146)\ttotal: 20m 58s\tremaining: 1h 59m 50s\n",
      "149:\ttest: 0.9322592\tbest: 0.9324229 (146)\ttotal: 21m 7s\tremaining: 1h 59m 43s\n",
      "150:\ttest: 0.9327206\tbest: 0.9327206 (150)\ttotal: 21m 15s\tremaining: 1h 59m 34s\n",
      "151:\ttest: 0.9316342\tbest: 0.9327206 (150)\ttotal: 21m 24s\tremaining: 1h 59m 24s\n",
      "152:\ttest: 0.9316342\tbest: 0.9327206 (150)\ttotal: 21m 32s\tremaining: 1h 59m 15s\n",
      "153:\ttest: 0.9316342\tbest: 0.9327206 (150)\ttotal: 21m 41s\tremaining: 1h 59m 8s\n",
      "154:\ttest: 0.9328842\tbest: 0.9328842 (154)\ttotal: 21m 49s\tremaining: 1h 59m\n",
      "155:\ttest: 0.9328842\tbest: 0.9328842 (154)\ttotal: 21m 58s\tremaining: 1h 58m 51s\n",
      "156:\ttest: 0.9328842\tbest: 0.9328842 (154)\ttotal: 22m 6s\tremaining: 1h 58m 41s\n",
      "157:\ttest: 0.9336433\tbest: 0.9336433 (157)\ttotal: 22m 14s\tremaining: 1h 58m 32s\n",
      "158:\ttest: 0.9338069\tbest: 0.9338069 (158)\ttotal: 22m 23s\tremaining: 1h 58m 23s\n",
      "159:\ttest: 0.9328842\tbest: 0.9338069 (158)\ttotal: 22m 31s\tremaining: 1h 58m 14s\n",
      "160:\ttest: 0.9333456\tbest: 0.9338069 (158)\ttotal: 22m 39s\tremaining: 1h 58m 4s\n",
      "161:\ttest: 0.9341046\tbest: 0.9341046 (161)\ttotal: 22m 48s\tremaining: 1h 57m 59s\n",
      "162:\ttest: 0.9341046\tbest: 0.9341046 (161)\ttotal: 22m 56s\tremaining: 1h 57m 49s\n",
      "163:\ttest: 0.9341046\tbest: 0.9341046 (161)\ttotal: 23m 5s\tremaining: 1h 57m 40s\n",
      "164:\ttest: 0.9341046\tbest: 0.9341046 (161)\ttotal: 23m 13s\tremaining: 1h 57m 31s\n",
      "165:\ttest: 0.9345659\tbest: 0.9345659 (165)\ttotal: 23m 21s\tremaining: 1h 57m 22s\n",
      "166:\ttest: 0.9345659\tbest: 0.9345659 (165)\ttotal: 23m 29s\tremaining: 1h 57m 12s\n",
      "167:\ttest: 0.9345659\tbest: 0.9345659 (165)\ttotal: 23m 38s\tremaining: 1h 57m 2s\n",
      "168:\ttest: 0.9345659\tbest: 0.9345659 (165)\ttotal: 23m 46s\tremaining: 1h 56m 53s\n",
      "169:\ttest: 0.9345659\tbest: 0.9345659 (165)\ttotal: 23m 54s\tremaining: 1h 56m 44s\n",
      "170:\ttest: 0.9345659\tbest: 0.9345659 (165)\ttotal: 24m 2s\tremaining: 1h 56m 35s\n",
      "171:\ttest: 0.9344023\tbest: 0.9345659 (165)\ttotal: 24m 11s\tremaining: 1h 56m 25s\n",
      "172:\ttest: 0.9344023\tbest: 0.9345659 (165)\ttotal: 24m 20s\tremaining: 1h 56m 20s\n",
      "173:\ttest: 0.9344023\tbest: 0.9345659 (165)\ttotal: 24m 28s\tremaining: 1h 56m 11s\n",
      "174:\ttest: 0.9344023\tbest: 0.9345659 (165)\ttotal: 24m 36s\tremaining: 1h 56m 1s\n",
      "175:\ttest: 0.9348636\tbest: 0.9348636 (175)\ttotal: 24m 44s\tremaining: 1h 55m 52s\n",
      "176:\ttest: 0.9353249\tbest: 0.9353249 (176)\ttotal: 24m 53s\tremaining: 1h 55m 42s\n",
      "177:\ttest: 0.9353249\tbest: 0.9353249 (176)\ttotal: 25m 1s\tremaining: 1h 55m 35s\n",
      "178:\ttest: 0.9353249\tbest: 0.9353249 (176)\ttotal: 25m 10s\tremaining: 1h 55m 26s\n",
      "179:\ttest: 0.9353249\tbest: 0.9353249 (176)\ttotal: 25m 18s\tremaining: 1h 55m 19s\n",
      "180:\ttest: 0.9353249\tbest: 0.9353249 (176)\ttotal: 25m 28s\tremaining: 1h 55m 14s\n",
      "181:\ttest: 0.9353249\tbest: 0.9353249 (176)\ttotal: 25m 36s\tremaining: 1h 55m 6s\n",
      "182:\ttest: 0.9348636\tbest: 0.9353249 (176)\ttotal: 25m 44s\tremaining: 1h 54m 57s\n",
      "183:\ttest: 0.9353249\tbest: 0.9353249 (176)\ttotal: 25m 53s\tremaining: 1h 54m 48s\n",
      "184:\ttest: 0.9357863\tbest: 0.9357863 (184)\ttotal: 26m 1s\tremaining: 1h 54m 38s\n",
      "185:\ttest: 0.9357863\tbest: 0.9357863 (184)\ttotal: 26m 9s\tremaining: 1h 54m 29s\n",
      "186:\ttest: 0.9357863\tbest: 0.9357863 (184)\ttotal: 26m 17s\tremaining: 1h 54m 19s\n",
      "187:\ttest: 0.9359499\tbest: 0.9359499 (187)\ttotal: 26m 26s\tremaining: 1h 54m 10s\n",
      "188:\ttest: 0.9359499\tbest: 0.9359499 (187)\ttotal: 26m 34s\tremaining: 1h 54m\n",
      "189:\ttest: 0.9359499\tbest: 0.9359499 (187)\ttotal: 26m 42s\tremaining: 1h 53m 51s\n",
      "190:\ttest: 0.9359499\tbest: 0.9359499 (187)\ttotal: 26m 50s\tremaining: 1h 53m 42s\n",
      "191:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 26m 58s\tremaining: 1h 53m 32s\n",
      "192:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 27m 7s\tremaining: 1h 53m 23s\n",
      "193:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 27m 15s\tremaining: 1h 53m 16s\n",
      "194:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 27m 24s\tremaining: 1h 53m 7s\n",
      "195:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 27m 32s\tremaining: 1h 52m 58s\n",
      "196:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 27m 41s\tremaining: 1h 52m 50s\n",
      "197:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 27m 49s\tremaining: 1h 52m 41s\n",
      "198:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 27m 57s\tremaining: 1h 52m 32s\n",
      "199:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 28m 5s\tremaining: 1h 52m 23s\n",
      "200:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 28m 14s\tremaining: 1h 52m 16s\n",
      "201:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 28m 22s\tremaining: 1h 52m 6s\n",
      "202:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 28m 30s\tremaining: 1h 51m 57s\n",
      "203:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 28m 39s\tremaining: 1h 51m 47s\n",
      "204:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 28m 48s\tremaining: 1h 51m 42s\n",
      "205:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 28m 56s\tremaining: 1h 51m 32s\n",
      "206:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 29m 4s\tremaining: 1h 51m 23s\n",
      "207:\ttest: 0.9354886\tbest: 0.9359499 (187)\ttotal: 29m 13s\tremaining: 1h 51m 16s\n",
      "208:\ttest: 0.9350273\tbest: 0.9359499 (187)\ttotal: 29m 21s\tremaining: 1h 51m 7s\n",
      "209:\ttest: 0.9345659\tbest: 0.9359499 (187)\ttotal: 29m 29s\tremaining: 1h 50m 57s\n",
      "210:\ttest: 0.9345659\tbest: 0.9359499 (187)\ttotal: 29m 38s\tremaining: 1h 50m 49s\n",
      "211:\ttest: 0.9345659\tbest: 0.9359499 (187)\ttotal: 29m 46s\tremaining: 1h 50m 40s\n",
      "212:\ttest: 0.9344023\tbest: 0.9359499 (187)\ttotal: 29m 54s\tremaining: 1h 50m 31s\n",
      "213:\ttest: 0.9344023\tbest: 0.9359499 (187)\ttotal: 30m 3s\tremaining: 1h 50m 24s\n",
      "214:\ttest: 0.9342386\tbest: 0.9359499 (187)\ttotal: 30m 11s\tremaining: 1h 50m 15s\n",
      "215:\ttest: 0.9342386\tbest: 0.9359499 (187)\ttotal: 30m 20s\tremaining: 1h 50m 6s\n",
      "216:\ttest: 0.9342386\tbest: 0.9359499 (187)\ttotal: 30m 28s\tremaining: 1h 49m 59s\n",
      "217:\ttest: 0.9346999\tbest: 0.9359499 (187)\ttotal: 30m 37s\tremaining: 1h 49m 50s\n",
      "218:\ttest: 0.9342386\tbest: 0.9359499 (187)\ttotal: 30m 45s\tremaining: 1h 49m 41s\n",
      "219:\ttest: 0.9342386\tbest: 0.9359499 (187)\ttotal: 30m 53s\tremaining: 1h 49m 32s\n",
      "220:\ttest: 0.9342386\tbest: 0.9359499 (187)\ttotal: 31m 2s\tremaining: 1h 49m 23s\n",
      "221:\ttest: 0.9342386\tbest: 0.9359499 (187)\ttotal: 31m 10s\tremaining: 1h 49m 16s\n",
      "222:\ttest: 0.9342386\tbest: 0.9359499 (187)\ttotal: 31m 19s\tremaining: 1h 49m 8s\n",
      "223:\ttest: 0.9342386\tbest: 0.9359499 (187)\ttotal: 31m 28s\tremaining: 1h 49m 1s\n",
      "224:\ttest: 0.9346999\tbest: 0.9359499 (187)\ttotal: 31m 36s\tremaining: 1h 48m 52s\n",
      "225:\ttest: 0.9346999\tbest: 0.9359499 (187)\ttotal: 31m 44s\tremaining: 1h 48m 43s\n",
      "226:\ttest: 0.9346999\tbest: 0.9359499 (187)\ttotal: 31m 52s\tremaining: 1h 48m 34s\n",
      "227:\ttest: 0.9346999\tbest: 0.9359499 (187)\ttotal: 32m 2s\tremaining: 1h 48m 28s\n",
      "228:\ttest: 0.9346999\tbest: 0.9359499 (187)\ttotal: 32m 10s\tremaining: 1h 48m 19s\n",
      "229:\ttest: 0.9356523\tbest: 0.9359499 (187)\ttotal: 32m 19s\tremaining: 1h 48m 12s\n",
      "230:\ttest: 0.9351909\tbest: 0.9359499 (187)\ttotal: 32m 27s\tremaining: 1h 48m 2s\n",
      "231:\ttest: 0.9351909\tbest: 0.9359499 (187)\ttotal: 32m 35s\tremaining: 1h 47m 53s\n",
      "232:\ttest: 0.9356523\tbest: 0.9359499 (187)\ttotal: 32m 43s\tremaining: 1h 47m 44s\n",
      "233:\ttest: 0.9350273\tbest: 0.9359499 (187)\ttotal: 32m 52s\tremaining: 1h 47m 37s\n",
      "234:\ttest: 0.9350273\tbest: 0.9359499 (187)\ttotal: 33m\tremaining: 1h 47m 28s\n",
      "235:\ttest: 0.9361136\tbest: 0.9361136 (235)\ttotal: 33m 9s\tremaining: 1h 47m 21s\n",
      "236:\ttest: 0.9361136\tbest: 0.9361136 (235)\ttotal: 33m 17s\tremaining: 1h 47m 11s\n",
      "237:\ttest: 0.9356523\tbest: 0.9361136 (235)\ttotal: 33m 26s\tremaining: 1h 47m 2s\n",
      "238:\ttest: 0.9356523\tbest: 0.9361136 (235)\ttotal: 33m 34s\tremaining: 1h 46m 53s\n",
      "239:\ttest: 0.9365749\tbest: 0.9365749 (239)\ttotal: 33m 43s\tremaining: 1h 46m 48s\n",
      "240:\ttest: 0.9365749\tbest: 0.9365749 (240)\ttotal: 33m 51s\tremaining: 1h 46m 38s\n",
      "241:\ttest: 0.9365749\tbest: 0.9365749 (240)\ttotal: 33m 59s\tremaining: 1h 46m 29s\n",
      "242:\ttest: 0.9365749\tbest: 0.9365749 (240)\ttotal: 34m 8s\tremaining: 1h 46m 20s\n",
      "243:\ttest: 0.9365749\tbest: 0.9365749 (240)\ttotal: 34m 16s\tremaining: 1h 46m 11s\n",
      "244:\ttest: 0.9365749\tbest: 0.9365749 (240)\ttotal: 34m 24s\tremaining: 1h 46m 2s\n",
      "245:\ttest: 0.9365749\tbest: 0.9365749 (240)\ttotal: 34m 33s\tremaining: 1h 45m 55s\n",
      "246:\ttest: 0.9365749\tbest: 0.9365749 (240)\ttotal: 34m 42s\tremaining: 1h 45m 48s\n",
      "247:\ttest: 0.9365749\tbest: 0.9365749 (240)\ttotal: 34m 50s\tremaining: 1h 45m 39s\n",
      "248:\ttest: 0.9365749\tbest: 0.9365749 (240)\ttotal: 34m 59s\tremaining: 1h 45m 31s\n",
      "249:\ttest: 0.9365749\tbest: 0.9365749 (240)\ttotal: 35m 7s\tremaining: 1h 45m 22s\n",
      "250:\ttest: 0.9365749\tbest: 0.9365749 (240)\ttotal: 35m 15s\tremaining: 1h 45m 13s\n",
      "251:\ttest: 0.9371999\tbest: 0.9371999 (251)\ttotal: 35m 24s\tremaining: 1h 45m 4s\n",
      "252:\ttest: 0.9371999\tbest: 0.9371999 (251)\ttotal: 35m 32s\tremaining: 1h 44m 55s\n",
      "253:\ttest: 0.9371999\tbest: 0.9371999 (251)\ttotal: 35m 40s\tremaining: 1h 44m 46s\n",
      "254:\ttest: 0.9371999\tbest: 0.9371999 (251)\ttotal: 35m 49s\tremaining: 1h 44m 40s\n",
      "255:\ttest: 0.9371999\tbest: 0.9371999 (251)\ttotal: 35m 57s\tremaining: 1h 44m 31s\n",
      "256:\ttest: 0.9364409\tbest: 0.9371999 (251)\ttotal: 36m 6s\tremaining: 1h 44m 22s\n",
      "257:\ttest: 0.9358159\tbest: 0.9371999 (251)\ttotal: 36m 14s\tremaining: 1h 44m 13s\n",
      "258:\ttest: 0.9359796\tbest: 0.9371999 (251)\ttotal: 36m 23s\tremaining: 1h 44m 6s\n",
      "259:\ttest: 0.9359796\tbest: 0.9371999 (251)\ttotal: 36m 31s\tremaining: 1h 43m 58s\n",
      "260:\ttest: 0.9358159\tbest: 0.9371999 (251)\ttotal: 36m 40s\tremaining: 1h 43m 51s\n",
      "261:\ttest: 0.9358159\tbest: 0.9371999 (251)\ttotal: 36m 49s\tremaining: 1h 43m 43s\n",
      "262:\ttest: 0.9362773\tbest: 0.9371999 (251)\ttotal: 36m 58s\tremaining: 1h 43m 36s\n",
      "263:\ttest: 0.9371999\tbest: 0.9371999 (251)\ttotal: 37m 6s\tremaining: 1h 43m 27s\n",
      "264:\ttest: 0.9375273\tbest: 0.9375273 (264)\ttotal: 37m 15s\tremaining: 1h 43m 20s\n",
      "265:\ttest: 0.9379886\tbest: 0.9379886 (265)\ttotal: 37m 23s\tremaining: 1h 43m 12s\n",
      "266:\ttest: 0.9379886\tbest: 0.9379886 (265)\ttotal: 37m 32s\tremaining: 1h 43m 4s\n",
      "267:\ttest: 0.9386136\tbest: 0.9386136 (267)\ttotal: 37m 41s\tremaining: 1h 42m 56s\n",
      "268:\ttest: 0.9386136\tbest: 0.9386136 (267)\ttotal: 37m 50s\tremaining: 1h 42m 50s\n",
      "269:\ttest: 0.9381523\tbest: 0.9386136 (267)\ttotal: 37m 59s\tremaining: 1h 42m 42s\n",
      "270:\ttest: 0.9381523\tbest: 0.9386136 (267)\ttotal: 38m 7s\tremaining: 1h 42m 33s\n",
      "271:\ttest: 0.9381523\tbest: 0.9386136 (267)\ttotal: 38m 16s\tremaining: 1h 42m 25s\n",
      "272:\ttest: 0.9381523\tbest: 0.9386136 (267)\ttotal: 38m 24s\tremaining: 1h 42m 16s\n",
      "273:\ttest: 0.9386136\tbest: 0.9386136 (267)\ttotal: 38m 33s\tremaining: 1h 42m 9s\n",
      "274:\ttest: 0.9386136\tbest: 0.9386136 (267)\ttotal: 38m 41s\tremaining: 1h 42m\n",
      "275:\ttest: 0.9381523\tbest: 0.9386136 (267)\ttotal: 38m 50s\tremaining: 1h 41m 53s\n",
      "276:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 38m 58s\tremaining: 1h 41m 44s\n",
      "277:\ttest: 0.9376909\tbest: 0.9386136 (267)\ttotal: 39m 7s\tremaining: 1h 41m 36s\n",
      "278:\ttest: 0.9376909\tbest: 0.9386136 (267)\ttotal: 39m 16s\tremaining: 1h 41m 29s\n",
      "279:\ttest: 0.9376909\tbest: 0.9386136 (267)\ttotal: 39m 25s\tremaining: 1h 41m 23s\n",
      "280:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 39m 34s\tremaining: 1h 41m 16s\n",
      "281:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 39m 43s\tremaining: 1h 41m 8s\n",
      "282:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 39m 51s\tremaining: 1h 40m 59s\n",
      "283:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 39m 59s\tremaining: 1h 40m 50s\n",
      "284:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 40m 8s\tremaining: 1h 40m 42s\n",
      "285:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 40m 16s\tremaining: 1h 40m 33s\n",
      "286:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 40m 25s\tremaining: 1h 40m 26s\n",
      "287:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 40m 33s\tremaining: 1h 40m 17s\n",
      "288:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 40m 42s\tremaining: 1h 40m 8s\n",
      "289:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 40m 50s\tremaining: 1h 39m 59s\n",
      "290:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 40m 59s\tremaining: 1h 39m 51s\n",
      "291:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 41m 7s\tremaining: 1h 39m 43s\n",
      "292:\ttest: 0.9372296\tbest: 0.9386136 (267)\ttotal: 41m 16s\tremaining: 1h 39m 34s\n",
      "293:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 41m 25s\tremaining: 1h 39m 29s\n",
      "294:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 41m 34s\tremaining: 1h 39m 21s\n",
      "295:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 41m 42s\tremaining: 1h 39m 13s\n",
      "296:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 41m 51s\tremaining: 1h 39m 4s\n",
      "297:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 41m 59s\tremaining: 1h 38m 56s\n",
      "298:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 42m 8s\tremaining: 1h 38m 48s\n",
      "299:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 42m 17s\tremaining: 1h 38m 40s\n",
      "300:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 42m 25s\tremaining: 1h 38m 31s\n",
      "301:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 42m 34s\tremaining: 1h 38m 23s\n",
      "302:\ttest: 0.9361433\tbest: 0.9386136 (267)\ttotal: 42m 42s\tremaining: 1h 38m 14s\n",
      "303:\ttest: 0.9361433\tbest: 0.9386136 (267)\ttotal: 42m 51s\tremaining: 1h 38m 6s\n",
      "304:\ttest: 0.9361433\tbest: 0.9386136 (267)\ttotal: 43m\tremaining: 1h 37m 59s\n",
      "305:\ttest: 0.9361433\tbest: 0.9386136 (267)\ttotal: 43m 8s\tremaining: 1h 37m 50s\n",
      "306:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 43m 16s\tremaining: 1h 37m 42s\n",
      "307:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 43m 25s\tremaining: 1h 37m 33s\n",
      "308:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 43m 34s\tremaining: 1h 37m 25s\n",
      "309:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 43m 43s\tremaining: 1h 37m 19s\n",
      "310:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 43m 51s\tremaining: 1h 37m 10s\n",
      "311:\ttest: 0.9361433\tbest: 0.9386136 (267)\ttotal: 43m 59s\tremaining: 1h 37m 1s\n",
      "312:\ttest: 0.9361433\tbest: 0.9386136 (267)\ttotal: 44m 8s\tremaining: 1h 36m 53s\n",
      "313:\ttest: 0.9361433\tbest: 0.9386136 (267)\ttotal: 44m 16s\tremaining: 1h 36m 44s\n",
      "314:\ttest: 0.9361433\tbest: 0.9386136 (267)\ttotal: 44m 26s\tremaining: 1h 36m 37s\n",
      "315:\ttest: 0.9361433\tbest: 0.9386136 (267)\ttotal: 44m 34s\tremaining: 1h 36m 30s\n",
      "316:\ttest: 0.9366046\tbest: 0.9386136 (267)\ttotal: 44m 44s\tremaining: 1h 36m 23s\n",
      "317:\ttest: 0.9361433\tbest: 0.9386136 (267)\ttotal: 44m 53s\tremaining: 1h 36m 15s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.9386136107\n",
      "bestIteration = 267\n",
      "\n",
      "Shrink model to first 268 iterations.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-13-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-13-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluation Metrics (Validation Set):\n",
      "Accuracy@1 : 0.8612\n",
      "Recall@3   : 0.9875\n",
      "MRR        : 0.9246\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-13-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cat boost takes it again, with a larger margin this time. Think this is the way to go. Next thing to do is to finalise the whole pipeline and do one final train."
   ],
   "metadata": {
    "id": "SMBxua118hi_"
   }
  }
 ]
}