{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Training - GP data added, pruned templates.\n",
        "\n",
        "Final training on full padded data. Catboost added. GP Data added in as well. Pruned templates to just necessary. No normalisation templates. Training both LightGBM and CatBoost."
      ],
      "metadata": {
        "id": "DjE0M1VsiF7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm\n",
        "!pip install sqlalchemy\n",
        "!pip install catboost\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "yNrhVNxUiAcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560ef149-1f45-44b2-fd69-5097904a395e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightgbm\n",
            "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.16.1)\n",
            "Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightgbm\n",
            "Successfully installed lightgbm-4.6.0\n",
            "Collecting sqlalchemy\n",
            "  Downloading sqlalchemy-2.0.43-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting greenlet>=1 (from sqlalchemy)\n",
            "  Downloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (4.14.1)\n",
            "Downloading sqlalchemy-2.0.43-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: greenlet, sqlalchemy\n",
            "Successfully installed greenlet-3.2.4 sqlalchemy-2.0.43\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting graphviz (from catboost)\n",
            "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: graphviz, catboost\n",
            "Successfully installed catboost-1.2.8 graphviz-0.21\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cat boost takes it again, with a larger margin this time. Think this is the way to go. Next thing to do is to finalise the whole pipeline and do one final train."
      ],
      "metadata": {
        "id": "SMBxua118hi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRanker, Pool\n",
        "from typing import Tuple\n",
        "\n",
        "# Paths to files in Drive\n",
        "_DB_PATH = \"/content/drive/MyDrive/Colab Notebooks/database.db\"\n",
        "_STD_VAL_IDS = \"val_std_ids.csv\"\n",
        "_STD_TEST_IDS = \"test_std_ids.csv\"\n",
        "_COMP_VAL_IDS = \"val_comp_ids.csv\"\n",
        "_COMP_TEST_IDS = \"test_comp_ids.csv\"\n",
        "\n",
        "# Tuned hyperparameters\n",
        "_BEST_PARAMS = {\n",
        "    \"loss_function\": \"YetiRank\",\n",
        "    \"eval_metric\": \"NDCG:top=3\",\n",
        "    \"random_seed\": 42,\n",
        "    \"learning_rate\": 0.13275757957731918,\n",
        "    \"depth\": 6,\n",
        "    \"l2_leaf_reg\": 7.142519331365267,\n",
        "    \"random_strength\": 3.395785387976391,\n",
        "    \"min_data_in_leaf\": 84,\n",
        "    \"subsample\": 0.9048958560910838,\n",
        "    \"colsample_bylevel\": 0.511123337191838,\n",
        "    \"grow_policy\": \"Lossguide\",\n",
        "}\n",
        "\n",
        "\n",
        "def _compute_ranking_metrics(df: pd.DataFrame, k: int = 3):\n",
        "    \"\"\"\n",
        "    Compute Accuracy@1, Recall@k, and MRR for a ranking prediction dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Must contain columns ['clean_row_id', 'score', 'label']\n",
        "        k (int): The cutoff rank for recall@k\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float, float]: (Accuracy@1, Recall@k, MRR)\n",
        "    \"\"\"\n",
        "    # Accuracy@1\n",
        "    top1 = df.loc[df.groupby(\"clean_row_id\")[\"score\"].idxmax()]\n",
        "    acc1 = (top1[\"label\"] == 1).mean()\n",
        "\n",
        "    # Recall@k\n",
        "    topk = df.groupby(\"clean_row_id\", group_keys=False).apply(\n",
        "        lambda g: g.nlargest(k, \"score\")\n",
        "    )\n",
        "    recall_k = topk.groupby(\"clean_row_id\")[\"label\"].max().mean()\n",
        "\n",
        "    # MRR\n",
        "    def reciprocal_rank(g: pd.DataFrame) -> float:\n",
        "        labels_sorted = g.sort_values(\"score\", ascending=False)[\"label\"].to_numpy()\n",
        "        for rank, label in enumerate(labels_sorted, start=1):\n",
        "            if label == 1:\n",
        "                return 1.0 / rank\n",
        "        return 0.0\n",
        "\n",
        "    mrr = df.groupby(\"clean_row_id\", group_keys=False).apply(reciprocal_rank).mean()\n",
        "\n",
        "    return acc1, recall_k, mrr\n",
        "\n",
        "\n",
        "def _train_catboost_model(\n",
        "    parameters: dict,\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    n_rounds: int = 500,\n",
        "    model_output_path: str = \"catboost_model.cbm\",\n",
        ") -> CatBoostRanker:\n",
        "    \"\"\"\n",
        "    Trains a CatBoost ranking model using the provided training and validation data.\n",
        "\n",
        "    Args:\n",
        "        parameters (dict): Parameters for CatBoostRanker.\n",
        "        train_df (pd.DataFrame): Training data with label, group info, and features.\n",
        "        val_df (pd.DataFrame): Validation data with same structure.\n",
        "        n_rounds (int): Maximum number of boosting rounds.\n",
        "        model_output_path (str): File path to save the trained CatBoost model.\n",
        "\n",
        "    Returns:\n",
        "        CatBoostRanker: Trained CatBoost model.\n",
        "    \"\"\"\n",
        "    drop_cols = [\"label\", \"clean_row_id\", \"investor\", \"firm\", \"template_id\"]\n",
        "\n",
        "    # Train\n",
        "    train_group_sizes = train_df.groupby(\"clean_row_id\", sort=False).size().tolist()\n",
        "    train_group_id = np.repeat(np.arange(len(train_group_sizes)), train_group_sizes)\n",
        "\n",
        "    X_train = train_df.drop(columns=drop_cols)\n",
        "    y_train = train_df[\"label\"]\n",
        "    del train_df  # Free memory early\n",
        "\n",
        "    train_pool = Pool(data=X_train, label=y_train, group_id=train_group_id)\n",
        "    del X_train, y_train, train_group_id  # Free memory\n",
        "    gc.collect()  # Call garbage collector to be extra sure\n",
        "\n",
        "    # Validation\n",
        "    val_group_sizes = val_df.groupby(\"clean_row_id\", sort=False).size().tolist()\n",
        "    val_group_id = np.repeat(np.arange(len(val_group_sizes)), val_group_sizes)\n",
        "\n",
        "    X_val = val_df.drop(columns=drop_cols)\n",
        "    y_val = val_df[\"label\"]\n",
        "\n",
        "    val_pool = Pool(data=X_val, label=y_val, group_id=val_group_id)\n",
        "    del X_val, y_val, val_group_id  # Free memory\n",
        "    gc.collect()  # Call garbage collector to be extra sure\n",
        "\n",
        "    # Train model\n",
        "    model = CatBoostRanker(iterations=n_rounds, **parameters)\n",
        "    model.fit(\n",
        "        train_pool,\n",
        "        eval_set=val_pool,\n",
        "        early_stopping_rounds=10,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    model.save_model(model_output_path)\n",
        "    print(f\"\\nModel saved to: {model_output_path}\")\n",
        "\n",
        "    # Score model\n",
        "    val_df = val_df.copy()  # preserve original structure\n",
        "    val_df[\"score\"] = model.predict(val_pool)\n",
        "\n",
        "    acc1, recall3, mrr = _compute_ranking_metrics(val_df, k=3)\n",
        "\n",
        "    print(\"\\nEvaluation Metrics (Validation Set):\")\n",
        "    print(f\"Accuracy@1 : {acc1:.4f}\")\n",
        "    print(f\"Recall@3   : {recall3:.4f}\")\n",
        "    print(f\"MRR        : {mrr:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_standard_and_complex_model(\n",
        "    n_rounds: int = 1000,\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains two CatBoost ranking models (standard and complex) on pre-split data and saves them to\n",
        "    disk.\n",
        "\n",
        "    The models are saved in `.cbm` format for compatibility with CatBoost's C++ inference engine.\n",
        "\n",
        "    Args:\n",
        "        n_rounds (int): Maximum number of boosting rounds for training (default: 1000).\n",
        "    \"\"\"\n",
        "    # Mount drive\n",
        "    import sys\n",
        "\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import drive\n",
        "\n",
        "        drive.mount(\"/content/drive\")\n",
        "\n",
        "    def train_model(\n",
        "        data_table: str, val_ids_path: str, test_ids_path: str, model_path: str\n",
        "    ):\n",
        "        print(f\"Training model: {model_path}\")\n",
        "        # Get ids\n",
        "        val_ids = (\n",
        "            pd.read_csv(val_ids_path)[\"val_ids\"].dropna().astype(int).tolist()\n",
        "        )\n",
        "        test_ids = (\n",
        "            pd.read_csv(test_ids_path)[\"test_ids\"].dropna().astype(int).tolist()\n",
        "        )\n",
        "\n",
        "        # Get data\n",
        "        with sqlite3.connect(_DB_PATH) as conn:\n",
        "            full_df = pd.read_sql_query(f\"SELECT * FROM {data_table}\", conn)\n",
        "\n",
        "        # Split the set\n",
        "        val_ids_set = set(val_ids)\n",
        "        test_ids_set = set(test_ids)\n",
        "        excluded_ids = val_ids_set | test_ids_set\n",
        "        val_df = full_df[full_df[\"clean_row_id\"].isin(val_ids_set)]\n",
        "        full_df = full_df[~full_df[\"clean_row_id\"].isin(excluded_ids)]\n",
        "\n",
        "        # Train model\n",
        "        return _train_catboost_model(\n",
        "            _BEST_PARAMS, full_df, val_df, n_rounds, model_path\n",
        "        )\n",
        "\n",
        "    # Start with standard\n",
        "    train_model(\"feature_matrix\", _STD_VAL_IDS, _STD_TEST_IDS, \"std_lightgbm_model.cbm\")\n",
        "    # Then complex\n",
        "    train_model(\"feature_matrix_complex\", _COMP_VAL_IDS, _COMP_TEST_IDS, \"comp_lightgbm_model.cbm\")\n",
        "\n",
        "    # return std_model, comp_model\n",
        "    return None\n",
        "\n",
        "\n",
        "train_standard_and_complex_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETPXIgXdjtD0",
        "outputId": "43efde7a-83cb-410a-dece-02e7f4074644"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Training model: std_lightgbm_model.cbm\n",
            "0:\ttest: 0.9716152\tbest: 0.9716152 (0)\ttotal: 1m 11s\tremaining: 19h 57m 53s\n",
            "1:\ttest: 0.9734437\tbest: 0.9734437 (1)\ttotal: 2m 34s\tremaining: 21h 22m 30s\n",
            "2:\ttest: 0.9740315\tbest: 0.9740315 (2)\ttotal: 3m 55s\tremaining: 21h 43m 48s\n",
            "3:\ttest: 0.9741940\tbest: 0.9741940 (3)\ttotal: 5m 8s\tremaining: 21h 21m 14s\n",
            "4:\ttest: 0.9742098\tbest: 0.9742098 (4)\ttotal: 6m 26s\tremaining: 21h 23m 26s\n",
            "5:\ttest: 0.9742733\tbest: 0.9742733 (5)\ttotal: 7m 52s\tremaining: 21h 45m 49s\n",
            "6:\ttest: 0.9746967\tbest: 0.9746967 (6)\ttotal: 9m 15s\tremaining: 21h 52m 14s\n",
            "7:\ttest: 0.9747648\tbest: 0.9747648 (7)\ttotal: 10m 40s\tremaining: 22h 3m 8s\n",
            "8:\ttest: 0.9748850\tbest: 0.9748850 (8)\ttotal: 11m 58s\tremaining: 21h 58m 50s\n",
            "9:\ttest: 0.9748730\tbest: 0.9748850 (8)\ttotal: 13m 13s\tremaining: 21h 49m 33s\n",
            "10:\ttest: 0.9748934\tbest: 0.9748934 (10)\ttotal: 14m 20s\tremaining: 21h 28m 48s\n",
            "11:\ttest: 0.9749946\tbest: 0.9749946 (11)\ttotal: 15m 28s\tremaining: 21h 14m 6s\n",
            "12:\ttest: 0.9750156\tbest: 0.9750156 (12)\ttotal: 16m 48s\tremaining: 21h 15m 41s\n",
            "13:\ttest: 0.9750487\tbest: 0.9750487 (13)\ttotal: 17m 58s\tremaining: 21h 6m 7s\n",
            "14:\ttest: 0.9750760\tbest: 0.9750760 (14)\ttotal: 19m 16s\tremaining: 21h 6m 9s\n",
            "15:\ttest: 0.9751295\tbest: 0.9751295 (15)\ttotal: 20m 29s\tremaining: 21h 22s\n",
            "16:\ttest: 0.9751053\tbest: 0.9751295 (15)\ttotal: 21m 47s\tremaining: 20h 59m 47s\n",
            "17:\ttest: 0.9751620\tbest: 0.9751620 (17)\ttotal: 23m 7s\tremaining: 21h 1m 9s\n",
            "18:\ttest: 0.9751160\tbest: 0.9751620 (17)\ttotal: 24m 17s\tremaining: 20h 53m 57s\n",
            "19:\ttest: 0.9751191\tbest: 0.9751620 (17)\ttotal: 25m 29s\tremaining: 20h 49m 6s\n",
            "20:\ttest: 0.9750478\tbest: 0.9751620 (17)\ttotal: 26m 47s\tremaining: 20h 48m 44s\n",
            "21:\ttest: 0.9751502\tbest: 0.9751620 (17)\ttotal: 27m 53s\tremaining: 20h 40m 1s\n",
            "22:\ttest: 0.9751623\tbest: 0.9751623 (22)\ttotal: 29m 6s\tremaining: 20h 36m 32s\n",
            "23:\ttest: 0.9751649\tbest: 0.9751649 (23)\ttotal: 30m 18s\tremaining: 20h 32m 46s\n",
            "24:\ttest: 0.9752488\tbest: 0.9752488 (24)\ttotal: 31m 26s\tremaining: 20h 25m 56s\n",
            "25:\ttest: 0.9752431\tbest: 0.9752488 (24)\ttotal: 32m 30s\tremaining: 20h 17m 41s\n",
            "26:\ttest: 0.9752278\tbest: 0.9752488 (24)\ttotal: 33m 45s\tremaining: 20h 16m 22s\n",
            "27:\ttest: 0.9752368\tbest: 0.9752488 (24)\ttotal: 34m 51s\tremaining: 20h 10m 3s\n",
            "28:\ttest: 0.9752520\tbest: 0.9752520 (28)\ttotal: 36m 12s\tremaining: 20h 12m 20s\n",
            "29:\ttest: 0.9751959\tbest: 0.9752520 (28)\ttotal: 37m 28s\tremaining: 20h 11m 46s\n",
            "30:\ttest: 0.9752189\tbest: 0.9752520 (28)\ttotal: 38m 39s\tremaining: 20h 8m 12s\n",
            "31:\ttest: 0.9752342\tbest: 0.9752520 (28)\ttotal: 39m 50s\tremaining: 20h 5m 13s\n",
            "32:\ttest: 0.9752767\tbest: 0.9752767 (32)\ttotal: 41m 1s\tremaining: 20h 2m 10s\n",
            "33:\ttest: 0.9751844\tbest: 0.9752767 (32)\ttotal: 42m 13s\tremaining: 19h 59m 31s\n",
            "34:\ttest: 0.9752494\tbest: 0.9752767 (32)\ttotal: 43m 28s\tremaining: 19h 58m 36s\n",
            "35:\ttest: 0.9752258\tbest: 0.9752767 (32)\ttotal: 44m 36s\tremaining: 19h 54m 37s\n",
            "36:\ttest: 0.9752080\tbest: 0.9752767 (32)\ttotal: 45m 46s\tremaining: 19h 51m 14s\n",
            "37:\ttest: 0.9752227\tbest: 0.9752767 (32)\ttotal: 46m 59s\tremaining: 19h 49m 29s\n",
            "38:\ttest: 0.9752474\tbest: 0.9752767 (32)\ttotal: 48m 16s\tremaining: 19h 49m 39s\n",
            "39:\ttest: 0.9752626\tbest: 0.9752767 (32)\ttotal: 49m 30s\tremaining: 19h 48m 17s\n",
            "40:\ttest: 0.9752385\tbest: 0.9752767 (32)\ttotal: 50m 40s\tremaining: 19h 45m 12s\n",
            "41:\ttest: 0.9752532\tbest: 0.9752767 (32)\ttotal: 52m 2s\tremaining: 19h 46m 59s\n",
            "42:\ttest: 0.9752442\tbest: 0.9752767 (32)\ttotal: 53m 26s\tremaining: 19h 49m 12s\n",
            "Stopped by overfitting detector  (10 iterations wait)\n",
            "\n",
            "bestTest = 0.9752767378\n",
            "bestIteration = 32\n",
            "\n",
            "Shrink model to first 33 iterations.\n",
            "\n",
            "Model saved to: std_lightgbm_model.cbm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3337695897.py:47: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  topk = df.groupby(\"clean_row_id\", group_keys=False).apply(\n",
            "/tmp/ipython-input-3337695897.py:60: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  mrr = df.groupby(\"clean_row_id\", group_keys=False).apply(reciprocal_rank).mean()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics (Validation Set):\n",
            "Accuracy@1 : 0.9383\n",
            "Recall@3   : 0.9982\n",
            "MRR        : 0.9677\n",
            "Training model: comp_lightgbm_model.cbm\n",
            "0:\ttest: 0.7612923\tbest: 0.7612923 (0)\ttotal: 2m 27s\tremaining: 1d 16h 55m 3s\n",
            "1:\ttest: 0.7849957\tbest: 0.7849957 (1)\ttotal: 4m 52s\tremaining: 1d 16h 33m\n",
            "2:\ttest: 0.8231467\tbest: 0.8231467 (2)\ttotal: 7m 5s\tremaining: 1d 15h 16m 16s\n",
            "3:\ttest: 0.8260953\tbest: 0.8260953 (3)\ttotal: 9m 25s\tremaining: 1d 15h 6m 23s\n",
            "4:\ttest: 0.8329959\tbest: 0.8329959 (4)\ttotal: 11m 37s\tremaining: 1d 14h 34m 54s\n",
            "5:\ttest: 0.8372667\tbest: 0.8372667 (5)\ttotal: 14m\tremaining: 1d 14h 41m 44s\n",
            "6:\ttest: 0.8368083\tbest: 0.8372667 (5)\ttotal: 16m 16s\tremaining: 1d 14h 29m 38s\n",
            "7:\ttest: 0.8483319\tbest: 0.8483319 (7)\ttotal: 18m 42s\tremaining: 1d 14h 40m 18s\n",
            "8:\ttest: 0.8495576\tbest: 0.8495576 (8)\ttotal: 21m 10s\tremaining: 1d 14h 50m 48s\n",
            "9:\ttest: 0.8614866\tbest: 0.8614866 (9)\ttotal: 23m 23s\tremaining: 1d 14h 35m 43s\n",
            "10:\ttest: 0.8773826\tbest: 0.8773826 (10)\ttotal: 25m 44s\tremaining: 1d 14h 34m 42s\n",
            "11:\ttest: 0.8793805\tbest: 0.8793805 (11)\ttotal: 28m 1s\tremaining: 1d 14h 27m 11s\n",
            "12:\ttest: 0.8826718\tbest: 0.8826718 (12)\ttotal: 30m 27s\tremaining: 1d 14h 32m 31s\n",
            "13:\ttest: 0.8846020\tbest: 0.8846020 (13)\ttotal: 32m 42s\tremaining: 1d 14h 23m 24s\n",
            "14:\ttest: 0.8881248\tbest: 0.8881248 (14)\ttotal: 34m 59s\tremaining: 1d 14h 17m 49s\n",
            "15:\ttest: 0.8903880\tbest: 0.8903880 (15)\ttotal: 37m 14s\tremaining: 1d 14h 10m 13s\n",
            "16:\ttest: 0.8926463\tbest: 0.8926463 (16)\ttotal: 39m 33s\tremaining: 1d 14h 7m 27s\n",
            "17:\ttest: 0.8971825\tbest: 0.8971825 (17)\ttotal: 41m 47s\tremaining: 1d 14h 21s\n",
            "18:\ttest: 0.8992961\tbest: 0.8992961 (18)\ttotal: 44m 2s\tremaining: 1d 13h 53m 38s\n",
            "19:\ttest: 0.9005460\tbest: 0.9005460 (19)\ttotal: 46m 20s\tremaining: 1d 13h 50m 31s\n",
            "20:\ttest: 0.9015498\tbest: 0.9015498 (20)\ttotal: 48m 33s\tremaining: 1d 13h 43m 30s\n",
            "21:\ttest: 0.9023701\tbest: 0.9023701 (21)\ttotal: 50m 46s\tremaining: 1d 13h 36m 54s\n",
            "22:\ttest: 0.9037696\tbest: 0.9037696 (22)\ttotal: 52m 54s\tremaining: 1d 13h 27m 44s\n",
            "23:\ttest: 0.9050726\tbest: 0.9050726 (23)\ttotal: 55m 4s\tremaining: 1d 13h 19m 25s\n",
            "24:\ttest: 0.9060763\tbest: 0.9060763 (24)\ttotal: 57m 13s\tremaining: 1d 13h 11m 41s\n",
            "25:\ttest: 0.9061729\tbest: 0.9061729 (25)\ttotal: 59m 21s\tremaining: 1d 13h 3m 54s\n",
            "26:\ttest: 0.9073793\tbest: 0.9073793 (26)\ttotal: 1h 1m 34s\tremaining: 1d 12h 59m 13s\n",
            "27:\ttest: 0.9083927\tbest: 0.9083927 (27)\ttotal: 1h 3m 52s\tremaining: 1d 12h 57m 35s\n",
            "28:\ttest: 0.9081996\tbest: 0.9083927 (27)\ttotal: 1h 6m 10s\tremaining: 1d 12h 55m 31s\n",
            "29:\ttest: 0.9078908\tbest: 0.9083927 (27)\ttotal: 1h 8m 27s\tremaining: 1d 12h 53m 19s\n",
            "30:\ttest: 0.9087980\tbest: 0.9087980 (30)\ttotal: 1h 10m 46s\tremaining: 1d 12h 52m 19s\n",
            "31:\ttest: 0.9093530\tbest: 0.9093530 (31)\ttotal: 1h 12m 55s\tremaining: 1d 12h 46m\n",
            "32:\ttest: 0.9101106\tbest: 0.9101106 (32)\ttotal: 1h 15m 4s\tremaining: 1d 12h 39m 58s\n",
            "33:\ttest: 0.9097149\tbest: 0.9101106 (32)\ttotal: 1h 17m 16s\tremaining: 1d 12h 35m 42s\n",
            "34:\ttest: 0.9102698\tbest: 0.9102698 (34)\ttotal: 1h 19m 25s\tremaining: 1d 12h 29m 45s\n",
            "35:\ttest: 0.9107717\tbest: 0.9107717 (35)\ttotal: 1h 21m 43s\tremaining: 1d 12h 28m 21s\n",
            "36:\ttest: 0.9114232\tbest: 0.9114232 (36)\ttotal: 1h 23m 56s\tremaining: 1d 12h 24m 42s\n",
            "37:\ttest: 0.9120312\tbest: 0.9120312 (37)\ttotal: 1h 26m 9s\tremaining: 1d 12h 21m 5s\n",
            "38:\ttest: 0.9125427\tbest: 0.9125427 (38)\ttotal: 1h 28m 18s\tremaining: 1d 12h 15m 56s\n",
            "39:\ttest: 0.9124896\tbest: 0.9125427 (38)\ttotal: 1h 30m 40s\tremaining: 1d 12h 16m 6s\n",
            "40:\ttest: 0.9124896\tbest: 0.9125427 (38)\ttotal: 1h 33m 8s\tremaining: 1d 12h 18m 42s\n",
            "41:\ttest: 0.9133534\tbest: 0.9133534 (41)\ttotal: 1h 35m 22s\tremaining: 1d 12h 15m 33s\n",
            "42:\ttest: 0.9137492\tbest: 0.9137492 (42)\ttotal: 1h 37m 35s\tremaining: 1d 12h 11m 49s\n",
            "43:\ttest: 0.9142945\tbest: 0.9142945 (43)\ttotal: 1h 39m 48s\tremaining: 1d 12h 8m 33s\n",
            "44:\ttest: 0.9139326\tbest: 0.9142945 (43)\ttotal: 1h 42m 14s\tremaining: 1d 12h 9m 38s\n",
            "45:\ttest: 0.9144441\tbest: 0.9144441 (45)\ttotal: 1h 44m 35s\tremaining: 1d 12h 9m 10s\n",
            "46:\ttest: 0.9141353\tbest: 0.9144441 (45)\ttotal: 1h 46m 56s\tremaining: 1d 12h 8m 30s\n",
            "47:\ttest: 0.9141353\tbest: 0.9144441 (45)\ttotal: 1h 49m 14s\tremaining: 1d 12h 6m 34s\n",
            "48:\ttest: 0.9145310\tbest: 0.9145310 (48)\ttotal: 1h 51m 30s\tremaining: 1d 12h 4m 19s\n",
            "49:\ttest: 0.9145841\tbest: 0.9145841 (49)\ttotal: 1h 53m 59s\tremaining: 1d 12h 5m 55s\n",
            "50:\ttest: 0.9145841\tbest: 0.9145841 (49)\ttotal: 1h 56m 12s\tremaining: 1d 12h 2m 27s\n",
            "51:\ttest: 0.9149364\tbest: 0.9149364 (51)\ttotal: 1h 58m 30s\tremaining: 1d 12h 22s\n",
            "52:\ttest: 0.9150425\tbest: 0.9150425 (52)\ttotal: 2h 59s\tremaining: 1d 12h 1m 53s\n",
            "53:\ttest: 0.9160124\tbest: 0.9160124 (53)\ttotal: 2h 3m 13s\tremaining: 1d 11h 58m 49s\n",
            "54:\ttest: 0.9159593\tbest: 0.9160124 (53)\ttotal: 2h 5m 43s\tremaining: 1d 12h 8s\n",
            "55:\ttest: 0.9166639\tbest: 0.9166639 (55)\ttotal: 2h 7m 52s\tremaining: 1d 11h 55m 39s\n",
            "56:\ttest: 0.9169727\tbest: 0.9169727 (56)\ttotal: 2h 10m 14s\tremaining: 1d 11h 54m 48s\n",
            "57:\ttest: 0.9185845\tbest: 0.9185845 (57)\ttotal: 2h 12m 27s\tremaining: 1d 11h 51m 24s\n",
            "58:\ttest: 0.9185845\tbest: 0.9185845 (57)\ttotal: 2h 14m 40s\tremaining: 1d 11h 47m 56s\n",
            "59:\ttest: 0.9179330\tbest: 0.9185845 (57)\ttotal: 2h 16m 57s\tremaining: 1d 11h 45m 37s\n",
            "60:\ttest: 0.9177834\tbest: 0.9185845 (57)\ttotal: 2h 19m 27s\tremaining: 1d 11h 46m 38s\n",
            "61:\ttest: 0.9189368\tbest: 0.9189368 (61)\ttotal: 2h 21m 35s\tremaining: 1d 11h 42m 7s\n",
            "62:\ttest: 0.9200467\tbest: 0.9200467 (62)\ttotal: 2h 23m 48s\tremaining: 1d 11h 38m 52s\n",
            "63:\ttest: 0.9198971\tbest: 0.9200467 (62)\ttotal: 2h 26m 6s\tremaining: 1d 11h 36m 43s\n",
            "64:\ttest: 0.9198971\tbest: 0.9200467 (62)\ttotal: 2h 28m 23s\tremaining: 1d 11h 34m 27s\n",
            "65:\ttest: 0.9200467\tbest: 0.9200467 (62)\ttotal: 2h 30m 32s\tremaining: 1d 11h 30m 23s\n",
            "66:\ttest: 0.9202494\tbest: 0.9202494 (66)\ttotal: 2h 32m 49s\tremaining: 1d 11h 28m 11s\n",
            "67:\ttest: 0.9203990\tbest: 0.9203990 (67)\ttotal: 2h 35m 12s\tremaining: 1d 11h 27m 9s\n",
            "68:\ttest: 0.9206982\tbest: 0.9206982 (68)\ttotal: 2h 37m 27s\tremaining: 1d 11h 24m 31s\n",
            "69:\ttest: 0.9209443\tbest: 0.9209443 (69)\ttotal: 2h 39m 44s\tremaining: 1d 11h 22m 15s\n",
            "70:\ttest: 0.9209443\tbest: 0.9209443 (69)\ttotal: 2h 41m 58s\tremaining: 1d 11h 19m 23s\n",
            "71:\ttest: 0.9207947\tbest: 0.9209443 (69)\ttotal: 2h 44m 15s\tremaining: 1d 11h 17m 5s\n",
            "72:\ttest: 0.9212966\tbest: 0.9212966 (72)\ttotal: 2h 46m 28s\tremaining: 1d 11h 13m 58s\n",
            "73:\ttest: 0.9211470\tbest: 0.9212966 (72)\ttotal: 2h 48m 40s\tremaining: 1d 11h 10m 47s\n",
            "74:\ttest: 0.9214462\tbest: 0.9214462 (74)\ttotal: 2h 50m 57s\tremaining: 1d 11h 8m 27s\n",
            "75:\ttest: 0.9220977\tbest: 0.9220977 (75)\ttotal: 2h 53m 10s\tremaining: 1d 11h 5m 25s\n",
            "76:\ttest: 0.9220012\tbest: 0.9220977 (75)\ttotal: 2h 55m 35s\tremaining: 1d 11h 4m 54s\n",
            "77:\ttest: 0.9216923\tbest: 0.9220977 (75)\ttotal: 2h 57m 52s\tremaining: 1d 11h 2m 38s\n",
            "78:\ttest: 0.9228022\tbest: 0.9228022 (78)\ttotal: 3h 1s\tremaining: 1d 10h 58m 50s\n",
            "79:\ttest: 0.9225996\tbest: 0.9228022 (78)\ttotal: 3h 2m 14s\tremaining: 1d 10h 55m 49s\n",
            "80:\ttest: 0.9225996\tbest: 0.9228022 (78)\ttotal: 3h 4m 31s\tremaining: 1d 10h 53m 38s\n",
            "81:\ttest: 0.9224500\tbest: 0.9228022 (78)\ttotal: 3h 6m 45s\tremaining: 1d 10h 50m 43s\n",
            "82:\ttest: 0.9224500\tbest: 0.9228022 (78)\ttotal: 3h 9m 6s\tremaining: 1d 10h 49m 21s\n",
            "83:\ttest: 0.9227492\tbest: 0.9228022 (78)\ttotal: 3h 11m 19s\tremaining: 1d 10h 46m 23s\n",
            "84:\ttest: 0.9225996\tbest: 0.9228022 (78)\ttotal: 3h 13m 37s\tremaining: 1d 10h 44m 18s\n",
            "85:\ttest: 0.9231014\tbest: 0.9231014 (85)\ttotal: 3h 15m 52s\tremaining: 1d 10h 41m 40s\n",
            "86:\ttest: 0.9232510\tbest: 0.9232510 (86)\ttotal: 3h 18m 9s\tremaining: 1d 10h 39m 30s\n",
            "87:\ttest: 0.9228988\tbest: 0.9232510 (86)\ttotal: 3h 20m 22s\tremaining: 1d 10h 36m 32s\n",
            "88:\ttest: 0.9229518\tbest: 0.9232510 (86)\ttotal: 3h 22m 43s\tremaining: 1d 10h 35m 4s\n",
            "89:\ttest: 0.9228553\tbest: 0.9232510 (86)\ttotal: 3h 24m 58s\tremaining: 1d 10h 32m 35s\n",
            "90:\ttest: 0.9228553\tbest: 0.9232510 (86)\ttotal: 3h 27m 13s\tremaining: 1d 10h 29m 55s\n",
            "91:\ttest: 0.9232076\tbest: 0.9232510 (86)\ttotal: 3h 29m 34s\tremaining: 1d 10h 28m 26s\n",
            "92:\ttest: 0.9233137\tbest: 0.9233137 (92)\ttotal: 3h 31m 48s\tremaining: 1d 10h 25m 41s\n",
            "93:\ttest: 0.9233137\tbest: 0.9233137 (92)\ttotal: 3h 34m 9s\tremaining: 1d 10h 24m 9s\n",
            "94:\ttest: 0.9237095\tbest: 0.9237095 (94)\ttotal: 3h 36m 31s\tremaining: 1d 10h 22m 39s\n",
            "95:\ttest: 0.9238591\tbest: 0.9238591 (95)\ttotal: 3h 38m 59s\tremaining: 1d 10h 22m 10s\n",
            "96:\ttest: 0.9238156\tbest: 0.9238591 (95)\ttotal: 3h 41m 16s\tremaining: 1d 10h 19m 54s\n",
            "97:\ttest: 0.9247228\tbest: 0.9247228 (97)\ttotal: 3h 43m 25s\tremaining: 1d 10h 16m 28s\n",
            "98:\ttest: 0.9247228\tbest: 0.9247228 (97)\ttotal: 3h 45m 43s\tremaining: 1d 10h 14m 18s\n",
            "99:\ttest: 0.9247228\tbest: 0.9247228 (97)\ttotal: 3h 48m\tremaining: 1d 10h 12m 1s\n",
            "100:\ttest: 0.9250221\tbest: 0.9250221 (100)\ttotal: 3h 50m 13s\tremaining: 1d 10h 9m 14s\n",
            "101:\ttest: 0.9249786\tbest: 0.9250221 (100)\ttotal: 3h 52m 30s\tremaining: 1d 10h 6m 58s\n",
            "102:\ttest: 0.9252778\tbest: 0.9252778 (102)\ttotal: 3h 54m 46s\tremaining: 1d 10h 4m 39s\n",
            "103:\ttest: 0.9255770\tbest: 0.9255770 (103)\ttotal: 3h 57m 4s\tremaining: 1d 10h 2m 30s\n",
            "104:\ttest: 0.9254274\tbest: 0.9255770 (103)\ttotal: 3h 59m 22s\tremaining: 1d 10h 20s\n",
            "105:\ttest: 0.9254274\tbest: 0.9255770 (103)\ttotal: 4h 1m 39s\tremaining: 1d 9h 58m 4s\n",
            "106:\ttest: 0.9254274\tbest: 0.9255770 (103)\ttotal: 4h 3m 59s\tremaining: 1d 9h 56m 22s\n",
            "107:\ttest: 0.9259727\tbest: 0.9259727 (107)\ttotal: 4h 6m 13s\tremaining: 1d 9h 53m 35s\n",
            "108:\ttest: 0.9264215\tbest: 0.9264215 (108)\ttotal: 4h 8m 27s\tremaining: 1d 9h 51m 2s\n",
            "109:\ttest: 0.9264215\tbest: 0.9264215 (108)\ttotal: 4h 10m 50s\tremaining: 1d 9h 49m 32s\n",
            "110:\ttest: 0.9264215\tbest: 0.9264215 (108)\ttotal: 4h 13m 7s\tremaining: 1d 9h 47m 16s\n",
            "111:\ttest: 0.9265712\tbest: 0.9265712 (111)\ttotal: 4h 15m 39s\tremaining: 1d 9h 47m\n",
            "112:\ttest: 0.9267208\tbest: 0.9267208 (112)\ttotal: 4h 17m 54s\tremaining: 1d 9h 44m 31s\n",
            "113:\ttest: 0.9271696\tbest: 0.9271696 (113)\ttotal: 4h 20m 15s\tremaining: 1d 9h 42m 40s\n",
            "114:\ttest: 0.9271696\tbest: 0.9271696 (113)\ttotal: 4h 22m 30s\tremaining: 1d 9h 40m 13s\n",
            "115:\ttest: 0.9271696\tbest: 0.9271696 (113)\ttotal: 4h 24m 50s\tremaining: 1d 9h 38m 12s\n",
            "116:\ttest: 0.9268269\tbest: 0.9271696 (113)\ttotal: 4h 27m 3s\tremaining: 1d 9h 35m 27s\n",
            "117:\ttest: 0.9269765\tbest: 0.9271696 (113)\ttotal: 4h 29m 15s\tremaining: 1d 9h 32m 38s\n",
            "118:\ttest: 0.9272757\tbest: 0.9272757 (118)\ttotal: 4h 31m 38s\tremaining: 1d 9h 31m 1s\n",
            "119:\ttest: 0.9271261\tbest: 0.9272757 (118)\ttotal: 4h 34m\tremaining: 1d 9h 29m 24s\n",
            "120:\ttest: 0.9269765\tbest: 0.9272757 (118)\ttotal: 4h 36m 14s\tremaining: 1d 9h 26m 47s\n",
            "121:\ttest: 0.9272323\tbest: 0.9272757 (118)\ttotal: 4h 38m 29s\tremaining: 1d 9h 24m 14s\n",
            "122:\ttest: 0.9272323\tbest: 0.9272757 (118)\ttotal: 4h 41m\tremaining: 1d 9h 23m 38s\n",
            "123:\ttest: 0.9270826\tbest: 0.9272757 (118)\ttotal: 4h 43m 23s\tremaining: 1d 9h 22m 3s\n",
            "124:\ttest: 0.9270826\tbest: 0.9272757 (118)\ttotal: 4h 45m 38s\tremaining: 1d 9h 19m 30s\n",
            "125:\ttest: 0.9270296\tbest: 0.9272757 (118)\ttotal: 4h 47m 55s\tremaining: 1d 9h 17m 11s\n",
            "126:\ttest: 0.9270296\tbest: 0.9272757 (118)\ttotal: 4h 50m 4s\tremaining: 1d 9h 13m 57s\n",
            "127:\ttest: 0.9273288\tbest: 0.9273288 (127)\ttotal: 4h 52m 30s\tremaining: 1d 9h 12m 40s\n",
            "128:\ttest: 0.9271792\tbest: 0.9273288 (127)\ttotal: 4h 54m 47s\tremaining: 1d 9h 10m 26s\n",
            "129:\ttest: 0.9271792\tbest: 0.9273288 (127)\ttotal: 4h 57m 9s\tremaining: 1d 9h 8m 39s\n",
            "130:\ttest: 0.9271792\tbest: 0.9273288 (127)\ttotal: 4h 59m 26s\tremaining: 1d 9h 6m 19s\n",
            "131:\ttest: 0.9273288\tbest: 0.9273288 (131)\ttotal: 5h 1m 38s\tremaining: 1d 9h 3m 33s\n",
            "132:\ttest: 0.9273288\tbest: 0.9273288 (131)\ttotal: 5h 3m 58s\tremaining: 1d 9h 1m 34s\n",
            "133:\ttest: 0.9271792\tbest: 0.9273288 (131)\ttotal: 5h 6m 16s\tremaining: 1d 8h 59m 19s\n",
            "134:\ttest: 0.9272323\tbest: 0.9273288 (131)\ttotal: 5h 8m 37s\tremaining: 1d 8h 57m 27s\n",
            "135:\ttest: 0.9272853\tbest: 0.9273288 (131)\ttotal: 5h 11m 3s\tremaining: 1d 8h 56m 7s\n",
            "136:\ttest: 0.9273915\tbest: 0.9273915 (136)\ttotal: 5h 13m 16s\tremaining: 1d 8h 53m 26s\n",
            "137:\ttest: 0.9273915\tbest: 0.9273915 (136)\ttotal: 5h 15m 38s\tremaining: 1d 8h 51m 37s\n",
            "138:\ttest: 0.9273915\tbest: 0.9273915 (136)\ttotal: 5h 17m 55s\tremaining: 1d 8h 49m 18s\n",
            "139:\ttest: 0.9276472\tbest: 0.9276472 (139)\ttotal: 5h 20m 23s\tremaining: 1d 8h 48m 8s\n",
            "140:\ttest: 0.9276376\tbest: 0.9276472 (139)\ttotal: 5h 22m 45s\tremaining: 1d 8h 46m 19s\n",
            "141:\ttest: 0.9272853\tbest: 0.9276472 (139)\ttotal: 5h 24m 58s\tremaining: 1d 8h 43m 37s\n",
            "142:\ttest: 0.9274880\tbest: 0.9276472 (139)\ttotal: 5h 27m 20s\tremaining: 1d 8h 41m 45s\n",
            "143:\ttest: 0.9274880\tbest: 0.9276472 (139)\ttotal: 5h 29m 37s\tremaining: 1d 8h 39m 27s\n",
            "144:\ttest: 0.9274349\tbest: 0.9276472 (139)\ttotal: 5h 31m 50s\tremaining: 1d 8h 36m 41s\n",
            "145:\ttest: 0.9273384\tbest: 0.9276472 (139)\ttotal: 5h 34m 3s\tremaining: 1d 8h 33m 59s\n",
            "146:\ttest: 0.9271357\tbest: 0.9276472 (139)\ttotal: 5h 36m 16s\tremaining: 1d 8h 31m 17s\n",
            "147:\ttest: 0.9273384\tbest: 0.9276472 (139)\ttotal: 5h 38m 34s\tremaining: 1d 8h 29m 4s\n",
            "148:\ttest: 0.9272853\tbest: 0.9276472 (139)\ttotal: 5h 40m 51s\tremaining: 1d 8h 26m 49s\n",
            "149:\ttest: 0.9270296\tbest: 0.9276472 (139)\ttotal: 5h 43m 5s\tremaining: 1d 8h 24m 10s\n",
            "Stopped by overfitting detector  (10 iterations wait)\n",
            "\n",
            "bestTest = 0.9276472161\n",
            "bestIteration = 139\n",
            "\n",
            "Shrink model to first 140 iterations.\n",
            "\n",
            "Model saved to: comp_lightgbm_model.cbm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3337695897.py:47: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  topk = df.groupby(\"clean_row_id\", group_keys=False).apply(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics (Validation Set):\n",
            "Accuracy@1 : 0.8419\n",
            "Recall@3   : 0.9850\n",
            "MRR        : 0.9114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3337695897.py:60: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  mrr = df.groupby(\"clean_row_id\", group_keys=False).apply(reciprocal_rank).mean()\n"
          ]
        }
      ]
    }
  ]
}