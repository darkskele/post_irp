{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Training - GP data added, pruned templates - no ultra rares.\n",
        "\n",
        "Real final training, for real this time."
      ],
      "metadata": {
        "id": "DjE0M1VsiF7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm\n",
        "!pip install sqlalchemy\n",
        "!pip install catboost\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "yNrhVNxUiAcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1e105a-ec61-49f1-bd38-ff4490ae4af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightgbm\n",
            "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.1)\n",
            "Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightgbm\n",
            "Successfully installed lightgbm-4.6.0\n",
            "Collecting sqlalchemy\n",
            "  Downloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting greenlet>=1 (from sqlalchemy)\n",
            "  Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (4.14.1)\n",
            "Downloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (607 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.6/607.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: greenlet, sqlalchemy\n",
            "Successfully installed greenlet-3.2.4 sqlalchemy-2.0.43\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting graphviz (from catboost)\n",
            "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: graphviz, catboost\n",
            "Successfully installed catboost-1.2.8 graphviz-0.21\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRanker, Pool\n",
        "from typing import Tuple\n",
        "\n",
        "# Paths to files in Drive\n",
        "_DB_PATH = \"/content/drive/MyDrive/Colab Notebooks/database.db\"\n",
        "_STD_VAL_IDS = \"val_std_ids.csv\"\n",
        "_STD_TEST_IDS = \"test_std_ids.csv\"\n",
        "_COMP_VAL_IDS = \"val_comp_ids.csv\"\n",
        "_COMP_TEST_IDS = \"test_comp_ids.csv\"\n",
        "\n",
        "# Tuned hyperparameters\n",
        "_BEST_PARAMS = {\n",
        "    \"loss_function\": \"YetiRank\",\n",
        "    \"eval_metric\": \"NDCG:top=3\",\n",
        "    \"random_seed\": 42,\n",
        "    \"learning_rate\": 0.13275757957731918,\n",
        "    \"depth\": 6,\n",
        "    \"l2_leaf_reg\": 7.142519331365267,\n",
        "    \"random_strength\": 3.395785387976391,\n",
        "    \"min_data_in_leaf\": 84,\n",
        "    \"subsample\": 0.9048958560910838,\n",
        "    \"colsample_bylevel\": 0.511123337191838,\n",
        "    \"grow_policy\": \"Lossguide\",\n",
        "}\n",
        "\n",
        "\n",
        "def _compute_ranking_metrics(df: pd.DataFrame, k: int = 3):\n",
        "    \"\"\"\n",
        "    Compute Accuracy@1, Recall@k, and MRR for a ranking prediction dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Must contain columns ['clean_row_id', 'score', 'label']\n",
        "        k (int): The cutoff rank for recall@k\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float, float]: (Accuracy@1, Recall@k, MRR)\n",
        "    \"\"\"\n",
        "    # Accuracy@1\n",
        "    top1 = df.loc[df.groupby(\"clean_row_id\")[\"score\"].idxmax()]\n",
        "    acc1 = (top1[\"label\"] == 1).mean()\n",
        "\n",
        "    # Recall@k\n",
        "    topk = df.groupby(\"clean_row_id\", group_keys=False).apply(\n",
        "        lambda g: g.nlargest(k, \"score\")\n",
        "    )\n",
        "    recall_k = topk.groupby(\"clean_row_id\")[\"label\"].max().mean()\n",
        "\n",
        "    # MRR\n",
        "    def reciprocal_rank(g: pd.DataFrame) -> float:\n",
        "        labels_sorted = g.sort_values(\"score\", ascending=False)[\"label\"].to_numpy()\n",
        "        for rank, label in enumerate(labels_sorted, start=1):\n",
        "            if label == 1:\n",
        "                return 1.0 / rank\n",
        "        return 0.0\n",
        "\n",
        "    mrr = df.groupby(\"clean_row_id\", group_keys=False).apply(reciprocal_rank).mean()\n",
        "\n",
        "    return acc1, recall_k, mrr\n",
        "\n",
        "\n",
        "def _train_catboost_model(\n",
        "    parameters: dict,\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    n_rounds: int = 500,\n",
        "    model_output_path: str = \"catboost_model.cbm\",\n",
        ") -> CatBoostRanker:\n",
        "    \"\"\"\n",
        "    Trains a CatBoost ranking model using the provided training and validation data.\n",
        "\n",
        "    Args:\n",
        "        parameters (dict): Parameters for CatBoostRanker.\n",
        "        train_df (pd.DataFrame): Training data with label, group info, and features.\n",
        "        val_df (pd.DataFrame): Validation data with same structure.\n",
        "        n_rounds (int): Maximum number of boosting rounds.\n",
        "        model_output_path (str): File path to save the trained CatBoost model.\n",
        "\n",
        "    Returns:\n",
        "        CatBoostRanker: Trained CatBoost model.\n",
        "    \"\"\"\n",
        "    drop_cols = [\"label\", \"clean_row_id\", \"investor\", \"firm\", \"template_id\"]\n",
        "\n",
        "    # Train\n",
        "    train_group_sizes = train_df.groupby(\"clean_row_id\", sort=False).size().tolist()\n",
        "    train_group_id = np.repeat(np.arange(len(train_group_sizes)), train_group_sizes)\n",
        "\n",
        "    X_train = train_df.drop(columns=drop_cols)\n",
        "    y_train = train_df[\"label\"]\n",
        "    del train_df  # Free memory early\n",
        "\n",
        "    train_pool = Pool(data=X_train, label=y_train, group_id=train_group_id)\n",
        "    del X_train, y_train, train_group_id  # Free memory\n",
        "    gc.collect()  # Call garbage collector to be extra sure\n",
        "\n",
        "    # Validation\n",
        "    val_group_sizes = val_df.groupby(\"clean_row_id\", sort=False).size().tolist()\n",
        "    val_group_id = np.repeat(np.arange(len(val_group_sizes)), val_group_sizes)\n",
        "\n",
        "    X_val = val_df.drop(columns=drop_cols)\n",
        "    y_val = val_df[\"label\"]\n",
        "\n",
        "    val_pool = Pool(data=X_val, label=y_val, group_id=val_group_id)\n",
        "    del X_val, y_val, val_group_id  # Free memory\n",
        "    gc.collect()  # Call garbage collector to be extra sure\n",
        "\n",
        "    # Train model\n",
        "    model = CatBoostRanker(iterations=n_rounds, **parameters)\n",
        "    model.fit(\n",
        "        train_pool,\n",
        "        eval_set=val_pool,\n",
        "        early_stopping_rounds=10,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    model.save_model(model_output_path)\n",
        "    print(f\"\\nModel saved to: {model_output_path}\")\n",
        "\n",
        "    # Score model\n",
        "    val_df = val_df.copy()  # preserve original structure\n",
        "    val_df[\"score\"] = model.predict(val_pool)\n",
        "\n",
        "    acc1, recall3, mrr = _compute_ranking_metrics(val_df, k=3)\n",
        "\n",
        "    print(\"\\nEvaluation Metrics (Validation Set):\")\n",
        "    print(f\"Accuracy@1 : {acc1:.4f}\")\n",
        "    print(f\"Recall@3   : {recall3:.4f}\")\n",
        "    print(f\"MRR        : {mrr:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_standard_and_complex_model(\n",
        "    n_rounds: int = 1000,\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains two CatBoost ranking models (standard and complex) on pre-split data and saves them to\n",
        "    disk.\n",
        "\n",
        "    The models are saved in `.cbm` format for compatibility with CatBoost's C++ inference engine.\n",
        "\n",
        "    Args:\n",
        "        n_rounds (int): Maximum number of boosting rounds for training (default: 1000).\n",
        "    \"\"\"\n",
        "    # Mount drive\n",
        "    import sys\n",
        "\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import drive\n",
        "\n",
        "        drive.mount(\"/content/drive\")\n",
        "\n",
        "    def train_model(\n",
        "        data_table: str, val_ids_path: str, test_ids_path: str, model_path: str\n",
        "    ):\n",
        "        print(f\"Training model: {model_path}\")\n",
        "        # Get ids\n",
        "        val_ids = (\n",
        "            pd.read_csv(val_ids_path)[\"val_ids\"].dropna().astype(int).tolist()\n",
        "        )\n",
        "        test_ids = (\n",
        "            pd.read_csv(test_ids_path)[\"test_ids\"].dropna().astype(int).tolist()\n",
        "        )\n",
        "\n",
        "        # Get data\n",
        "        chunk_size = 100000\n",
        "        chunks = []\n",
        "        with sqlite3.connect(_DB_PATH) as conn:\n",
        "            for chunk in pd.read_sql_query(f\"SELECT * FROM {data_table}\", conn, chunksize=chunk_size):\n",
        "                chunks.append(chunk)\n",
        "            full_df = pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "        # Split the set\n",
        "        val_ids_set = set(val_ids)\n",
        "        test_ids_set = set(test_ids)\n",
        "        excluded_ids = val_ids_set | test_ids_set\n",
        "        val_df = full_df[full_df[\"clean_row_id\"].isin(val_ids_set)]\n",
        "        full_df = full_df[~full_df[\"clean_row_id\"].isin(excluded_ids)]\n",
        "\n",
        "        # Train model\n",
        "        return _train_catboost_model(\n",
        "            _BEST_PARAMS, full_df, val_df, n_rounds, model_path\n",
        "        )\n",
        "\n",
        "    # Start with standard\n",
        "    train_model(\"feature_matrix\", _STD_VAL_IDS, _STD_TEST_IDS, \"std_lightgbm_model.cbm\")\n",
        "    # Then complex\n",
        "    train_model(\"feature_matrix_complex\", _COMP_VAL_IDS, _COMP_TEST_IDS, \"comp_lightgbm_model.cbm\")\n",
        "\n",
        "    # return std_model, comp_model\n",
        "    return None\n",
        "\n",
        "\n",
        "train_standard_and_complex_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETPXIgXdjtD0",
        "outputId": "b91d874f-3898-44f9-ba47-c869211fcbfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Training model: std_lightgbm_model.cbm\n",
            "0:\ttest: 0.9710703\tbest: 0.9710703 (0)\ttotal: 45.3s\tremaining: 12h 34m 25s\n",
            "1:\ttest: 0.9728100\tbest: 0.9728100 (1)\ttotal: 1m 34s\tremaining: 13h 4m 11s\n",
            "2:\ttest: 0.9733131\tbest: 0.9733131 (2)\ttotal: 2m 22s\tremaining: 13h 11m 2s\n",
            "3:\ttest: 0.9737284\tbest: 0.9737284 (3)\ttotal: 3m 9s\tremaining: 13h 5m 39s\n",
            "4:\ttest: 0.9737646\tbest: 0.9737646 (4)\ttotal: 3m 58s\tremaining: 13h 11m 26s\n",
            "5:\ttest: 0.9741095\tbest: 0.9741095 (5)\ttotal: 4m 48s\tremaining: 13h 16m 7s\n",
            "6:\ttest: 0.9744848\tbest: 0.9744848 (6)\ttotal: 5m 38s\tremaining: 13h 20m 49s\n",
            "7:\ttest: 0.9746007\tbest: 0.9746007 (7)\ttotal: 6m 28s\tremaining: 13h 23m 12s\n",
            "8:\ttest: 0.9744615\tbest: 0.9746007 (7)\ttotal: 7m 13s\tremaining: 13h 15m 36s\n",
            "9:\ttest: 0.9744831\tbest: 0.9746007 (7)\ttotal: 8m 8s\tremaining: 13h 26m 8s\n",
            "10:\ttest: 0.9744863\tbest: 0.9746007 (7)\ttotal: 9m\tremaining: 13h 30m 12s\n",
            "11:\ttest: 0.9744894\tbest: 0.9746007 (7)\ttotal: 9m 48s\tremaining: 13h 27m 32s\n",
            "12:\ttest: 0.9745110\tbest: 0.9746007 (7)\ttotal: 10m 36s\tremaining: 13h 25m 22s\n",
            "13:\ttest: 0.9746145\tbest: 0.9746145 (13)\ttotal: 11m 30s\tremaining: 13h 30m 1s\n",
            "14:\ttest: 0.9745904\tbest: 0.9746145 (13)\ttotal: 12m 18s\tremaining: 13h 27m 52s\n",
            "15:\ttest: 0.9745967\tbest: 0.9746145 (13)\ttotal: 13m 7s\tremaining: 13h 27m 1s\n",
            "16:\ttest: 0.9745967\tbest: 0.9746145 (13)\ttotal: 14m 2s\tremaining: 13h 32m\n",
            "17:\ttest: 0.9745846\tbest: 0.9746145 (13)\ttotal: 14m 49s\tremaining: 13h 28m 39s\n",
            "18:\ttest: 0.9746902\tbest: 0.9746902 (18)\ttotal: 15m 37s\tremaining: 13h 26m 38s\n",
            "19:\ttest: 0.9746813\tbest: 0.9746902 (18)\ttotal: 16m 28s\tremaining: 13h 27m 39s\n",
            "20:\ttest: 0.9746723\tbest: 0.9746902 (18)\ttotal: 17m 22s\tremaining: 13h 30m 20s\n",
            "21:\ttest: 0.9747359\tbest: 0.9747359 (21)\ttotal: 18m 9s\tremaining: 13h 27m 31s\n",
            "22:\ttest: 0.9747569\tbest: 0.9747569 (22)\ttotal: 18m 59s\tremaining: 13h 26m 40s\n",
            "23:\ttest: 0.9747601\tbest: 0.9747601 (23)\ttotal: 19m 44s\tremaining: 13h 22m 48s\n",
            "24:\ttest: 0.9747822\tbest: 0.9747822 (24)\ttotal: 20m 29s\tremaining: 13h 19m 6s\n",
            "25:\ttest: 0.9748216\tbest: 0.9748216 (25)\ttotal: 21m 14s\tremaining: 13h 15m 53s\n",
            "26:\ttest: 0.9748305\tbest: 0.9748305 (26)\ttotal: 22m 3s\tremaining: 13h 15m 10s\n",
            "27:\ttest: 0.9748426\tbest: 0.9748426 (27)\ttotal: 22m 46s\tremaining: 13h 10m 29s\n",
            "28:\ttest: 0.9748662\tbest: 0.9748662 (28)\ttotal: 23m 28s\tremaining: 13h 5m 58s\n",
            "29:\ttest: 0.9748662\tbest: 0.9748662 (28)\ttotal: 24m 16s\tremaining: 13h 4m 58s\n",
            "30:\ttest: 0.9748993\tbest: 0.9748993 (30)\ttotal: 25m 7s\tremaining: 13h 5m 18s\n",
            "31:\ttest: 0.9749165\tbest: 0.9749165 (31)\ttotal: 25m 49s\tremaining: 13h 1m 12s\n",
            "32:\ttest: 0.9749349\tbest: 0.9749349 (32)\ttotal: 26m 31s\tremaining: 12h 57m 30s\n",
            "33:\ttest: 0.9749344\tbest: 0.9749349 (32)\ttotal: 27m 22s\tremaining: 12h 57m 39s\n",
            "34:\ttest: 0.9749910\tbest: 0.9749910 (34)\ttotal: 28m 16s\tremaining: 12h 59m 25s\n",
            "35:\ttest: 0.9749821\tbest: 0.9749910 (34)\ttotal: 29m 10s\tremaining: 13h 1m 27s\n",
            "36:\ttest: 0.9749974\tbest: 0.9749974 (36)\ttotal: 29m 55s\tremaining: 12h 58m 53s\n",
            "37:\ttest: 0.9750158\tbest: 0.9750158 (37)\ttotal: 30m 40s\tremaining: 12h 56m 29s\n",
            "38:\ttest: 0.9750074\tbest: 0.9750158 (37)\ttotal: 31m 26s\tremaining: 12h 54m 49s\n",
            "39:\ttest: 0.9749922\tbest: 0.9750158 (37)\ttotal: 32m 20s\tremaining: 12h 56m 1s\n",
            "40:\ttest: 0.9749833\tbest: 0.9750158 (37)\ttotal: 33m 10s\tremaining: 12h 56m 4s\n",
            "41:\ttest: 0.9749959\tbest: 0.9750158 (37)\ttotal: 33m 55s\tremaining: 12h 53m 41s\n",
            "42:\ttest: 0.9749997\tbest: 0.9750158 (37)\ttotal: 34m 36s\tremaining: 12h 50m 24s\n",
            "43:\ttest: 0.9749965\tbest: 0.9750158 (37)\ttotal: 35m 24s\tremaining: 12h 49m 10s\n",
            "44:\ttest: 0.9750074\tbest: 0.9750158 (37)\ttotal: 36m 14s\tremaining: 12h 49m\n",
            "45:\ttest: 0.9749922\tbest: 0.9750158 (37)\ttotal: 36m 56s\tremaining: 12h 46m 3s\n",
            "46:\ttest: 0.9749922\tbest: 0.9750158 (37)\ttotal: 37m 40s\tremaining: 12h 44m\n",
            "47:\ttest: 0.9749890\tbest: 0.9750158 (37)\ttotal: 38m 22s\tremaining: 12h 41m 3s\n",
            "Stopped by overfitting detector  (10 iterations wait)\n",
            "\n",
            "bestTest = 0.9750157649\n",
            "bestIteration = 37\n",
            "\n",
            "Shrink model to first 38 iterations.\n",
            "\n",
            "Model saved to: std_lightgbm_model.cbm\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1226705638.py:47: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  topk = df.groupby(\"clean_row_id\", group_keys=False).apply(\n",
            "/tmp/ipython-input-1226705638.py:60: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  mrr = df.groupby(\"clean_row_id\", group_keys=False).apply(reciprocal_rank).mean()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics (Validation Set):\n",
            "Accuracy@1 : 0.9374\n",
            "Recall@3   : 0.9984\n",
            "MRR        : 0.9672\n",
            "Training model: comp_lightgbm_model.cbm\n",
            "0:\ttest: 0.7576870\tbest: 0.7576870 (0)\ttotal: 2m 23s\tremaining: 1d 15h 42m 1s\n",
            "1:\ttest: 0.8041258\tbest: 0.8041258 (1)\ttotal: 5m 4s\tremaining: 1d 18h 10m 48s\n",
            "2:\ttest: 0.8182105\tbest: 0.8182105 (2)\ttotal: 7m 31s\tremaining: 1d 17h 39m 17s\n",
            "3:\ttest: 0.8354011\tbest: 0.8354011 (3)\ttotal: 10m 4s\tremaining: 1d 17h 46m 48s\n",
            "4:\ttest: 0.8374243\tbest: 0.8374243 (4)\ttotal: 12m 32s\tremaining: 1d 17h 35m 24s\n",
            "5:\ttest: 0.8604993\tbest: 0.8604993 (5)\ttotal: 14m 55s\tremaining: 1d 17h 11m 57s\n",
            "6:\ttest: 0.8618760\tbest: 0.8618760 (6)\ttotal: 17m 23s\tremaining: 1d 17h 7m 48s\n",
            "7:\ttest: 0.8674218\tbest: 0.8674218 (7)\ttotal: 19m 47s\tremaining: 1d 16h 55m 3s\n",
            "8:\ttest: 0.8690335\tbest: 0.8690335 (8)\ttotal: 22m 18s\tremaining: 1d 16h 57m 8s\n",
            "9:\ttest: 0.8745844\tbest: 0.8745844 (9)\ttotal: 24m 54s\tremaining: 1d 17h 5m 27s\n",
            "10:\ttest: 0.8804390\tbest: 0.8804390 (10)\ttotal: 27m 16s\tremaining: 1d 16h 51m 39s\n",
            "11:\ttest: 0.8841329\tbest: 0.8841329 (11)\ttotal: 29m 37s\tremaining: 1d 16h 39m 35s\n",
            "12:\ttest: 0.8850735\tbest: 0.8850735 (12)\ttotal: 32m 13s\tremaining: 1d 16h 47m 13s\n",
            "13:\ttest: 0.8857790\tbest: 0.8857790 (13)\ttotal: 34m 42s\tremaining: 1d 16h 44m 16s\n",
            "14:\ttest: 0.8915747\tbest: 0.8915747 (14)\ttotal: 37m 5s\tremaining: 1d 16h 35m 29s\n",
            "15:\ttest: 0.8937354\tbest: 0.8937354 (15)\ttotal: 39m 29s\tremaining: 1d 16h 28m 19s\n",
            "16:\ttest: 0.8941813\tbest: 0.8941813 (16)\ttotal: 41m 51s\tremaining: 1d 16h 20m 47s\n",
            "17:\ttest: 0.8965231\tbest: 0.8965231 (17)\ttotal: 44m 7s\tremaining: 1d 16h 7m 34s\n",
            "18:\ttest: 0.8982231\tbest: 0.8982231 (18)\ttotal: 46m 29s\tremaining: 1d 16h 36s\n",
            "19:\ttest: 0.9007805\tbest: 0.9007805 (19)\ttotal: 48m 52s\tremaining: 1d 15h 55m 14s\n",
            "20:\ttest: 0.9028822\tbest: 0.9028822 (20)\ttotal: 51m 14s\tremaining: 1d 15h 49m 9s\n",
            "21:\ttest: 0.9065223\tbest: 0.9065223 (21)\ttotal: 53m 30s\tremaining: 1d 15h 38m 50s\n",
            "22:\ttest: 0.9062724\tbest: 0.9065223 (21)\ttotal: 56m 4s\tremaining: 1d 15h 41m 46s\n",
            "23:\ttest: 0.9060225\tbest: 0.9065223 (21)\ttotal: 58m 25s\tremaining: 1d 15h 35m 41s\n",
            "24:\ttest: 0.9070954\tbest: 0.9070954 (24)\ttotal: 1h 53s\tremaining: 1d 15h 34m 32s\n",
            "25:\ttest: 0.9070954\tbest: 0.9070954 (24)\ttotal: 1h 3m 16s\tremaining: 1d 15h 30m 10s\n",
            "26:\ttest: 0.9092413\tbest: 0.9092413 (26)\ttotal: 1h 5m 36s\tremaining: 1d 15h 24m 23s\n",
            "27:\ttest: 0.9118428\tbest: 0.9118428 (27)\ttotal: 1h 7m 51s\tremaining: 1d 15h 15m 31s\n",
            "28:\ttest: 0.9118525\tbest: 0.9118525 (28)\ttotal: 1h 10m 7s\tremaining: 1d 15h 7m 51s\n",
            "29:\ttest: 0.9134889\tbest: 0.9134889 (29)\ttotal: 1h 12m 28s\tremaining: 1d 15h 3m 24s\n",
            "30:\ttest: 0.9150077\tbest: 0.9150077 (30)\ttotal: 1h 14m 59s\tremaining: 1d 15h 4m\n",
            "31:\ttest: 0.9154094\tbest: 0.9154094 (31)\ttotal: 1h 17m 21s\tremaining: 1d 15h 11s\n",
            "32:\ttest: 0.9153653\tbest: 0.9154094 (31)\ttotal: 1h 19m 43s\tremaining: 1d 14h 56m 2s\n",
            "33:\ttest: 0.9151498\tbest: 0.9154094 (31)\ttotal: 1h 21m 59s\tremaining: 1d 14h 49m 36s\n",
            "34:\ttest: 0.9156054\tbest: 0.9156054 (34)\ttotal: 1h 24m 21s\tremaining: 1d 14h 46m 5s\n",
            "35:\ttest: 0.9159631\tbest: 0.9159631 (35)\ttotal: 1h 26m 44s\tremaining: 1d 14h 42m 42s\n",
            "36:\ttest: 0.9160611\tbest: 0.9160611 (36)\ttotal: 1h 29m 18s\tremaining: 1d 14h 44m 31s\n",
            "37:\ttest: 0.9167323\tbest: 0.9167323 (37)\ttotal: 1h 31m 34s\tremaining: 1d 14h 38m 10s\n",
            "38:\ttest: 0.9177611\tbest: 0.9177611 (38)\ttotal: 1h 33m 55s\tremaining: 1d 14h 34m 34s\n",
            "39:\ttest: 0.9179129\tbest: 0.9179129 (39)\ttotal: 1h 36m 36s\tremaining: 1d 14h 38m 37s\n",
            "40:\ttest: 0.9183245\tbest: 0.9183245 (40)\ttotal: 1h 39m 15s\tremaining: 1d 14h 41m 46s\n",
            "41:\ttest: 0.9184225\tbest: 0.9184225 (41)\ttotal: 1h 41m 48s\tremaining: 1d 14h 42m 18s\n",
            "42:\ttest: 0.9182265\tbest: 0.9184225 (41)\ttotal: 1h 44m 23s\tremaining: 1d 14h 43m 8s\n",
            "43:\ttest: 0.9183783\tbest: 0.9184225 (41)\ttotal: 1h 47m 3s\tremaining: 1d 14h 46m 2s\n",
            "44:\ttest: 0.9186821\tbest: 0.9186821 (44)\ttotal: 1h 49m 25s\tremaining: 1d 14h 42m 15s\n",
            "45:\ttest: 0.9189859\tbest: 0.9189859 (45)\ttotal: 1h 52m 4s\tremaining: 1d 14h 44m 15s\n",
            "46:\ttest: 0.9194415\tbest: 0.9194415 (46)\ttotal: 1h 54m 38s\tremaining: 1d 14h 44m 41s\n",
            "47:\ttest: 0.9201470\tbest: 0.9201470 (47)\ttotal: 1h 57m 17s\tremaining: 1d 14h 46m 20s\n",
            "48:\ttest: 0.9198972\tbest: 0.9201470 (47)\ttotal: 1h 59m 39s\tremaining: 1d 14h 42m 22s\n",
            "49:\ttest: 0.9199952\tbest: 0.9201470 (47)\ttotal: 2h 2m 12s\tremaining: 1d 14h 41m 54s\n",
            "50:\ttest: 0.9208182\tbest: 0.9208182 (50)\ttotal: 2h 4m 27s\tremaining: 1d 14h 35m 55s\n",
            "51:\ttest: 0.9200588\tbest: 0.9208182 (50)\ttotal: 2h 7m 13s\tremaining: 1d 14h 39m 20s\n",
            "52:\ttest: 0.9202107\tbest: 0.9208182 (50)\ttotal: 2h 9m 35s\tremaining: 1d 14h 35m 32s\n",
            "53:\ttest: 0.9204164\tbest: 0.9208182 (50)\ttotal: 2h 12m 20s\tremaining: 1d 14h 38m 27s\n",
            "54:\ttest: 0.9204703\tbest: 0.9208182 (50)\ttotal: 2h 15m 6s\tremaining: 1d 14h 41m 27s\n",
            "55:\ttest: 0.9217490\tbest: 0.9217490 (55)\ttotal: 2h 17m 27s\tremaining: 1d 14h 37m 5s\n",
            "56:\ttest: 0.9218568\tbest: 0.9218568 (56)\ttotal: 2h 19m 55s\tremaining: 1d 14h 34m 53s\n",
            "57:\ttest: 0.9219106\tbest: 0.9219106 (57)\ttotal: 2h 22m 23s\tremaining: 1d 14h 32m 45s\n",
            "58:\ttest: 0.9218126\tbest: 0.9219106 (57)\ttotal: 2h 24m 52s\tremaining: 1d 14h 30m 31s\n",
            "59:\ttest: 0.9214550\tbest: 0.9219106 (57)\ttotal: 2h 27m 14s\tremaining: 1d 14h 26m 43s\n",
            "60:\ttest: 0.9213031\tbest: 0.9219106 (57)\ttotal: 2h 29m 40s\tremaining: 1d 14h 23m 57s\n",
            "61:\ttest: 0.9213031\tbest: 0.9219106 (57)\ttotal: 2h 32m 7s\tremaining: 1d 14h 21m 36s\n",
            "62:\ttest: 0.9218126\tbest: 0.9219106 (57)\ttotal: 2h 34m 29s\tremaining: 1d 14h 17m 47s\n",
            "63:\ttest: 0.9219106\tbest: 0.9219106 (57)\ttotal: 2h 36m 57s\tremaining: 1d 14h 15m 31s\n",
            "64:\ttest: 0.9225279\tbest: 0.9225279 (64)\ttotal: 2h 39m 23s\tremaining: 1d 14h 12m 51s\n",
            "65:\ttest: 0.9226259\tbest: 0.9226259 (65)\ttotal: 2h 41m 52s\tremaining: 1d 14h 10m 48s\n",
            "66:\ttest: 0.9226259\tbest: 0.9226259 (65)\ttotal: 2h 44m 38s\tremaining: 1d 14h 12m 42s\n",
            "67:\ttest: 0.9228856\tbest: 0.9228856 (67)\ttotal: 2h 47m 9s\tremaining: 1d 14h 11m 4s\n",
            "68:\ttest: 0.9234049\tbest: 0.9234049 (68)\ttotal: 2h 49m 43s\tremaining: 1d 14h 9m 57s\n",
            "69:\ttest: 0.9236645\tbest: 0.9236645 (69)\ttotal: 2h 52m 21s\tremaining: 1d 14h 9m 58s\n",
            "70:\ttest: 0.9233069\tbest: 0.9236645 (69)\ttotal: 2h 54m 54s\tremaining: 1d 14h 8m 40s\n",
            "71:\ttest: 0.9234587\tbest: 0.9236645 (69)\ttotal: 2h 57m 23s\tremaining: 1d 14h 6m 21s\n",
            "72:\ttest: 0.9238164\tbest: 0.9238164 (72)\ttotal: 2h 59m 50s\tremaining: 1d 14h 3m 41s\n",
            "73:\ttest: 0.9237625\tbest: 0.9238164 (72)\ttotal: 3h 2m 24s\tremaining: 1d 14h 2m 27s\n",
            "74:\ttest: 0.9235567\tbest: 0.9238164 (72)\ttotal: 3h 4m 51s\tremaining: 1d 13h 59m 58s\n",
            "75:\ttest: 0.9237086\tbest: 0.9238164 (72)\ttotal: 3h 7m 20s\tremaining: 1d 13h 57m 37s\n",
            "76:\ttest: 0.9239144\tbest: 0.9239144 (76)\ttotal: 3h 9m 47s\tremaining: 1d 13h 55m 4s\n",
            "77:\ttest: 0.9239144\tbest: 0.9239144 (76)\ttotal: 3h 12m 21s\tremaining: 1d 13h 53m 50s\n",
            "78:\ttest: 0.9250314\tbest: 0.9250314 (78)\ttotal: 3h 15m 6s\tremaining: 1d 13h 54m 37s\n",
            "79:\ttest: 0.9246738\tbest: 0.9250314 (78)\ttotal: 3h 17m 27s\tremaining: 1d 13h 50m 47s\n",
            "80:\ttest: 0.9249775\tbest: 0.9250314 (78)\ttotal: 3h 19m 49s\tremaining: 1d 13h 47m 7s\n",
            "81:\ttest: 0.9253352\tbest: 0.9253352 (81)\ttotal: 3h 22m 23s\tremaining: 1d 13h 45m 52s\n",
            "82:\ttest: 0.9254332\tbest: 0.9254332 (82)\ttotal: 3h 24m 44s\tremaining: 1d 13h 42m 4s\n",
            "83:\ttest: 0.9254871\tbest: 0.9254871 (83)\ttotal: 3h 27m 12s\tremaining: 1d 13h 39m 29s\n",
            "84:\ttest: 0.9257908\tbest: 0.9257908 (84)\ttotal: 3h 29m 40s\tremaining: 1d 13h 37m 8s\n",
            "85:\ttest: 0.9257908\tbest: 0.9257908 (84)\ttotal: 3h 32m 3s\tremaining: 1d 13h 33m 46s\n",
            "86:\ttest: 0.9254871\tbest: 0.9257908 (84)\ttotal: 3h 34m 27s\tremaining: 1d 13h 30m 31s\n",
            "87:\ttest: 0.9253352\tbest: 0.9257908 (84)\ttotal: 3h 37m 9s\tremaining: 1d 13h 30m 29s\n",
            "88:\ttest: 0.9252813\tbest: 0.9257908 (84)\ttotal: 3h 39m 31s\tremaining: 1d 13h 27m 2s\n",
            "89:\ttest: 0.9254871\tbest: 0.9257908 (84)\ttotal: 3h 42m 7s\tremaining: 1d 13h 25m 59s\n",
            "90:\ttest: 0.9256389\tbest: 0.9257908 (84)\ttotal: 3h 44m 35s\tremaining: 1d 13h 23m 25s\n",
            "91:\ttest: 0.9260505\tbest: 0.9260505 (91)\ttotal: 3h 47m 19s\tremaining: 1d 13h 23m 40s\n",
            "92:\ttest: 0.9262562\tbest: 0.9262562 (92)\ttotal: 3h 49m 46s\tremaining: 1d 13h 20m 59s\n",
            "93:\ttest: 0.9263101\tbest: 0.9263101 (93)\ttotal: 3h 52m 23s\tremaining: 1d 13h 19m 49s\n",
            "94:\ttest: 0.9262562\tbest: 0.9263101 (93)\ttotal: 3h 54m 57s\tremaining: 1d 13h 18m 12s\n",
            "95:\ttest: 0.9260505\tbest: 0.9263101 (93)\ttotal: 3h 57m 19s\tremaining: 1d 13h 14m 48s\n",
            "96:\ttest: 0.9260505\tbest: 0.9263101 (93)\ttotal: 3h 59m 54s\tremaining: 1d 13h 13m 19s\n",
            "97:\ttest: 0.9257908\tbest: 0.9263101 (93)\ttotal: 4h 2m 35s\tremaining: 1d 13h 12m 52s\n",
            "98:\ttest: 0.9253891\tbest: 0.9263101 (93)\ttotal: 4h 4m 58s\tremaining: 1d 13h 9m 29s\n",
            "99:\ttest: 0.9256928\tbest: 0.9263101 (93)\ttotal: 4h 7m 42s\tremaining: 1d 13h 9m 20s\n",
            "100:\ttest: 0.9256928\tbest: 0.9263101 (93)\ttotal: 4h 10m 22s\tremaining: 1d 13h 8m 37s\n",
            "101:\ttest: 0.9253352\tbest: 0.9263101 (93)\ttotal: 4h 12m 56s\tremaining: 1d 13h 6m 49s\n",
            "102:\ttest: 0.9254871\tbest: 0.9263101 (93)\ttotal: 4h 15m 41s\tremaining: 1d 13h 6m 41s\n",
            "103:\ttest: 0.9261044\tbest: 0.9263101 (93)\ttotal: 4h 18m 17s\tremaining: 1d 13h 5m 16s\n",
            "Stopped by overfitting detector  (10 iterations wait)\n",
            "\n",
            "bestTest = 0.9263101131\n",
            "bestIteration = 93\n",
            "\n",
            "Shrink model to first 94 iterations.\n",
            "\n",
            "Model saved to: comp_lightgbm_model.cbm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1226705638.py:47: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  topk = df.groupby(\"clean_row_id\", group_keys=False).apply(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics (Validation Set):\n",
            "Accuracy@1 : 0.8407\n",
            "Recall@3   : 0.9860\n",
            "MRR        : 0.9119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1226705638.py:60: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  mrr = df.groupby(\"clean_row_id\", group_keys=False).apply(reciprocal_rank).mean()\n"
          ]
        }
      ]
    }
  ]
}