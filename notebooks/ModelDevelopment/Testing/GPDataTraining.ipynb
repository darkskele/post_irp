{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Training - GP data added\n",
        "\n",
        "Final training on full padded data. Catboost added. GP Data added in as well. Full template and token coverage, alot of candidates to rank here."
      ],
      "metadata": {
        "id": "DjE0M1VsiF7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm\n",
        "!pip install sqlalchemy\n",
        "!pip install catboost\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "yNrhVNxUiAcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bf9548c-e8fe-494e-e92f-c84422ec02d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightgbm\n",
            "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.16.1)\n",
            "Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightgbm\n",
            "Successfully installed lightgbm-4.6.0\n",
            "Collecting sqlalchemy\n",
            "  Downloading sqlalchemy-2.0.43-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting greenlet>=1 (from sqlalchemy)\n",
            "  Downloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (4.14.1)\n",
            "Downloading sqlalchemy-2.0.43-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: greenlet, sqlalchemy\n",
            "Successfully installed greenlet-3.2.4 sqlalchemy-2.0.43\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting graphviz (from catboost)\n",
            "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: graphviz, catboost\n",
            "Successfully installed catboost-1.2.8 graphviz-0.21\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cat boost takes it again, with a larger margin this time. Think this is the way to go. Next thing to do is to finalise the whole pipeline and do one final train."
      ],
      "metadata": {
        "id": "SMBxua118hi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRanker, Pool\n",
        "from typing import Tuple\n",
        "\n",
        "# Paths to files in Drive\n",
        "_DB_PATH = \"/content/drive/MyDrive/Colab Notebooks/database.db\"\n",
        "_STD_VAL_IDS = \"val_std_ids.csv\"\n",
        "_STD_TEST_IDS = \"test_std_ids.csv\"\n",
        "_COMP_VAL_IDS = \"val_comp_ids.csv\"\n",
        "_COMP_TEST_IDS = \"test_comp_ids.csv\"\n",
        "\n",
        "# Tuned hyperparameters\n",
        "_BEST_PARAMS = {\n",
        "    \"loss_function\": \"YetiRank\",\n",
        "    \"eval_metric\": \"NDCG:top=3\",\n",
        "    \"random_seed\": 42,\n",
        "    \"learning_rate\": 0.13275757957731918,\n",
        "    \"depth\": 6,\n",
        "    \"l2_leaf_reg\": 7.142519331365267,\n",
        "    \"random_strength\": 3.395785387976391,\n",
        "    \"min_data_in_leaf\": 84,\n",
        "    \"subsample\": 0.9048958560910838,\n",
        "    \"colsample_bylevel\": 0.511123337191838,\n",
        "    \"grow_policy\": \"Lossguide\",\n",
        "}\n",
        "\n",
        "\n",
        "def _compute_ranking_metrics(df: pd.DataFrame, k: int = 3):\n",
        "    \"\"\"\n",
        "    Compute Accuracy@1, Recall@k, and MRR for a ranking prediction dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Must contain columns ['clean_row_id', 'score', 'label']\n",
        "        k (int): The cutoff rank for recall@k\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float, float]: (Accuracy@1, Recall@k, MRR)\n",
        "    \"\"\"\n",
        "    # Accuracy@1\n",
        "    top1 = df.loc[df.groupby(\"clean_row_id\")[\"score\"].idxmax()]\n",
        "    acc1 = (top1[\"label\"] == 1).mean()\n",
        "\n",
        "    # Recall@k\n",
        "    topk = df.groupby(\"clean_row_id\", group_keys=False).apply(\n",
        "        lambda g: g.nlargest(k, \"score\")\n",
        "    )\n",
        "    recall_k = topk.groupby(\"clean_row_id\")[\"label\"].max().mean()\n",
        "\n",
        "    # MRR\n",
        "    def reciprocal_rank(g: pd.DataFrame) -> float:\n",
        "        labels_sorted = g.sort_values(\"score\", ascending=False)[\"label\"].to_numpy()\n",
        "        for rank, label in enumerate(labels_sorted, start=1):\n",
        "            if label == 1:\n",
        "                return 1.0 / rank\n",
        "        return 0.0\n",
        "\n",
        "    mrr = df.groupby(\"clean_row_id\", group_keys=False).apply(reciprocal_rank).mean()\n",
        "\n",
        "    return acc1, recall_k, mrr\n",
        "\n",
        "\n",
        "def _train_catboost_model(\n",
        "    parameters: dict,\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    n_rounds: int = 500,\n",
        "    model_output_path: str = \"catboost_model.cbm\",\n",
        ") -> CatBoostRanker:\n",
        "    \"\"\"\n",
        "    Trains a CatBoost ranking model using the provided training and validation data.\n",
        "\n",
        "    Args:\n",
        "        parameters (dict): Parameters for CatBoostRanker.\n",
        "        train_df (pd.DataFrame): Training data with label, group info, and features.\n",
        "        val_df (pd.DataFrame): Validation data with same structure.\n",
        "        n_rounds (int): Maximum number of boosting rounds.\n",
        "        model_output_path (str): File path to save the trained CatBoost model.\n",
        "\n",
        "    Returns:\n",
        "        CatBoostRanker: Trained CatBoost model.\n",
        "    \"\"\"\n",
        "    drop_cols = [\"label\", \"clean_row_id\", \"investor\", \"firm\", \"template_id\"]\n",
        "\n",
        "    # Train\n",
        "    train_group_sizes = train_df.groupby(\"clean_row_id\", sort=False).size().tolist()\n",
        "    train_group_id = np.repeat(np.arange(len(train_group_sizes)), train_group_sizes)\n",
        "\n",
        "    X_train = train_df.drop(columns=drop_cols)\n",
        "    y_train = train_df[\"label\"]\n",
        "    del train_df  # Free memory early\n",
        "\n",
        "    train_pool = Pool(data=X_train, label=y_train, group_id=train_group_id)\n",
        "    del X_train, y_train, train_group_id  # Free memory\n",
        "    gc.collect()  # Call garbage collector to be extra sure\n",
        "\n",
        "    # Validation\n",
        "    val_group_sizes = val_df.groupby(\"clean_row_id\", sort=False).size().tolist()\n",
        "    val_group_id = np.repeat(np.arange(len(val_group_sizes)), val_group_sizes)\n",
        "\n",
        "    X_val = val_df.drop(columns=drop_cols)\n",
        "    y_val = val_df[\"label\"]\n",
        "\n",
        "    val_pool = Pool(data=X_val, label=y_val, group_id=val_group_id)\n",
        "    del X_val, y_val, val_group_id  # Free memory\n",
        "    gc.collect()  # Call garbage collector to be extra sure\n",
        "\n",
        "    # Train model\n",
        "    model = CatBoostRanker(iterations=n_rounds, **parameters)\n",
        "    model.fit(\n",
        "        train_pool,\n",
        "        eval_set=val_pool,\n",
        "        early_stopping_rounds=10,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    model.save_model(model_output_path)\n",
        "    print(f\"\\nModel saved to: {model_output_path}\")\n",
        "\n",
        "    # Score model\n",
        "    val_df = val_df.copy()  # preserve original structure\n",
        "    val_df[\"score\"] = model.predict(val_pool)\n",
        "\n",
        "    acc1, recall3, mrr = _compute_ranking_metrics(val_df, k=3)\n",
        "\n",
        "    print(\"\\nEvaluation Metrics (Validation Set):\")\n",
        "    print(f\"Accuracy@1 : {acc1:.4f}\")\n",
        "    print(f\"Recall@3   : {recall3:.4f}\")\n",
        "    print(f\"MRR        : {mrr:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_standard_and_complex_model(\n",
        "    n_rounds: int = 1000,\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains two CatBoost ranking models (standard and complex) on pre-split data and saves them to\n",
        "    disk.\n",
        "\n",
        "    The models are saved in `.cbm` format for compatibility with CatBoost's C++ inference engine.\n",
        "\n",
        "    Args:\n",
        "        n_rounds (int): Maximum number of boosting rounds for training (default: 1000).\n",
        "    \"\"\"\n",
        "    # Mount drive\n",
        "    import sys\n",
        "\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import drive\n",
        "\n",
        "        drive.mount(\"/content/drive\")\n",
        "\n",
        "    def train_model(\n",
        "        data_table: str, val_ids_path: str, test_ids_path: str, model_path: str\n",
        "    ):\n",
        "        print(f\"Training model: {model_path}\")\n",
        "        # Get ids\n",
        "        val_ids = (\n",
        "            pd.read_csv(val_ids_path)[\"val_ids\"].dropna().astype(int).tolist()\n",
        "        )\n",
        "        test_ids = (\n",
        "            pd.read_csv(test_ids_path)[\"test_ids\"].dropna().astype(int).tolist()\n",
        "        )\n",
        "\n",
        "        # Get data\n",
        "        with sqlite3.connect(_DB_PATH) as conn:\n",
        "            full_df = pd.read_sql_query(f\"SELECT * FROM {data_table}\", conn)\n",
        "\n",
        "        # Split the set\n",
        "        val_ids_set = set(val_ids)\n",
        "        test_ids_set = set(test_ids)\n",
        "        excluded_ids = val_ids_set | test_ids_set\n",
        "        val_df = full_df[full_df[\"clean_row_id\"].isin(val_ids_set)]\n",
        "        full_df = full_df[~full_df[\"clean_row_id\"].isin(excluded_ids)]\n",
        "\n",
        "        # Train model\n",
        "        return _train_catboost_model(\n",
        "            _BEST_PARAMS, full_df, val_df, n_rounds, model_path\n",
        "        )\n",
        "\n",
        "    # Start with standard\n",
        "    train_model(\"feature_matrix\", _STD_VAL_IDS, _STD_TEST_IDS, \"std_lightgbm_model.cbm\")\n",
        "    # Then complex\n",
        "    train_model(\"feature_matrix_complex\", _COMP_VAL_IDS, _COMP_TEST_IDS, \"comp_lightgbm_model.cbm\")\n",
        "\n",
        "    # return std_model, comp_model\n",
        "    return None\n",
        "\n",
        "\n",
        "train_standard_and_complex_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETPXIgXdjtD0",
        "outputId": "9da359f7-a5c0-4a49-df94-f3178e92b986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Training model: std_lightgbm_model.cbm\n",
            "0:\ttest: 0.9658214\tbest: 0.9658214 (0)\ttotal: 49.2s\tremaining: 13h 38m 40s\n",
            "1:\ttest: 0.9658214\tbest: 0.9658214 (0)\ttotal: 1m 30s\tremaining: 12h 29m 22s\n",
            "2:\ttest: 0.9714184\tbest: 0.9714184 (2)\ttotal: 2m 16s\tremaining: 12h 33m 41s\n",
            "3:\ttest: 0.9720715\tbest: 0.9720715 (3)\ttotal: 3m 1s\tremaining: 12h 34m 39s\n",
            "4:\ttest: 0.9721750\tbest: 0.9721750 (4)\ttotal: 3m 53s\tremaining: 12h 53m 13s\n",
            "5:\ttest: 0.9724847\tbest: 0.9724847 (5)\ttotal: 4m 45s\tremaining: 13h 9m 38s\n",
            "6:\ttest: 0.9727083\tbest: 0.9727083 (6)\ttotal: 5m 40s\tremaining: 13h 25m 8s\n",
            "7:\ttest: 0.9729202\tbest: 0.9729202 (7)\ttotal: 6m 32s\tremaining: 13h 31m 20s\n",
            "8:\ttest: 0.9729259\tbest: 0.9729259 (8)\ttotal: 7m 23s\tremaining: 13h 33m 48s\n",
            "9:\ttest: 0.9728815\tbest: 0.9729259 (8)\ttotal: 8m 11s\tremaining: 13h 30m 22s\n",
            "10:\ttest: 0.9729222\tbest: 0.9729259 (8)\ttotal: 9m 2s\tremaining: 13h 32m 42s\n",
            "11:\ttest: 0.9729500\tbest: 0.9729500 (11)\ttotal: 9m 53s\tremaining: 13h 34m 30s\n",
            "12:\ttest: 0.9730489\tbest: 0.9730489 (12)\ttotal: 10m 48s\tremaining: 13h 40m 18s\n",
            "13:\ttest: 0.9730762\tbest: 0.9730762 (13)\ttotal: 11m 36s\tremaining: 13h 37m 20s\n",
            "14:\ttest: 0.9730647\tbest: 0.9730762 (13)\ttotal: 12m 29s\tremaining: 13h 40m 35s\n",
            "15:\ttest: 0.9731559\tbest: 0.9731559 (15)\ttotal: 13m 15s\tremaining: 13h 35m\n",
            "16:\ttest: 0.9731203\tbest: 0.9731559 (15)\ttotal: 14m 4s\tremaining: 13h 33m 24s\n",
            "17:\ttest: 0.9732198\tbest: 0.9732198 (17)\ttotal: 14m 57s\tremaining: 13h 36m 9s\n",
            "18:\ttest: 0.9732198\tbest: 0.9732198 (17)\ttotal: 15m 45s\tremaining: 13h 33m 51s\n",
            "19:\ttest: 0.9732078\tbest: 0.9732198 (17)\ttotal: 16m 39s\tremaining: 13h 36m 22s\n",
            "20:\ttest: 0.9732774\tbest: 0.9732774 (20)\ttotal: 17m 30s\tremaining: 13h 36m 21s\n",
            "21:\ttest: 0.9733766\tbest: 0.9733766 (21)\ttotal: 18m 17s\tremaining: 13h 32m 53s\n",
            "22:\ttest: 0.9734311\tbest: 0.9734311 (22)\ttotal: 19m 5s\tremaining: 13h 31m 12s\n",
            "23:\ttest: 0.9734190\tbest: 0.9734311 (22)\ttotal: 19m 48s\tremaining: 13h 25m 44s\n",
            "24:\ttest: 0.9734394\tbest: 0.9734394 (24)\ttotal: 20m 34s\tremaining: 13h 22m 26s\n",
            "25:\ttest: 0.9734211\tbest: 0.9734394 (24)\ttotal: 21m 27s\tremaining: 13h 23m 41s\n",
            "26:\ttest: 0.9734096\tbest: 0.9734394 (24)\ttotal: 22m 16s\tremaining: 13h 22m 26s\n",
            "27:\ttest: 0.9734096\tbest: 0.9734394 (24)\ttotal: 23m 2s\tremaining: 13h 20m 7s\n",
            "28:\ttest: 0.9734362\tbest: 0.9734394 (24)\ttotal: 23m 47s\tremaining: 13h 16m 50s\n",
            "29:\ttest: 0.9734661\tbest: 0.9734661 (29)\ttotal: 24m 36s\tremaining: 13h 15m 50s\n",
            "30:\ttest: 0.9734896\tbest: 0.9734896 (30)\ttotal: 25m 21s\tremaining: 13h 12m 44s\n",
            "31:\ttest: 0.9735105\tbest: 0.9735105 (31)\ttotal: 26m 6s\tremaining: 13h 9m 51s\n",
            "32:\ttest: 0.9734933\tbest: 0.9735105 (31)\ttotal: 26m 51s\tremaining: 13h 7m 1s\n",
            "33:\ttest: 0.9734437\tbest: 0.9735105 (31)\ttotal: 27m 41s\tremaining: 13h 6m 34s\n",
            "34:\ttest: 0.9733846\tbest: 0.9735105 (31)\ttotal: 28m 25s\tremaining: 13h 3m 52s\n",
            "35:\ttest: 0.9733574\tbest: 0.9735105 (31)\ttotal: 29m 11s\tremaining: 13h 1m 49s\n",
            "36:\ttest: 0.9733072\tbest: 0.9735105 (31)\ttotal: 30m\tremaining: 13h 1m 14s\n",
            "37:\ttest: 0.9733250\tbest: 0.9735105 (31)\ttotal: 30m 46s\tremaining: 12h 59m 7s\n",
            "38:\ttest: 0.9733365\tbest: 0.9735105 (31)\ttotal: 31m 38s\tremaining: 12h 59m 41s\n",
            "39:\ttest: 0.9733631\tbest: 0.9735105 (31)\ttotal: 32m 21s\tremaining: 12h 56m 28s\n",
            "40:\ttest: 0.9733866\tbest: 0.9735105 (31)\ttotal: 33m 6s\tremaining: 12h 54m 30s\n",
            "41:\ttest: 0.9733746\tbest: 0.9735105 (31)\ttotal: 33m 56s\tremaining: 12h 54m 15s\n",
            "Stopped by overfitting detector  (10 iterations wait)\n",
            "\n",
            "bestTest = 0.9735105065\n",
            "bestIteration = 31\n",
            "\n",
            "Shrink model to first 32 iterations.\n",
            "\n",
            "Model saved to: std_lightgbm_model.cbm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3337695897.py:47: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  topk = df.groupby(\"clean_row_id\", group_keys=False).apply(\n",
            "/tmp/ipython-input-3337695897.py:60: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  mrr = df.groupby(\"clean_row_id\", group_keys=False).apply(reciprocal_rank).mean()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics (Validation Set):\n",
            "Accuracy@1 : 0.9336\n",
            "Recall@3   : 0.9984\n",
            "MRR        : 0.9652\n",
            "Training model: comp_lightgbm_model.cbm\n",
            "0:\ttest: 0.6158198\tbest: 0.6158198 (0)\ttotal: 1m 27s\tremaining: 1d 18m 39s\n",
            "1:\ttest: 0.6376602\tbest: 0.6376602 (1)\ttotal: 2m 54s\tremaining: 1d 9m 27s\n",
            "2:\ttest: 0.6603577\tbest: 0.6603577 (2)\ttotal: 4m 20s\tremaining: 1d 2m 41s\n",
            "3:\ttest: 0.6780361\tbest: 0.6780361 (3)\ttotal: 5m 49s\tremaining: 1d 8m 32s\n",
            "4:\ttest: 0.6963747\tbest: 0.6963747 (4)\ttotal: 7m 11s\tremaining: 23h 51m 4s\n",
            "5:\ttest: 0.6959076\tbest: 0.6963747 (4)\ttotal: 8m 37s\tremaining: 23h 47m 35s\n",
            "6:\ttest: 0.7388896\tbest: 0.7388896 (6)\ttotal: 10m 2s\tremaining: 23h 43m 42s\n",
            "7:\ttest: 0.7450297\tbest: 0.7450297 (7)\ttotal: 11m 26s\tremaining: 23h 38m 18s\n",
            "8:\ttest: 0.7474377\tbest: 0.7474377 (8)\ttotal: 12m 50s\tremaining: 23h 34m 7s\n",
            "9:\ttest: 0.7575798\tbest: 0.7575798 (9)\ttotal: 14m 12s\tremaining: 23h 26m 59s\n",
            "10:\ttest: 0.7776858\tbest: 0.7776858 (10)\ttotal: 15m 34s\tremaining: 23h 20m 51s\n",
            "11:\ttest: 0.7792509\tbest: 0.7792509 (11)\ttotal: 17m\tremaining: 23h 20m 34s\n",
            "12:\ttest: 0.7793857\tbest: 0.7793857 (12)\ttotal: 18m 25s\tremaining: 23h 18m 53s\n",
            "13:\ttest: 0.7859257\tbest: 0.7859257 (13)\ttotal: 19m 50s\tremaining: 23h 17m 38s\n",
            "14:\ttest: 0.7946715\tbest: 0.7946715 (14)\ttotal: 21m 13s\tremaining: 23h 13m 14s\n",
            "15:\ttest: 0.7923068\tbest: 0.7946715 (14)\ttotal: 22m 33s\tremaining: 23h 7m 7s\n",
            "16:\ttest: 0.7973877\tbest: 0.7973877 (16)\ttotal: 23m 57s\tremaining: 23h 4m 53s\n",
            "17:\ttest: 0.8110454\tbest: 0.8110454 (17)\ttotal: 25m 22s\tremaining: 23h 4m 6s\n",
            "18:\ttest: 0.8110646\tbest: 0.8110646 (18)\ttotal: 26m 44s\tremaining: 23h 38s\n",
            "19:\ttest: 0.8210577\tbest: 0.8210577 (19)\ttotal: 28m 8s\tremaining: 22h 58m 43s\n",
            "20:\ttest: 0.8288785\tbest: 0.8288785 (20)\ttotal: 29m 30s\tremaining: 22h 55m 30s\n",
            "21:\ttest: 0.8311902\tbest: 0.8311902 (21)\ttotal: 30m 51s\tremaining: 22h 51m 34s\n",
            "22:\ttest: 0.8341712\tbest: 0.8341712 (22)\ttotal: 32m 10s\tremaining: 22h 46m 36s\n",
            "23:\ttest: 0.8394592\tbest: 0.8394592 (23)\ttotal: 33m 31s\tremaining: 22h 43m 39s\n",
            "24:\ttest: 0.8422284\tbest: 0.8422284 (24)\ttotal: 34m 54s\tremaining: 22h 41m 40s\n",
            "25:\ttest: 0.8445592\tbest: 0.8445592 (25)\ttotal: 36m 17s\tremaining: 22h 39m 40s\n",
            "26:\ttest: 0.8455514\tbest: 0.8455514 (26)\ttotal: 37m 40s\tremaining: 22h 37m 51s\n",
            "27:\ttest: 0.8480315\tbest: 0.8480315 (27)\ttotal: 39m 5s\tremaining: 22h 37m\n",
            "28:\ttest: 0.8513207\tbest: 0.8513207 (28)\ttotal: 40m 27s\tremaining: 22h 34m 36s\n",
            "29:\ttest: 0.8516097\tbest: 0.8516097 (29)\ttotal: 41m 51s\tremaining: 22h 33m 26s\n",
            "30:\ttest: 0.8538443\tbest: 0.8538443 (30)\ttotal: 43m 12s\tremaining: 22h 30m 30s\n",
            "31:\ttest: 0.8548122\tbest: 0.8548122 (31)\ttotal: 44m 33s\tremaining: 22h 28m 5s\n",
            "32:\ttest: 0.8577886\tbest: 0.8577886 (32)\ttotal: 46m 3s\tremaining: 22h 29m 37s\n",
            "33:\ttest: 0.8589493\tbest: 0.8589493 (33)\ttotal: 47m 25s\tremaining: 22h 27m 15s\n",
            "34:\ttest: 0.8605578\tbest: 0.8605578 (34)\ttotal: 48m 46s\tremaining: 22h 24m 53s\n",
            "35:\ttest: 0.8607696\tbest: 0.8607696 (35)\ttotal: 50m 10s\tremaining: 22h 23m 34s\n",
            "36:\ttest: 0.8631439\tbest: 0.8631439 (36)\ttotal: 51m 34s\tremaining: 22h 22m 9s\n",
            "37:\ttest: 0.8650318\tbest: 0.8650318 (37)\ttotal: 52m 55s\tremaining: 22h 19m 56s\n",
            "38:\ttest: 0.8658938\tbest: 0.8658938 (38)\ttotal: 54m 16s\tremaining: 22h 17m 35s\n",
            "39:\ttest: 0.8670978\tbest: 0.8670978 (39)\ttotal: 55m 36s\tremaining: 22h 14m 46s\n",
            "40:\ttest: 0.8685041\tbest: 0.8685041 (40)\ttotal: 57m\tremaining: 22h 13m 24s\n",
            "41:\ttest: 0.8684607\tbest: 0.8685041 (40)\ttotal: 58m 23s\tremaining: 22h 12m 3s\n",
            "42:\ttest: 0.8691109\tbest: 0.8691109 (42)\ttotal: 59m 45s\tremaining: 22h 9m 58s\n",
            "43:\ttest: 0.8695058\tbest: 0.8695058 (43)\ttotal: 1h 1m 6s\tremaining: 22h 7m 53s\n",
            "44:\ttest: 0.8713166\tbest: 0.8713166 (44)\ttotal: 1h 2m 28s\tremaining: 22h 5m 41s\n",
            "45:\ttest: 0.8721257\tbest: 0.8721257 (45)\ttotal: 1h 3m 49s\tremaining: 22h 3m 39s\n",
            "46:\ttest: 0.8733922\tbest: 0.8733922 (46)\ttotal: 1h 5m 10s\tremaining: 22h 1m 35s\n",
            "47:\ttest: 0.8737438\tbest: 0.8737438 (47)\ttotal: 1h 6m 32s\tremaining: 21h 59m 38s\n",
            "48:\ttest: 0.8737968\tbest: 0.8737968 (48)\ttotal: 1h 7m 57s\tremaining: 21h 58m 59s\n",
            "49:\ttest: 0.8743506\tbest: 0.8743506 (49)\ttotal: 1h 9m 20s\tremaining: 21h 57m 31s\n",
            "50:\ttest: 0.8758628\tbest: 0.8758628 (50)\ttotal: 1h 10m 42s\tremaining: 21h 55m 38s\n",
            "51:\ttest: 0.8764166\tbest: 0.8764166 (51)\ttotal: 1h 12m 1s\tremaining: 21h 53m 5s\n",
            "52:\ttest: 0.8761710\tbest: 0.8764166 (51)\ttotal: 1h 13m 23s\tremaining: 21h 51m 15s\n",
            "53:\ttest: 0.8775677\tbest: 0.8775677 (53)\ttotal: 1h 14m 44s\tremaining: 21h 49m 27s\n",
            "54:\ttest: 0.8784201\tbest: 0.8784201 (54)\ttotal: 1h 16m 6s\tremaining: 21h 47m 40s\n",
            "55:\ttest: 0.8793976\tbest: 0.8793976 (55)\ttotal: 1h 17m 27s\tremaining: 21h 45m 46s\n",
            "56:\ttest: 0.8801537\tbest: 0.8801537 (56)\ttotal: 1h 18m 49s\tremaining: 21h 43m 57s\n",
            "57:\ttest: 0.8804523\tbest: 0.8804523 (57)\ttotal: 1h 20m 10s\tremaining: 21h 42m 15s\n",
            "58:\ttest: 0.8802501\tbest: 0.8804523 (57)\ttotal: 1h 21m 38s\tremaining: 21h 42m 6s\n",
            "59:\ttest: 0.8800478\tbest: 0.8804523 (57)\ttotal: 1h 23m 2s\tremaining: 21h 40m 55s\n",
            "60:\ttest: 0.8807076\tbest: 0.8807076 (60)\ttotal: 1h 24m 23s\tremaining: 21h 39m 10s\n",
            "61:\ttest: 0.8811651\tbest: 0.8811651 (61)\ttotal: 1h 25m 45s\tremaining: 21h 37m 32s\n",
            "62:\ttest: 0.8812614\tbest: 0.8812614 (62)\ttotal: 1h 27m 7s\tremaining: 21h 35m 46s\n",
            "63:\ttest: 0.8814637\tbest: 0.8814637 (63)\ttotal: 1h 28m 36s\tremaining: 21h 35m 56s\n",
            "64:\ttest: 0.8826243\tbest: 0.8826243 (64)\ttotal: 1h 30m 1s\tremaining: 21h 34m 56s\n",
            "65:\ttest: 0.8828266\tbest: 0.8828266 (65)\ttotal: 1h 31m 27s\tremaining: 21h 34m 11s\n",
            "66:\ttest: 0.8829759\tbest: 0.8829759 (66)\ttotal: 1h 32m 50s\tremaining: 21h 32m 50s\n",
            "67:\ttest: 0.8826868\tbest: 0.8829759 (66)\ttotal: 1h 34m 14s\tremaining: 21h 31m 42s\n",
            "68:\ttest: 0.8833466\tbest: 0.8833466 (68)\ttotal: 1h 35m 37s\tremaining: 21h 30m 19s\n",
            "69:\ttest: 0.8838475\tbest: 0.8838475 (69)\ttotal: 1h 37m 3s\tremaining: 21h 29m 23s\n",
            "70:\ttest: 0.8836452\tbest: 0.8838475 (69)\ttotal: 1h 38m 33s\tremaining: 21h 29m 32s\n",
            "71:\ttest: 0.8841990\tbest: 0.8841990 (71)\ttotal: 1h 39m 54s\tremaining: 21h 27m 41s\n",
            "72:\ttest: 0.8842424\tbest: 0.8842424 (72)\ttotal: 1h 41m 13s\tremaining: 21h 25m 30s\n",
            "73:\ttest: 0.8845940\tbest: 0.8845940 (73)\ttotal: 1h 42m 38s\tremaining: 21h 24m 29s\n",
            "74:\ttest: 0.8844976\tbest: 0.8845940 (73)\ttotal: 1h 44m 4s\tremaining: 21h 23m 40s\n",
            "75:\ttest: 0.8846999\tbest: 0.8846999 (75)\ttotal: 1h 45m 31s\tremaining: 21h 22m 53s\n",
            "76:\ttest: 0.8846469\tbest: 0.8846999 (75)\ttotal: 1h 46m 56s\tremaining: 21h 21m 51s\n",
            "77:\ttest: 0.8844976\tbest: 0.8846999 (75)\ttotal: 1h 48m 22s\tremaining: 21h 20m 57s\n",
            "78:\ttest: 0.8852008\tbest: 0.8852008 (78)\ttotal: 1h 49m 50s\tremaining: 21h 20m 35s\n",
            "79:\ttest: 0.8856583\tbest: 0.8856583 (79)\ttotal: 1h 51m 14s\tremaining: 21h 19m 14s\n",
            "80:\ttest: 0.8861062\tbest: 0.8861062 (80)\ttotal: 1h 52m 36s\tremaining: 21h 17m 32s\n",
            "81:\ttest: 0.8871175\tbest: 0.8871175 (81)\ttotal: 1h 54m\tremaining: 21h 16m 20s\n",
            "82:\ttest: 0.8872668\tbest: 0.8872668 (82)\ttotal: 1h 55m 29s\tremaining: 21h 16m 3s\n",
            "83:\ttest: 0.8883649\tbest: 0.8883649 (83)\ttotal: 1h 56m 51s\tremaining: 21h 14m 17s\n",
            "84:\ttest: 0.8889621\tbest: 0.8889621 (84)\ttotal: 1h 58m 17s\tremaining: 21h 13m 20s\n",
            "85:\ttest: 0.8887598\tbest: 0.8889621 (84)\ttotal: 1h 59m 40s\tremaining: 21h 11m 56s\n",
            "86:\ttest: 0.8881096\tbest: 0.8889621 (84)\ttotal: 2h 1m 4s\tremaining: 21h 10m 31s\n",
            "87:\ttest: 0.8884082\tbest: 0.8889621 (84)\ttotal: 2h 2m 29s\tremaining: 21h 9m 29s\n",
            "88:\ttest: 0.8887068\tbest: 0.8889621 (84)\ttotal: 2h 3m 55s\tremaining: 21h 8m 33s\n",
            "89:\ttest: 0.8887598\tbest: 0.8889621 (84)\ttotal: 2h 5m 19s\tremaining: 21h 7m 6s\n",
            "90:\ttest: 0.8886635\tbest: 0.8889621 (84)\ttotal: 2h 6m 42s\tremaining: 21h 5m 39s\n",
            "91:\ttest: 0.8887598\tbest: 0.8889621 (84)\ttotal: 2h 8m 7s\tremaining: 21h 4m 32s\n",
            "92:\ttest: 0.8885575\tbest: 0.8889621 (84)\ttotal: 2h 9m 37s\tremaining: 21h 4m 9s\n",
            "93:\ttest: 0.8884612\tbest: 0.8889621 (84)\ttotal: 2h 10m 56s\tremaining: 21h 2m 6s\n",
            "94:\ttest: 0.8881626\tbest: 0.8889621 (84)\ttotal: 2h 12m 20s\tremaining: 21h 41s\n",
            "Stopped by overfitting detector  (10 iterations wait)\n",
            "\n",
            "bestTest = 0.8889620613\n",
            "bestIteration = 84\n",
            "\n",
            "Shrink model to first 85 iterations.\n",
            "\n",
            "Model saved to: comp_lightgbm_model.cbm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3337695897.py:47: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  topk = df.groupby(\"clean_row_id\", group_keys=False).apply(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics (Validation Set):\n",
            "Accuracy@1 : 0.7609\n",
            "Recall@3   : 0.9729\n",
            "MRR        : 0.8664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3337695897.py:60: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  mrr = df.groupby(\"clean_row_id\", group_keys=False).apply(reciprocal_rank).mean()\n"
          ]
        }
      ]
    }
  ]
}