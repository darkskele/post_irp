{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training Model with Padded Investors\n",
    "\n",
    "This notebook was run on collab in order to fit all the data in RAM. This dataset includes all the investor data as well as the synthetic investors that were added to normalise the distributions of investor counts.\n",
    "\n",
    "The validation set was pulled from the clean non padded dataset such that we don't pullout our metrics with synthetic data."
   ],
   "metadata": {
    "id": "DjE0M1VsiF7a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from sqlalchemy import select, func\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ],
   "metadata": {
    "id": "SugiQtxNPKd_"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-kXyYQfHGJk",
    "outputId": "82a24272-dfb3-427c-9c75-0b750d325fc8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Paths to files in Drive\n",
    "db_path = \"/content/drive/MyDrive/Colab Notebooks/database.db\"\n",
    "train_ids_path = \"/content/train_ids.csv\"\n",
    "val_ids_path = \"/content/validation_ids.csv\""
   ],
   "metadata": {
    "id": "85sODZLeOz8Z"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pull whole dataset from SQL db into pandas frame."
   ],
   "metadata": {
    "id": "4ewxrbKKinbw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "conn = sqlite3.connect(db_path)  # or your local path\n",
    "full_df = pd.read_sql_query(\"SELECT * FROM feature_matrix\", conn)"
   ],
   "metadata": {
    "id": "4wNb5P0S4tEQ"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saved the training and validation IDs in a CSV so we can split the data set."
   ],
   "metadata": {
    "id": "Eh1_pW9kircr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_ids = pd.read_csv(train_ids_path)[\"train_ids\"].dropna().astype(int).tolist()\n",
    "val_ids = pd.read_csv(val_ids_path)[\"validation_ids\"].dropna().astype(int).tolist()"
   ],
   "metadata": {
    "id": "jiXxsfyUO6-s"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Validation dataset pulled from full set, remianing is training set. This is to ensure that we don't train on validation set."
   ],
   "metadata": {
    "id": "Ng6Bh82fizFD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "val_ids_set = set(val_ids)\n",
    "val_df = full_df[full_df[\"clean_row_id\"].isin(val_ids_set)]\n",
    "full_df = full_df[~full_df[\"clean_row_id\"].isin(val_ids_set)]"
   ],
   "metadata": {
    "id": "e-DWeeNh8gA3"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This was pulled from the initial model development notebook."
   ],
   "metadata": {
    "id": "8gT0AB51i6xZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_ranking_metrics(df, k=3):\n",
    "    # Group\n",
    "    grouped = df.groupby(\"clean_row_id\")\n",
    "\n",
    "    # Get top 1 and calculate accuracy\n",
    "    top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
    "    acc1 = (top1[\"label\"] == 1).mean()\n",
    "\n",
    "    # Same with recall\n",
    "    topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
    "    recall_k = topk.groupby(\"clean_row_id\")[\"label\"].max().mean()\n",
    "\n",
    "    # MMR\n",
    "    def reciprocal_rank(g):\n",
    "        sorted_g = g.sort_values(\"score\", ascending=False).reset_index()\n",
    "        match = sorted_g[sorted_g[\"label\"] == 1]\n",
    "        return 1.0 / (match.index[0] + 1) if not match.empty else 0.0\n",
    "\n",
    "    mrr = grouped.apply(reciprocal_rank).mean()\n",
    "    return acc1, recall_k, mrr"
   ],
   "metadata": {
    "id": "wHpoBtSfHSQT"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is so we can save logs to file in case collab logs us out and we lose our printouts."
   ],
   "metadata": {
    "id": "7sJFfIqLjAp1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Paths\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_path = f\"/content/drive/MyDrive/lgb_log_{timestamp}.txt\"\n",
    "model_path = f\"/content/drive/MyDrive/lgb_model_{timestamp}.txt\"\n",
    "\n",
    "# Logger\n",
    "log_file = open(log_path, \"w\")\n",
    "\n",
    "\n",
    "def log_callback(period=10):\n",
    "    def _callback(env):\n",
    "        if env.iteration % period == 0 or env.iteration + 1 == env.end_iteration:\n",
    "            result = f\"[{env.iteration}] \"\n",
    "            for data_name, eval_name, result_val, _ in env.evaluation_result_list:\n",
    "                result += f\"{data_name} {eval_name}: {result_val:.5f}  \"\n",
    "            print(result)\n",
    "            log_file.write(result + \"\\n\")\n",
    "            log_file.flush()\n",
    "\n",
    "    return _callback"
   ],
   "metadata": {
    "id": "s3xJFA0FazOO"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the model development notebook, changed slightly to fit our dataframes."
   ],
   "metadata": {
    "id": "jchAOpQrjIal"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(\n",
    "    train_ids,\n",
    "    val_ids,\n",
    "    parameters: dict,\n",
    "    n_rounds: int = 100,\n",
    "    lr_decay_gamma: float = 0.95,\n",
    "):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    # Load validation set\n",
    "    X_val = val_df.drop(\n",
    "        columns=[\"label\", \"clean_row_id\", \"investor\", \"firm\", \"template_id\"]\n",
    "    )\n",
    "    y_val = val_df[\"label\"]\n",
    "    val_group_sizes = val_df.groupby(\"clean_row_id\").size().tolist()\n",
    "    lgb_val = lgb.Dataset(\n",
    "        X_val, label=y_val, group=val_group_sizes, free_raw_data=False\n",
    "    )\n",
    "\n",
    "    # Load full training set\n",
    "    full_df\n",
    "    X_train = full_df.drop(\n",
    "        columns=[\"label\", \"clean_row_id\", \"investor\", \"firm\", \"template_id\"]\n",
    "    )\n",
    "    y_train = full_df[\"label\"]\n",
    "    train_group_sizes = full_df.groupby(\"clean_row_id\").size().tolist()\n",
    "    lgb_train = lgb.Dataset(\n",
    "        X_train, label=y_train, group=train_group_sizes, free_raw_data=False\n",
    "    )\n",
    "\n",
    "    # Learning rate schedule\n",
    "    def lr_decay(current_round):\n",
    "        return parameters[\"learning_rate\"] * (lr_decay_gamma**current_round)\n",
    "\n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params=parameters,\n",
    "        train_set=lgb_train,\n",
    "        num_boost_round=n_rounds,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        valid_names=[\"train\", \"val\"],\n",
    "        callbacks=[\n",
    "            lgb.reset_parameter(learning_rate=lr_decay),\n",
    "            lgb.early_stopping(stopping_rounds=20),\n",
    "            lgb.log_evaluation(period=1),\n",
    "            log_callback(10),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Predict and Evaluate\n",
    "    preds = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    val_df[\"score\"] = preds\n",
    "\n",
    "    acc1, recall3, mrr = compute_ranking_metrics(val_df, k=3)\n",
    "\n",
    "    print(\"\\Evaluation Metrics (Validation Set):\")\n",
    "    print(f\"Accuracy@1 : {acc1:.4f}\")\n",
    "    print(f\"Recall@3   : {recall3:.4f}\")\n",
    "    print(f\"MRR        : {mrr:.4f}\")\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "id": "M5s6rQbbHPp1"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": [\"ndcg\"],\n",
    "    \"eval_at\": [1, 3],\n",
    "    \"label_gain\": [0, 1],\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"num_leaves\": 31,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_split_gain\": 1e-3,\n",
    "    \"min_child_weight\": 1e-2,\n",
    "    \"max_delta_step\": 1.0,\n",
    "    \"scale_pos_weight\": 294,\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"force_row_wise\": True,\n",
    "}"
   ],
   "metadata": {
    "id": "9D7SQGsFUSFq"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Retrain\n",
    "model = train_model(train_ids, val_ids, params, n_rounds=100)\n",
    "log_file.close()\n",
    "model.save_model(model_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2nuOB1MPUci",
    "outputId": "ae1ba4ee-7f3e-46d3-def4-639b79ba1cd7"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0] train ndcg@1: 0.88226  train ndcg@3: 0.94278  val ndcg@1: 0.71994  val ndcg@3: 0.84847  \n",
      "[1]\ttrain's ndcg@1: 0.882255\ttrain's ndcg@3: 0.942779\tval's ndcg@1: 0.719942\tval's ndcg@3: 0.848469\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\ttrain's ndcg@1: 0.888038\ttrain's ndcg@3: 0.946874\tval's ndcg@1: 0.739981\tval's ndcg@3: 0.861974\n",
      "[3]\ttrain's ndcg@1: 0.895414\ttrain's ndcg@3: 0.955466\tval's ndcg@1: 0.766191\tval's ndcg@3: 0.892616\n",
      "[4]\ttrain's ndcg@1: 0.895566\ttrain's ndcg@3: 0.955594\tval's ndcg@1: 0.767092\tval's ndcg@3: 0.893209\n",
      "[5]\ttrain's ndcg@1: 0.906558\ttrain's ndcg@3: 0.961495\tval's ndcg@1: 0.803148\tval's ndcg@3: 0.913123\n",
      "[6]\ttrain's ndcg@1: 0.906558\ttrain's ndcg@3: 0.961495\tval's ndcg@1: 0.803148\tval's ndcg@3: 0.913123\n",
      "[7]\ttrain's ndcg@1: 0.906558\ttrain's ndcg@3: 0.961468\tval's ndcg@1: 0.803148\tval's ndcg@3: 0.913149\n",
      "[8]\ttrain's ndcg@1: 0.914675\ttrain's ndcg@3: 0.965774\tval's ndcg@1: 0.832201\tval's ndcg@3: 0.929222\n",
      "[9]\ttrain's ndcg@1: 0.915017\ttrain's ndcg@3: 0.965944\tval's ndcg@1: 0.833241\tval's ndcg@3: 0.929707\n",
      "[10]\ttrain's ndcg@1: 0.915073\ttrain's ndcg@3: 0.965981\tval's ndcg@1: 0.833241\tval's ndcg@3: 0.92962\n",
      "[10] train ndcg@1: 0.91498  train ndcg@3: 0.96600  val ndcg@1: 0.83393  val ndcg@3: 0.92995  \n",
      "[11]\ttrain's ndcg@1: 0.914977\ttrain's ndcg@3: 0.966\tval's ndcg@1: 0.833934\tval's ndcg@3: 0.929945\n",
      "[12]\ttrain's ndcg@1: 0.914977\ttrain's ndcg@3: 0.965993\tval's ndcg@1: 0.833934\tval's ndcg@3: 0.930024\n",
      "[13]\ttrain's ndcg@1: 0.914938\ttrain's ndcg@3: 0.965998\tval's ndcg@1: 0.833934\tval's ndcg@3: 0.929989\n",
      "[14]\ttrain's ndcg@1: 0.914938\ttrain's ndcg@3: 0.96601\tval's ndcg@1: 0.833934\tval's ndcg@3: 0.929989\n",
      "[15]\ttrain's ndcg@1: 0.914938\ttrain's ndcg@3: 0.966045\tval's ndcg@1: 0.833865\tval's ndcg@3: 0.929998\n",
      "[16]\ttrain's ndcg@1: 0.914938\ttrain's ndcg@3: 0.966045\tval's ndcg@1: 0.833865\tval's ndcg@3: 0.929998\n",
      "[17]\ttrain's ndcg@1: 0.914938\ttrain's ndcg@3: 0.966068\tval's ndcg@1: 0.833865\tval's ndcg@3: 0.930015\n",
      "[18]\ttrain's ndcg@1: 0.916395\ttrain's ndcg@3: 0.966666\tval's ndcg@1: 0.835182\tval's ndcg@3: 0.930891\n",
      "[19]\ttrain's ndcg@1: 0.920665\ttrain's ndcg@3: 0.968312\tval's ndcg@1: 0.840105\tval's ndcg@3: 0.932901\n",
      "[20]\ttrain's ndcg@1: 0.921995\ttrain's ndcg@3: 0.968853\tval's ndcg@1: 0.842186\tval's ndcg@3: 0.933968\n",
      "[20] train ndcg@1: 0.92264  train ndcg@3: 0.96913  val ndcg@1: 0.84295  val ndcg@3: 0.93447  \n",
      "[21]\ttrain's ndcg@1: 0.92264\ttrain's ndcg@3: 0.969132\tval's ndcg@1: 0.842948\tval's ndcg@3: 0.93447\n",
      "[22]\ttrain's ndcg@1: 0.923915\ttrain's ndcg@3: 0.96964\tval's ndcg@1: 0.843503\tval's ndcg@3: 0.934711\n",
      "[23]\ttrain's ndcg@1: 0.924281\ttrain's ndcg@3: 0.969799\tval's ndcg@1: 0.843711\tval's ndcg@3: 0.934825\n",
      "[24]\ttrain's ndcg@1: 0.924377\ttrain's ndcg@3: 0.969867\tval's ndcg@1: 0.843711\tval's ndcg@3: 0.934907\n",
      "[25]\ttrain's ndcg@1: 0.925046\ttrain's ndcg@3: 0.970162\tval's ndcg@1: 0.844335\tval's ndcg@3: 0.935307\n",
      "[26]\ttrain's ndcg@1: 0.92777\ttrain's ndcg@3: 0.971174\tval's ndcg@1: 0.846415\tval's ndcg@3: 0.9361\n",
      "[27]\ttrain's ndcg@1: 0.927818\ttrain's ndcg@3: 0.971229\tval's ndcg@1: 0.846485\tval's ndcg@3: 0.936282\n",
      "[28]\ttrain's ndcg@1: 0.927985\ttrain's ndcg@3: 0.971351\tval's ndcg@1: 0.846901\tval's ndcg@3: 0.936587\n",
      "[29]\ttrain's ndcg@1: 0.928471\ttrain's ndcg@3: 0.971562\tval's ndcg@1: 0.847039\tval's ndcg@3: 0.936922\n",
      "[30]\ttrain's ndcg@1: 0.928519\ttrain's ndcg@3: 0.971619\tval's ndcg@1: 0.84697\tval's ndcg@3: 0.936984\n",
      "[30] train ndcg@1: 0.92858  train ndcg@3: 0.97166  val ndcg@1: 0.84718  val ndcg@3: 0.93715  \n",
      "[31]\ttrain's ndcg@1: 0.928582\ttrain's ndcg@3: 0.971657\tval's ndcg@1: 0.847178\tval's ndcg@3: 0.937147\n",
      "[32]\ttrain's ndcg@1: 0.928598\ttrain's ndcg@3: 0.971698\tval's ndcg@1: 0.847109\tval's ndcg@3: 0.937209\n",
      "[33]\ttrain's ndcg@1: 0.929466\ttrain's ndcg@3: 0.972029\tval's ndcg@1: 0.848287\tval's ndcg@3: 0.937662\n",
      "[34]\ttrain's ndcg@1: 0.929634\ttrain's ndcg@3: 0.972097\tval's ndcg@1: 0.848634\tval's ndcg@3: 0.937877\n",
      "[35]\ttrain's ndcg@1: 0.929642\ttrain's ndcg@3: 0.9721\tval's ndcg@1: 0.848634\tval's ndcg@3: 0.937921\n",
      "[36]\ttrain's ndcg@1: 0.929673\ttrain's ndcg@3: 0.972139\tval's ndcg@1: 0.848911\tval's ndcg@3: 0.938268\n",
      "[37]\ttrain's ndcg@1: 0.929721\ttrain's ndcg@3: 0.972181\tval's ndcg@1: 0.848981\tval's ndcg@3: 0.938355\n",
      "[38]\ttrain's ndcg@1: 0.929777\ttrain's ndcg@3: 0.972205\tval's ndcg@1: 0.848981\tval's ndcg@3: 0.938337\n",
      "[39]\ttrain's ndcg@1: 0.929777\ttrain's ndcg@3: 0.972205\tval's ndcg@1: 0.84905\tval's ndcg@3: 0.938328\n",
      "[40]\ttrain's ndcg@1: 0.929825\ttrain's ndcg@3: 0.972228\tval's ndcg@1: 0.849189\tval's ndcg@3: 0.938379\n",
      "[40] train ndcg@1: 0.92977  train ndcg@3: 0.97222  val ndcg@1: 0.84919  val ndcg@3: 0.93838  \n",
      "[41]\ttrain's ndcg@1: 0.929769\ttrain's ndcg@3: 0.972217\tval's ndcg@1: 0.849189\tval's ndcg@3: 0.938379\n",
      "[42]\ttrain's ndcg@1: 0.929777\ttrain's ndcg@3: 0.97222\tval's ndcg@1: 0.849258\tval's ndcg@3: 0.938414\n",
      "[43]\ttrain's ndcg@1: 0.929777\ttrain's ndcg@3: 0.972221\tval's ndcg@1: 0.849327\tval's ndcg@3: 0.938379\n",
      "[44]\ttrain's ndcg@1: 0.929777\ttrain's ndcg@3: 0.972221\tval's ndcg@1: 0.849327\tval's ndcg@3: 0.938379\n",
      "[45]\ttrain's ndcg@1: 0.929777\ttrain's ndcg@3: 0.972223\tval's ndcg@1: 0.849258\tval's ndcg@3: 0.938344\n",
      "[46]\ttrain's ndcg@1: 0.929904\ttrain's ndcg@3: 0.972298\tval's ndcg@1: 0.849327\tval's ndcg@3: 0.93837\n",
      "[47]\ttrain's ndcg@1: 0.929897\ttrain's ndcg@3: 0.972338\tval's ndcg@1: 0.849327\tval's ndcg@3: 0.938379\n",
      "[48]\ttrain's ndcg@1: 0.930064\ttrain's ndcg@3: 0.972401\tval's ndcg@1: 0.849466\tval's ndcg@3: 0.93843\n",
      "[49]\ttrain's ndcg@1: 0.930064\ttrain's ndcg@3: 0.972414\tval's ndcg@1: 0.849535\tval's ndcg@3: 0.938421\n",
      "[50]\ttrain's ndcg@1: 0.93008\ttrain's ndcg@3: 0.972421\tval's ndcg@1: 0.849743\tval's ndcg@3: 0.938516\n",
      "[50] train ndcg@1: 0.93010  train ndcg@3: 0.97243  val ndcg@1: 0.84981  val ndcg@3: 0.93858  \n",
      "[51]\ttrain's ndcg@1: 0.930104\ttrain's ndcg@3: 0.972434\tval's ndcg@1: 0.849813\tval's ndcg@3: 0.938576\n",
      "[52]\ttrain's ndcg@1: 0.930104\ttrain's ndcg@3: 0.972437\tval's ndcg@1: 0.849813\tval's ndcg@3: 0.938595\n",
      "[53]\ttrain's ndcg@1: 0.930088\ttrain's ndcg@3: 0.972431\tval's ndcg@1: 0.849813\tval's ndcg@3: 0.938595\n",
      "[54]\ttrain's ndcg@1: 0.930167\ttrain's ndcg@3: 0.97246\tval's ndcg@1: 0.849951\tval's ndcg@3: 0.938646\n",
      "[55]\ttrain's ndcg@1: 0.930167\ttrain's ndcg@3: 0.97246\tval's ndcg@1: 0.849951\tval's ndcg@3: 0.938646\n",
      "[56]\ttrain's ndcg@1: 0.930167\ttrain's ndcg@3: 0.972349\tval's ndcg@1: 0.849951\tval's ndcg@3: 0.938091\n",
      "[57]\ttrain's ndcg@1: 0.930167\ttrain's ndcg@3: 0.972353\tval's ndcg@1: 0.849951\tval's ndcg@3: 0.938109\n",
      "[58]\ttrain's ndcg@1: 0.930199\ttrain's ndcg@3: 0.972365\tval's ndcg@1: 0.85009\tval's ndcg@3: 0.93816\n",
      "[59]\ttrain's ndcg@1: 0.930247\ttrain's ndcg@3: 0.972388\tval's ndcg@1: 0.850021\tval's ndcg@3: 0.938135\n",
      "[60]\ttrain's ndcg@1: 0.930247\ttrain's ndcg@3: 0.972388\tval's ndcg@1: 0.850021\tval's ndcg@3: 0.938135\n",
      "[60] train ndcg@1: 0.93032  train ndcg@3: 0.97241  val ndcg@1: 0.84995  val ndcg@3: 0.93812  \n",
      "[61]\ttrain's ndcg@1: 0.930319\ttrain's ndcg@3: 0.972414\tval's ndcg@1: 0.849951\tval's ndcg@3: 0.938118\n",
      "[62]\ttrain's ndcg@1: 0.930319\ttrain's ndcg@3: 0.972414\tval's ndcg@1: 0.849951\tval's ndcg@3: 0.938118\n",
      "[63]\ttrain's ndcg@1: 0.930343\ttrain's ndcg@3: 0.972423\tval's ndcg@1: 0.85009\tval's ndcg@3: 0.938179\n",
      "[64]\ttrain's ndcg@1: 0.930343\ttrain's ndcg@3: 0.972423\tval's ndcg@1: 0.85009\tval's ndcg@3: 0.938179\n",
      "[65]\ttrain's ndcg@1: 0.93039\ttrain's ndcg@3: 0.972441\tval's ndcg@1: 0.850229\tval's ndcg@3: 0.93823\n",
      "[66]\ttrain's ndcg@1: 0.93039\ttrain's ndcg@3: 0.972441\tval's ndcg@1: 0.850229\tval's ndcg@3: 0.93823\n",
      "[67]\ttrain's ndcg@1: 0.93039\ttrain's ndcg@3: 0.972441\tval's ndcg@1: 0.850229\tval's ndcg@3: 0.93823\n",
      "[68]\ttrain's ndcg@1: 0.93039\ttrain's ndcg@3: 0.972441\tval's ndcg@1: 0.850229\tval's ndcg@3: 0.938239\n",
      "[69]\ttrain's ndcg@1: 0.930406\ttrain's ndcg@3: 0.972447\tval's ndcg@1: 0.850159\tval's ndcg@3: 0.938213\n",
      "[70]\ttrain's ndcg@1: 0.930406\ttrain's ndcg@3: 0.972447\tval's ndcg@1: 0.850159\tval's ndcg@3: 0.938213\n",
      "[70] train ndcg@1: 0.93041  train ndcg@3: 0.97245  val ndcg@1: 0.85016  val ndcg@3: 0.93821  \n",
      "[71]\ttrain's ndcg@1: 0.930406\ttrain's ndcg@3: 0.972446\tval's ndcg@1: 0.850159\tval's ndcg@3: 0.938213\n",
      "[72]\ttrain's ndcg@1: 0.930406\ttrain's ndcg@3: 0.972446\tval's ndcg@1: 0.850159\tval's ndcg@3: 0.938222\n",
      "[73]\ttrain's ndcg@1: 0.930406\ttrain's ndcg@3: 0.972446\tval's ndcg@1: 0.850159\tval's ndcg@3: 0.938222\n",
      "[74]\ttrain's ndcg@1: 0.930406\ttrain's ndcg@3: 0.972446\tval's ndcg@1: 0.850159\tval's ndcg@3: 0.938222\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttrain's ndcg@1: 0.930167\ttrain's ndcg@3: 0.97246\tval's ndcg@1: 0.849951\tval's ndcg@3: 0.938646\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-8-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\Evaluation Metrics (Validation Set):\n",
      "Accuracy@1 : 0.8500\n",
      "Recall@3   : 0.9940\n",
      "MRR        : 0.9206\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-8-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7b50c17edbd0>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a drastic improvement.\n",
    "\n",
    "NDCG@1: 0.84995 - precision at top 1 spot is correct almost 85% of the time.\n",
    "\n",
    "NDCG@3: 0.93865 - at top3, it is correct almost 93% of the time.\n",
    "\n",
    "Accuracy@1: 0.8500, Recall@3: 0.9940 - indicating the same thing, most of the time our top three ranking predictions contains the correct template.\n",
    "\n",
    "Next thing to do is pull the incorrectly predicted ids and try to identify any trends in them.\n"
   ],
   "metadata": {
    "id": "_QsdKq8qjZ4z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_failed_topk_ids(df, k=3):\n",
    "    grouped = df.groupby(\"clean_row_id\")\n",
    "\n",
    "    # Get top-k by score\n",
    "    topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
    "\n",
    "    # Group again to check if any of the top-k have the correct label\n",
    "    has_hit = topk.groupby(\"clean_row_id\")[\"label\"].max()  # 1 if hit, 0 if miss\n",
    "    failed_ids = has_hit[has_hit == 0].index.tolist()\n",
    "\n",
    "    return failed_ids"
   ],
   "metadata": {
    "id": "lpbhYqGPkd2Z"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "failed_ids = get_failed_topk_ids(val_df, k=3)\n",
    "print(\n",
    "    f\"{len(failed_ids)} validation groups failed to rank the correct template in top {3}.\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZjdaHweWki-3",
    "outputId": "f175c908-19b8-4012-e18e-3563b2e1c9b9"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "87 validation groups failed to rank the correct template in top 3.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Time to save and i will analyse off line."
   ],
   "metadata": {
    "id": "L_-D7dM5rzuU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pd.DataFrame({\"clean_row_id\": failed_ids}).to_csv(\"failed_top3_ids.csv\", index=False)"
   ],
   "metadata": {
    "id": "zrjfllp9r23D"
   },
   "execution_count": 16,
   "outputs": []
  }
 ]
}