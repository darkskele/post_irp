{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V6E1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Catboost Benchmarking - Complex Names\n",
    "\n",
    "This notebook is to benchmark the LightGBM model with a catboost alternative. This iteration of benchmarking will focus on the complex name set.\n",
    "\n",
    "Complex names are flagged in the `lp_clean` dataset with these flags:\n",
    "\n",
    "```python\n",
    "complex_flags = [\n",
    "    \"has_multiple_first_names\",\n",
    "    \"has_middle_name\",\n",
    "    \"has_multiple_middle_names\",\n",
    "    \"has_multiple_last_names\",\n",
    "    \"has_nfkd_normalized\",\n",
    "    \"has_german_char\"\n",
    "]\n",
    "```"
   ],
   "metadata": {
    "id": "DjE0M1VsiF7a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install lightgbm\n",
    "!pip install sqlalchemy\n",
    "!pip install catboost"
   ],
   "metadata": {
    "id": "yNrhVNxUiAcx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0de988e7-ad03-429b-8ffe-a7c098a3f5c1"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
      "Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Collecting sqlalchemy\n",
      "  Downloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy)\n",
      "  Downloading greenlet-3.2.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (4.14.1)\n",
      "Downloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.2.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (585 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.5/585.5 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.2.3 sqlalchemy-2.0.41\n",
      "Collecting catboost\n",
      "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
      "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\n",
      "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: graphviz, catboost\n",
      "Successfully installed catboost-1.2.8 graphviz-0.21\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from catboost import CatBoostRanker, Pool\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from sqlalchemy import select, func\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ],
   "metadata": {
    "id": "SugiQtxNPKd_"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r-kXyYQfHGJk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0547f679-8a9f-41bb-aee2-92f575d55e85"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Paths to files in Drive\n",
    "db_path = \"/content/drive/MyDrive/Colab Notebooks/database.db\"\n",
    "train_ids_path = \"/content/train_com_ids.csv\"\n",
    "val_ids_path = \"/content/validation_com_ids.csv\""
   ],
   "metadata": {
    "id": "85sODZLeOz8Z"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pull whole complex dataset from SQL db into pandas frame."
   ],
   "metadata": {
    "id": "4ewxrbKKinbw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "conn = sqlite3.connect(db_path)  # or your local path\n",
    "full_df = pd.read_sql_query(\"SELECT * FROM feature_matrix_complex\", conn)"
   ],
   "metadata": {
    "id": "4wNb5P0S4tEQ"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saved the training and validation IDs in a CSV so we can split the data set."
   ],
   "metadata": {
    "id": "Eh1_pW9kircr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_ids = pd.read_csv(train_ids_path)[\"train_ids\"].dropna().astype(int).tolist()\n",
    "val_ids = pd.read_csv(val_ids_path)[\"validation_ids\"].dropna().astype(int).tolist()"
   ],
   "metadata": {
    "id": "jiXxsfyUO6-s"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Validation dataset pulled from full set, remianing is training set. This is to ensure that we don't train on validation set."
   ],
   "metadata": {
    "id": "Ng6Bh82fizFD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "val_ids_set = set(val_ids)\n",
    "val_df = full_df[full_df[\"clean_row_id\"].isin(val_ids_set)]\n",
    "full_df = full_df[~full_df[\"clean_row_id\"].isin(val_ids_set)]"
   ],
   "metadata": {
    "id": "e-DWeeNh8gA3"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This was pulled from the initial model development notebook."
   ],
   "metadata": {
    "id": "8gT0AB51i6xZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_ranking_metrics(df, k=3):\n",
    "    # Group\n",
    "    grouped = df.groupby(\"clean_row_id\")\n",
    "\n",
    "    # Get top 1 and calculate accuracy\n",
    "    top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
    "    acc1 = (top1[\"label\"] == 1).mean()\n",
    "\n",
    "    # Same with recall\n",
    "    topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n",
    "    recall_k = topk.groupby(\"clean_row_id\")[\"label\"].max().mean()\n",
    "\n",
    "    # MMR\n",
    "    def reciprocal_rank(g):\n",
    "        sorted_g = g.sort_values(\"score\", ascending=False).reset_index()\n",
    "        match = sorted_g[sorted_g[\"label\"] == 1]\n",
    "        return 1.0 / (match.index[0] + 1) if not match.empty else 0.0\n",
    "\n",
    "    mrr = grouped.apply(reciprocal_rank).mean()\n",
    "    return acc1, recall_k, mrr"
   ],
   "metadata": {
    "id": "wHpoBtSfHSQT"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This has been altered to fit a catboost model. It is largely the same but trains a different model."
   ],
   "metadata": {
    "id": "jchAOpQrjIal"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_catboost_model(train_df, val_df, parameters: dict, n_rounds: int = 500):\n",
    "    # Prepare group sizes (queries)\n",
    "    train_group = train_df.groupby(\"clean_row_id\").size().values\n",
    "    val_group = val_df.groupby(\"clean_row_id\").size().values\n",
    "\n",
    "    # Extract features\n",
    "    X_train = train_df.drop(\n",
    "        columns=[\"label\", \"clean_row_id\", \"investor\", \"firm\", \"template_id\"]\n",
    "    )\n",
    "    y_train = train_df[\"label\"]\n",
    "\n",
    "    X_val = val_df.drop(\n",
    "        columns=[\"label\", \"clean_row_id\", \"investor\", \"firm\", \"template_id\"]\n",
    "    )\n",
    "    y_val = val_df[\"label\"]\n",
    "\n",
    "    # Create Pools for CatBoost\n",
    "    train_pool = Pool(\n",
    "        data=X_train,\n",
    "        label=y_train,\n",
    "        group_id=np.repeat(range(len(train_group)), train_group),\n",
    "    )\n",
    "    val_pool = Pool(\n",
    "        data=X_val, label=y_val, group_id=np.repeat(range(len(val_group)), val_group)\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model = CatBoostRanker(iterations=n_rounds, **parameters)\n",
    "\n",
    "    model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=50, verbose=True)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    preds = model.predict(val_pool)\n",
    "    val_df = val_df.copy()\n",
    "    val_df[\"score\"] = preds\n",
    "\n",
    "    acc1, recall3, mrr = compute_ranking_metrics(val_df, k=3)\n",
    "\n",
    "    print(\"\\nEvaluation Metrics (Validation Set):\")\n",
    "    print(f\"Accuracy@1 : {acc1:.4f}\")\n",
    "    print(f\"Recall@3   : {recall3:.4f}\")\n",
    "    print(f\"MRR        : {mrr:.4f}\")\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "id": "M5s6rQbbHPp1"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "catboost_params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"loss_function\": \"YetiRank\",\n",
    "    \"random_seed\": 42,\n",
    "    \"task_type\": \"CPU\",\n",
    "}"
   ],
   "metadata": {
    "id": "9D7SQGsFUSFq"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Retrain\n",
    "model = train_catboost_model(full_df, val_df, catboost_params, n_rounds=2000)"
   ],
   "metadata": {
    "id": "H2nuOB1MPUci",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3e63f410-b8c8-4dc3-fc31-3a1a71a4283c"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:\ttest: 0.7712870\tbest: 0.7712870 (0)\ttotal: 7.46s\tremaining: 4h 8m 24s\n",
      "1:\ttest: 0.8074694\tbest: 0.8074694 (1)\ttotal: 14.8s\tremaining: 4h 6m 41s\n",
      "2:\ttest: 0.8082809\tbest: 0.8082809 (2)\ttotal: 22.3s\tremaining: 4h 7m 12s\n",
      "3:\ttest: 0.8889232\tbest: 0.8889232 (3)\ttotal: 29.7s\tremaining: 4h 7m 6s\n",
      "4:\ttest: 0.8892686\tbest: 0.8892686 (4)\ttotal: 37.2s\tremaining: 4h 7m 28s\n",
      "5:\ttest: 0.8890383\tbest: 0.8892686 (4)\ttotal: 44.7s\tremaining: 4h 7m 34s\n",
      "6:\ttest: 0.9091286\tbest: 0.9091286 (6)\ttotal: 52.2s\tremaining: 4h 7m 42s\n",
      "7:\ttest: 0.9266756\tbest: 0.9266756 (7)\ttotal: 59.7s\tremaining: 4h 7m 44s\n",
      "8:\ttest: 0.9282899\tbest: 0.9282899 (8)\ttotal: 1m 7s\tremaining: 4h 8m 4s\n",
      "9:\ttest: 0.9293530\tbest: 0.9293530 (9)\ttotal: 1m 14s\tremaining: 4h 8m 5s\n",
      "10:\ttest: 0.9279502\tbest: 0.9293530 (9)\ttotal: 1m 22s\tremaining: 4h 8m 15s\n",
      "11:\ttest: 0.9308014\tbest: 0.9308014 (11)\ttotal: 1m 29s\tremaining: 4h 8m 22s\n",
      "12:\ttest: 0.9310256\tbest: 0.9310256 (12)\ttotal: 1m 37s\tremaining: 4h 8m 38s\n",
      "13:\ttest: 0.9307229\tbest: 0.9310256 (12)\ttotal: 1m 45s\tremaining: 4h 8m 59s\n",
      "14:\ttest: 0.9396783\tbest: 0.9396783 (14)\ttotal: 1m 52s\tremaining: 4h 8m 54s\n",
      "15:\ttest: 0.9370000\tbest: 0.9396783 (14)\ttotal: 2m\tremaining: 4h 8m 55s\n",
      "16:\ttest: 0.9401603\tbest: 0.9401603 (16)\ttotal: 2m 8s\tremaining: 4h 9m 3s\n",
      "17:\ttest: 0.9421637\tbest: 0.9421637 (17)\ttotal: 2m 15s\tremaining: 4h 8m 56s\n",
      "18:\ttest: 0.9423231\tbest: 0.9423231 (18)\ttotal: 2m 23s\tremaining: 4h 8m 49s\n",
      "19:\ttest: 0.9433534\tbest: 0.9433534 (19)\ttotal: 2m 30s\tremaining: 4h 8m 43s\n",
      "20:\ttest: 0.9436076\tbest: 0.9436076 (20)\ttotal: 2m 38s\tremaining: 4h 8m 42s\n",
      "21:\ttest: 0.9434482\tbest: 0.9436076 (20)\ttotal: 2m 45s\tremaining: 4h 8m 33s\n",
      "22:\ttest: 0.9439748\tbest: 0.9439748 (22)\ttotal: 2m 53s\tremaining: 4h 8m 35s\n",
      "23:\ttest: 0.9443498\tbest: 0.9443498 (23)\ttotal: 3m 1s\tremaining: 4h 8m 26s\n",
      "24:\ttest: 0.9443498\tbest: 0.9443498 (23)\ttotal: 3m 8s\tremaining: 4h 8m 33s\n",
      "25:\ttest: 0.9443036\tbest: 0.9443498 (23)\ttotal: 3m 15s\tremaining: 4h 7m 49s\n",
      "26:\ttest: 0.9444187\tbest: 0.9444187 (26)\ttotal: 3m 22s\tremaining: 4h 7m 4s\n",
      "27:\ttest: 0.9449063\tbest: 0.9449063 (27)\ttotal: 3m 30s\tremaining: 4h 6m 30s\n",
      "28:\ttest: 0.9449063\tbest: 0.9449063 (27)\ttotal: 3m 37s\tremaining: 4h 6m 27s\n",
      "29:\ttest: 0.9457772\tbest: 0.9457772 (29)\ttotal: 3m 45s\tremaining: 4h 6m 22s\n",
      "30:\ttest: 0.9496824\tbest: 0.9496824 (30)\ttotal: 3m 52s\tremaining: 4h 6m 17s\n",
      "31:\ttest: 0.9498699\tbest: 0.9498699 (31)\ttotal: 4m\tremaining: 4h 6m 21s\n",
      "32:\ttest: 0.9504357\tbest: 0.9504357 (32)\ttotal: 4m 7s\tremaining: 4h 6m 20s\n",
      "33:\ttest: 0.9499278\tbest: 0.9504357 (32)\ttotal: 4m 15s\tremaining: 4h 6m 22s\n",
      "34:\ttest: 0.9505319\tbest: 0.9505319 (34)\ttotal: 4m 23s\tremaining: 4h 6m 17s\n",
      "35:\ttest: 0.9566235\tbest: 0.9566235 (35)\ttotal: 4m 30s\tremaining: 4h 6m 14s\n",
      "36:\ttest: 0.9566235\tbest: 0.9566235 (35)\ttotal: 4m 38s\tremaining: 4h 6m 12s\n",
      "37:\ttest: 0.9572345\tbest: 0.9572345 (37)\ttotal: 4m 45s\tremaining: 4h 6m 4s\n",
      "38:\ttest: 0.9573696\tbest: 0.9573696 (38)\ttotal: 4m 53s\tremaining: 4h 6m 2s\n",
      "39:\ttest: 0.9573696\tbest: 0.9573696 (38)\ttotal: 5m\tremaining: 4h 5m 33s\n",
      "40:\ttest: 0.9574528\tbest: 0.9574528 (40)\ttotal: 5m 7s\tremaining: 4h 5m\n",
      "41:\ttest: 0.9600638\tbest: 0.9600638 (41)\ttotal: 5m 14s\tremaining: 4h 4m 38s\n",
      "42:\ttest: 0.9604107\tbest: 0.9604107 (42)\ttotal: 5m 22s\tremaining: 4h 4m 38s\n",
      "43:\ttest: 0.9602794\tbest: 0.9604107 (42)\ttotal: 5m 30s\tremaining: 4h 4m 37s\n",
      "44:\ttest: 0.9609270\tbest: 0.9609270 (44)\ttotal: 5m 37s\tremaining: 4h 4m 10s\n",
      "45:\ttest: 0.9609270\tbest: 0.9609270 (44)\ttotal: 5m 44s\tremaining: 4h 3m 42s\n",
      "46:\ttest: 0.9610102\tbest: 0.9610102 (46)\ttotal: 5m 51s\tremaining: 4h 3m 22s\n",
      "47:\ttest: 0.9610275\tbest: 0.9610275 (47)\ttotal: 5m 58s\tremaining: 4h 3m 16s\n",
      "48:\ttest: 0.9608747\tbest: 0.9610275 (47)\ttotal: 6m 6s\tremaining: 4h 3m 16s\n",
      "49:\ttest: 0.9626015\tbest: 0.9626015 (49)\ttotal: 6m 14s\tremaining: 4h 3m 13s\n",
      "50:\ttest: 0.9633172\tbest: 0.9633172 (50)\ttotal: 6m 21s\tremaining: 4h 3m 7s\n",
      "51:\ttest: 0.9633172\tbest: 0.9633172 (50)\ttotal: 6m 29s\tremaining: 4h 3m 3s\n",
      "52:\ttest: 0.9634765\tbest: 0.9634765 (52)\ttotal: 6m 36s\tremaining: 4h 2m 57s\n",
      "53:\ttest: 0.9638515\tbest: 0.9638515 (53)\ttotal: 6m 44s\tremaining: 4h 2m 53s\n",
      "54:\ttest: 0.9639667\tbest: 0.9639667 (54)\ttotal: 6m 51s\tremaining: 4h 2m 45s\n",
      "55:\ttest: 0.9636198\tbest: 0.9639667 (54)\ttotal: 6m 59s\tremaining: 4h 2m 43s\n",
      "56:\ttest: 0.9636198\tbest: 0.9639667 (54)\ttotal: 7m 7s\tremaining: 4h 2m 40s\n",
      "57:\ttest: 0.9643417\tbest: 0.9643417 (57)\ttotal: 7m 14s\tremaining: 4h 2m 35s\n",
      "58:\ttest: 0.9654667\tbest: 0.9654667 (58)\ttotal: 7m 22s\tremaining: 4h 2m 31s\n",
      "59:\ttest: 0.9654667\tbest: 0.9654667 (58)\ttotal: 7m 29s\tremaining: 4h 2m 27s\n",
      "60:\ttest: 0.9653073\tbest: 0.9654667 (58)\ttotal: 7m 37s\tremaining: 4h 2m 24s\n",
      "61:\ttest: 0.9651719\tbest: 0.9654667 (58)\ttotal: 7m 45s\tremaining: 4h 2m 20s\n",
      "62:\ttest: 0.9646895\tbest: 0.9654667 (58)\ttotal: 7m 52s\tremaining: 4h 2m 17s\n",
      "63:\ttest: 0.9652870\tbest: 0.9654667 (58)\ttotal: 8m\tremaining: 4h 2m 15s\n",
      "64:\ttest: 0.9651276\tbest: 0.9654667 (58)\ttotal: 8m 8s\tremaining: 4h 2m 16s\n",
      "65:\ttest: 0.9649922\tbest: 0.9654667 (58)\ttotal: 8m 16s\tremaining: 4h 2m 14s\n",
      "66:\ttest: 0.9651797\tbest: 0.9654667 (58)\ttotal: 8m 23s\tremaining: 4h 2m 9s\n",
      "67:\ttest: 0.9653390\tbest: 0.9654667 (58)\ttotal: 8m 31s\tremaining: 4h 2m 6s\n",
      "68:\ttest: 0.9655265\tbest: 0.9655265 (68)\ttotal: 8m 39s\tremaining: 4h 2m 4s\n",
      "69:\ttest: 0.9653282\tbest: 0.9655265 (68)\ttotal: 8m 46s\tremaining: 4h 2m 1s\n",
      "70:\ttest: 0.9661012\tbest: 0.9661012 (70)\ttotal: 8m 54s\tremaining: 4h 1m 56s\n",
      "71:\ttest: 0.9661293\tbest: 0.9661293 (71)\ttotal: 9m 1s\tremaining: 4h 1m 52s\n",
      "72:\ttest: 0.9665363\tbest: 0.9665363 (72)\ttotal: 9m 9s\tremaining: 4h 1m 47s\n",
      "73:\ttest: 0.9666846\tbest: 0.9666846 (73)\ttotal: 9m 17s\tremaining: 4h 1m 44s\n",
      "74:\ttest: 0.9675216\tbest: 0.9675216 (74)\ttotal: 9m 25s\tremaining: 4h 1m 41s\n",
      "75:\ttest: 0.9674050\tbest: 0.9675216 (74)\ttotal: 9m 32s\tremaining: 4h 1m 37s\n",
      "76:\ttest: 0.9670300\tbest: 0.9675216 (74)\ttotal: 9m 40s\tremaining: 4h 1m 33s\n",
      "77:\ttest: 0.9669148\tbest: 0.9675216 (74)\ttotal: 9m 47s\tremaining: 4h 1m 28s\n",
      "78:\ttest: 0.9668262\tbest: 0.9675216 (74)\ttotal: 9m 55s\tremaining: 4h 1m 24s\n",
      "79:\ttest: 0.9668706\tbest: 0.9675216 (74)\ttotal: 10m 3s\tremaining: 4h 1m 19s\n",
      "80:\ttest: 0.9667555\tbest: 0.9675216 (74)\ttotal: 10m 10s\tremaining: 4h 1m 13s\n",
      "81:\ttest: 0.9674773\tbest: 0.9675216 (74)\ttotal: 10m 18s\tremaining: 4h 1m 8s\n",
      "82:\ttest: 0.9678566\tbest: 0.9678566 (82)\ttotal: 10m 26s\tremaining: 4h 1m 3s\n",
      "83:\ttest: 0.9673461\tbest: 0.9678566 (82)\ttotal: 10m 33s\tremaining: 4h 59s\n",
      "84:\ttest: 0.9674168\tbest: 0.9678566 (82)\ttotal: 10m 41s\tremaining: 4h 54s\n",
      "85:\ttest: 0.9675762\tbest: 0.9678566 (82)\ttotal: 10m 49s\tremaining: 4h 50s\n",
      "86:\ttest: 0.9670699\tbest: 0.9678566 (82)\ttotal: 10m 56s\tremaining: 4h 44s\n",
      "87:\ttest: 0.9673189\tbest: 0.9678566 (82)\ttotal: 11m 4s\tremaining: 4h 37s\n",
      "88:\ttest: 0.9676216\tbest: 0.9678566 (82)\ttotal: 11m 12s\tremaining: 4h 32s\n",
      "89:\ttest: 0.9681841\tbest: 0.9681841 (89)\ttotal: 11m 19s\tremaining: 4h 26s\n",
      "90:\ttest: 0.9685527\tbest: 0.9685527 (90)\ttotal: 11m 27s\tremaining: 4h 22s\n",
      "91:\ttest: 0.9687832\tbest: 0.9687832 (91)\ttotal: 11m 35s\tremaining: 4h 17s\n",
      "92:\ttest: 0.9689643\tbest: 0.9689643 (92)\ttotal: 11m 42s\tremaining: 4h 11s\n",
      "93:\ttest: 0.9690260\tbest: 0.9690260 (93)\ttotal: 11m 50s\tremaining: 4h 7s\n",
      "94:\ttest: 0.9687234\tbest: 0.9690260 (93)\ttotal: 11m 58s\tremaining: 4h 2s\n",
      "95:\ttest: 0.9685359\tbest: 0.9690260 (93)\ttotal: 12m 5s\tremaining: 3h 59m 55s\n",
      "96:\ttest: 0.9686952\tbest: 0.9690260 (93)\ttotal: 12m 13s\tremaining: 3h 59m 49s\n",
      "97:\ttest: 0.9684446\tbest: 0.9690260 (93)\ttotal: 12m 21s\tremaining: 3h 59m 43s\n",
      "98:\ttest: 0.9686524\tbest: 0.9690260 (93)\ttotal: 12m 28s\tremaining: 3h 59m 37s\n",
      "99:\ttest: 0.9685763\tbest: 0.9690260 (93)\ttotal: 12m 36s\tremaining: 3h 59m 30s\n",
      "100:\ttest: 0.9685698\tbest: 0.9690260 (93)\ttotal: 12m 44s\tremaining: 3h 59m 25s\n",
      "101:\ttest: 0.9685873\tbest: 0.9690260 (93)\ttotal: 12m 51s\tremaining: 3h 59m 19s\n",
      "102:\ttest: 0.9683998\tbest: 0.9690260 (93)\ttotal: 12m 59s\tremaining: 3h 59m 13s\n",
      "103:\ttest: 0.9682123\tbest: 0.9690260 (93)\ttotal: 13m 7s\tremaining: 3h 59m 7s\n",
      "104:\ttest: 0.9685150\tbest: 0.9690260 (93)\ttotal: 13m 14s\tremaining: 3h 59m\n",
      "105:\ttest: 0.9685086\tbest: 0.9690260 (93)\ttotal: 13m 22s\tremaining: 3h 58m 54s\n",
      "106:\ttest: 0.9686961\tbest: 0.9690260 (93)\ttotal: 13m 29s\tremaining: 3h 58m 48s\n",
      "107:\ttest: 0.9679742\tbest: 0.9690260 (93)\ttotal: 13m 37s\tremaining: 3h 58m 41s\n",
      "108:\ttest: 0.9681097\tbest: 0.9690260 (93)\ttotal: 13m 45s\tremaining: 3h 58m 35s\n",
      "109:\ttest: 0.9687592\tbest: 0.9690260 (93)\ttotal: 13m 52s\tremaining: 3h 58m 30s\n",
      "110:\ttest: 0.9687592\tbest: 0.9690260 (93)\ttotal: 14m\tremaining: 3h 58m 24s\n",
      "111:\ttest: 0.9683275\tbest: 0.9690260 (93)\ttotal: 14m 8s\tremaining: 3h 58m 18s\n",
      "112:\ttest: 0.9683275\tbest: 0.9690260 (93)\ttotal: 14m 15s\tremaining: 3h 58m 13s\n",
      "113:\ttest: 0.9681920\tbest: 0.9690260 (93)\ttotal: 14m 23s\tremaining: 3h 58m 8s\n",
      "114:\ttest: 0.9687545\tbest: 0.9690260 (93)\ttotal: 14m 31s\tremaining: 3h 58m 3s\n",
      "115:\ttest: 0.9692650\tbest: 0.9692650 (115)\ttotal: 14m 38s\tremaining: 3h 57m 55s\n",
      "116:\ttest: 0.9692650\tbest: 0.9692650 (115)\ttotal: 14m 46s\tremaining: 3h 57m 49s\n",
      "117:\ttest: 0.9688900\tbest: 0.9692650 (115)\ttotal: 14m 54s\tremaining: 3h 57m 43s\n",
      "118:\ttest: 0.9687545\tbest: 0.9692650 (115)\ttotal: 15m 2s\tremaining: 3h 57m 38s\n",
      "119:\ttest: 0.9688900\tbest: 0.9692650 (115)\ttotal: 15m 9s\tremaining: 3h 57m 30s\n",
      "120:\ttest: 0.9685873\tbest: 0.9692650 (115)\ttotal: 15m 17s\tremaining: 3h 57m 25s\n",
      "121:\ttest: 0.9685873\tbest: 0.9692650 (115)\ttotal: 15m 25s\tremaining: 3h 57m 19s\n",
      "122:\ttest: 0.9685873\tbest: 0.9692650 (115)\ttotal: 15m 32s\tremaining: 3h 57m 12s\n",
      "123:\ttest: 0.9691434\tbest: 0.9692650 (115)\ttotal: 15m 40s\tremaining: 3h 57m 5s\n",
      "124:\ttest: 0.9691434\tbest: 0.9692650 (115)\ttotal: 15m 47s\tremaining: 3h 56m 58s\n",
      "125:\ttest: 0.9691434\tbest: 0.9692650 (115)\ttotal: 15m 55s\tremaining: 3h 56m 51s\n",
      "126:\ttest: 0.9686611\tbest: 0.9692650 (115)\ttotal: 16m 3s\tremaining: 3h 56m 44s\n",
      "127:\ttest: 0.9685017\tbest: 0.9692650 (115)\ttotal: 16m 10s\tremaining: 3h 56m 37s\n",
      "128:\ttest: 0.9685081\tbest: 0.9692650 (115)\ttotal: 16m 18s\tremaining: 3h 56m 32s\n",
      "129:\ttest: 0.9688043\tbest: 0.9692650 (115)\ttotal: 16m 26s\tremaining: 3h 56m 25s\n",
      "130:\ttest: 0.9688043\tbest: 0.9692650 (115)\ttotal: 16m 33s\tremaining: 3h 56m 21s\n",
      "131:\ttest: 0.9687788\tbest: 0.9692650 (115)\ttotal: 16m 41s\tremaining: 3h 56m 13s\n",
      "132:\ttest: 0.9685602\tbest: 0.9692650 (115)\ttotal: 16m 49s\tremaining: 3h 56m 8s\n",
      "133:\ttest: 0.9685602\tbest: 0.9692650 (115)\ttotal: 16m 57s\tremaining: 3h 56m 2s\n",
      "134:\ttest: 0.9687477\tbest: 0.9692650 (115)\ttotal: 17m 4s\tremaining: 3h 55m 54s\n",
      "135:\ttest: 0.9687477\tbest: 0.9692650 (115)\ttotal: 17m 12s\tremaining: 3h 55m 46s\n",
      "136:\ttest: 0.9689070\tbest: 0.9692650 (115)\ttotal: 17m 19s\tremaining: 3h 55m 41s\n",
      "137:\ttest: 0.9686956\tbest: 0.9692650 (115)\ttotal: 17m 27s\tremaining: 3h 55m 35s\n",
      "138:\ttest: 0.9688831\tbest: 0.9692650 (115)\ttotal: 17m 35s\tremaining: 3h 55m 28s\n",
      "139:\ttest: 0.9688831\tbest: 0.9692650 (115)\ttotal: 17m 42s\tremaining: 3h 55m 22s\n",
      "140:\ttest: 0.9688550\tbest: 0.9692650 (115)\ttotal: 17m 50s\tremaining: 3h 55m 14s\n",
      "141:\ttest: 0.9690425\tbest: 0.9692650 (115)\ttotal: 17m 58s\tremaining: 3h 55m 7s\n",
      "142:\ttest: 0.9690425\tbest: 0.9692650 (115)\ttotal: 18m 5s\tremaining: 3h 55m 1s\n",
      "143:\ttest: 0.9688831\tbest: 0.9692650 (115)\ttotal: 18m 13s\tremaining: 3h 54m 53s\n",
      "144:\ttest: 0.9686956\tbest: 0.9692650 (115)\ttotal: 18m 21s\tremaining: 3h 54m 46s\n",
      "145:\ttest: 0.9686956\tbest: 0.9692650 (115)\ttotal: 18m 28s\tremaining: 3h 54m 38s\n",
      "146:\ttest: 0.9685363\tbest: 0.9692650 (115)\ttotal: 18m 36s\tremaining: 3h 54m 31s\n",
      "147:\ttest: 0.9689113\tbest: 0.9692650 (115)\ttotal: 18m 43s\tremaining: 3h 54m 23s\n",
      "148:\ttest: 0.9684548\tbest: 0.9692650 (115)\ttotal: 18m 51s\tremaining: 3h 54m 17s\n",
      "149:\ttest: 0.9681361\tbest: 0.9692650 (115)\ttotal: 18m 59s\tremaining: 3h 54m 10s\n",
      "150:\ttest: 0.9680101\tbest: 0.9692650 (115)\ttotal: 19m 6s\tremaining: 3h 54m 3s\n",
      "151:\ttest: 0.9683825\tbest: 0.9692650 (115)\ttotal: 19m 14s\tremaining: 3h 53m 56s\n",
      "152:\ttest: 0.9687627\tbest: 0.9692650 (115)\ttotal: 19m 22s\tremaining: 3h 53m 50s\n",
      "153:\ttest: 0.9685752\tbest: 0.9692650 (115)\ttotal: 19m 29s\tremaining: 3h 53m 42s\n",
      "154:\ttest: 0.9687627\tbest: 0.9692650 (115)\ttotal: 19m 37s\tremaining: 3h 53m 36s\n",
      "155:\ttest: 0.9688670\tbest: 0.9692650 (115)\ttotal: 19m 45s\tremaining: 3h 53m 30s\n",
      "156:\ttest: 0.9691096\tbest: 0.9692650 (115)\ttotal: 19m 52s\tremaining: 3h 53m 24s\n",
      "157:\ttest: 0.9690264\tbest: 0.9692650 (115)\ttotal: 20m\tremaining: 3h 53m 16s\n",
      "158:\ttest: 0.9693733\tbest: 0.9693733 (158)\ttotal: 20m 8s\tremaining: 3h 53m 8s\n",
      "159:\ttest: 0.9692659\tbest: 0.9693733 (158)\ttotal: 20m 15s\tremaining: 3h 53m 2s\n",
      "160:\ttest: 0.9690023\tbest: 0.9693733 (158)\ttotal: 20m 23s\tremaining: 3h 52m 55s\n",
      "161:\ttest: 0.9690023\tbest: 0.9693733 (158)\ttotal: 20m 31s\tremaining: 3h 52m 48s\n",
      "162:\ttest: 0.9691898\tbest: 0.9693733 (158)\ttotal: 20m 38s\tremaining: 3h 52m 42s\n",
      "163:\ttest: 0.9686554\tbest: 0.9693733 (158)\ttotal: 20m 46s\tremaining: 3h 52m 34s\n",
      "164:\ttest: 0.9687533\tbest: 0.9693733 (158)\ttotal: 20m 54s\tremaining: 3h 52m 26s\n",
      "165:\ttest: 0.9690304\tbest: 0.9693733 (158)\ttotal: 21m 1s\tremaining: 3h 52m 19s\n",
      "166:\ttest: 0.9686554\tbest: 0.9693733 (158)\ttotal: 21m 9s\tremaining: 3h 52m 13s\n",
      "167:\ttest: 0.9686554\tbest: 0.9693733 (158)\ttotal: 21m 17s\tremaining: 3h 52m 6s\n",
      "168:\ttest: 0.9686554\tbest: 0.9693733 (158)\ttotal: 21m 24s\tremaining: 3h 51m 58s\n",
      "169:\ttest: 0.9686554\tbest: 0.9693733 (158)\ttotal: 21m 32s\tremaining: 3h 51m 51s\n",
      "170:\ttest: 0.9686554\tbest: 0.9693733 (158)\ttotal: 21m 39s\tremaining: 3h 51m 44s\n",
      "171:\ttest: 0.9686554\tbest: 0.9693733 (158)\ttotal: 21m 47s\tremaining: 3h 51m 37s\n",
      "172:\ttest: 0.9687705\tbest: 0.9693733 (158)\ttotal: 21m 55s\tremaining: 3h 51m 30s\n",
      "173:\ttest: 0.9689580\tbest: 0.9693733 (158)\ttotal: 22m 2s\tremaining: 3h 51m 23s\n",
      "174:\ttest: 0.9687705\tbest: 0.9693733 (158)\ttotal: 22m 10s\tremaining: 3h 51m 16s\n",
      "175:\ttest: 0.9690559\tbest: 0.9693733 (158)\ttotal: 22m 18s\tremaining: 3h 51m 9s\n",
      "176:\ttest: 0.9691455\tbest: 0.9693733 (158)\ttotal: 22m 25s\tremaining: 3h 51m 1s\n",
      "177:\ttest: 0.9693330\tbest: 0.9693733 (158)\ttotal: 22m 33s\tremaining: 3h 50m 54s\n",
      "178:\ttest: 0.9693330\tbest: 0.9693733 (158)\ttotal: 22m 41s\tremaining: 3h 50m 46s\n",
      "179:\ttest: 0.9693330\tbest: 0.9693733 (158)\ttotal: 22m 48s\tremaining: 3h 50m 39s\n",
      "180:\ttest: 0.9693330\tbest: 0.9693733 (158)\ttotal: 22m 56s\tremaining: 3h 50m 32s\n",
      "181:\ttest: 0.9696560\tbest: 0.9696560 (181)\ttotal: 23m 4s\tremaining: 3h 50m 25s\n",
      "182:\ttest: 0.9694940\tbest: 0.9696560 (181)\ttotal: 23m 11s\tremaining: 3h 50m 17s\n",
      "183:\ttest: 0.9692810\tbest: 0.9696560 (181)\ttotal: 23m 19s\tremaining: 3h 50m 11s\n",
      "184:\ttest: 0.9692810\tbest: 0.9696560 (181)\ttotal: 23m 27s\tremaining: 3h 50m 4s\n",
      "185:\ttest: 0.9692810\tbest: 0.9696560 (181)\ttotal: 23m 34s\tremaining: 3h 49m 58s\n",
      "186:\ttest: 0.9701289\tbest: 0.9701289 (186)\ttotal: 23m 42s\tremaining: 3h 49m 51s\n",
      "187:\ttest: 0.9702440\tbest: 0.9702440 (187)\ttotal: 23m 50s\tremaining: 3h 49m 43s\n",
      "188:\ttest: 0.9696423\tbest: 0.9702440 (187)\ttotal: 23m 57s\tremaining: 3h 49m 36s\n",
      "189:\ttest: 0.9696115\tbest: 0.9702440 (187)\ttotal: 24m 5s\tremaining: 3h 49m 29s\n",
      "190:\ttest: 0.9696115\tbest: 0.9702440 (187)\ttotal: 24m 13s\tremaining: 3h 49m 23s\n",
      "191:\ttest: 0.9696115\tbest: 0.9702440 (187)\ttotal: 24m 20s\tremaining: 3h 49m 16s\n",
      "192:\ttest: 0.9697990\tbest: 0.9702440 (187)\ttotal: 24m 28s\tremaining: 3h 49m 9s\n",
      "193:\ttest: 0.9694240\tbest: 0.9702440 (187)\ttotal: 24m 36s\tremaining: 3h 49m 2s\n",
      "194:\ttest: 0.9692997\tbest: 0.9702440 (187)\ttotal: 24m 43s\tremaining: 3h 48m 55s\n",
      "195:\ttest: 0.9694590\tbest: 0.9702440 (187)\ttotal: 24m 51s\tremaining: 3h 48m 48s\n",
      "196:\ttest: 0.9701528\tbest: 0.9702440 (187)\ttotal: 24m 59s\tremaining: 3h 48m 40s\n",
      "197:\ttest: 0.9703122\tbest: 0.9703122 (197)\ttotal: 25m 6s\tremaining: 3h 48m 34s\n",
      "198:\ttest: 0.9701528\tbest: 0.9703122 (197)\ttotal: 25m 14s\tremaining: 3h 48m 27s\n",
      "199:\ttest: 0.9703122\tbest: 0.9703122 (197)\ttotal: 25m 22s\tremaining: 3h 48m 20s\n",
      "200:\ttest: 0.9706351\tbest: 0.9706351 (200)\ttotal: 25m 29s\tremaining: 3h 48m 12s\n",
      "201:\ttest: 0.9706351\tbest: 0.9706351 (201)\ttotal: 25m 37s\tremaining: 3h 48m 4s\n",
      "202:\ttest: 0.9702882\tbest: 0.9706351 (201)\ttotal: 25m 44s\tremaining: 3h 47m 56s\n",
      "203:\ttest: 0.9704757\tbest: 0.9706351 (201)\ttotal: 25m 52s\tremaining: 3h 47m 48s\n",
      "204:\ttest: 0.9702882\tbest: 0.9706351 (201)\ttotal: 26m\tremaining: 3h 47m 42s\n",
      "205:\ttest: 0.9706632\tbest: 0.9706632 (205)\ttotal: 26m 7s\tremaining: 3h 47m 35s\n",
      "206:\ttest: 0.9707945\tbest: 0.9707945 (206)\ttotal: 26m 15s\tremaining: 3h 47m 27s\n",
      "207:\ttest: 0.9703564\tbest: 0.9707945 (206)\ttotal: 26m 23s\tremaining: 3h 47m 20s\n",
      "208:\ttest: 0.9710220\tbest: 0.9710220 (208)\ttotal: 26m 30s\tremaining: 3h 47m 12s\n",
      "209:\ttest: 0.9710220\tbest: 0.9710220 (208)\ttotal: 26m 38s\tremaining: 3h 47m 5s\n",
      "210:\ttest: 0.9712445\tbest: 0.9712445 (210)\ttotal: 26m 46s\tremaining: 3h 46m 59s\n",
      "211:\ttest: 0.9710289\tbest: 0.9712445 (210)\ttotal: 26m 54s\tremaining: 3h 46m 53s\n",
      "212:\ttest: 0.9710289\tbest: 0.9712445 (210)\ttotal: 27m 1s\tremaining: 3h 46m 45s\n",
      "213:\ttest: 0.9706189\tbest: 0.9712445 (210)\ttotal: 27m 9s\tremaining: 3h 46m 38s\n",
      "214:\ttest: 0.9704314\tbest: 0.9712445 (210)\ttotal: 27m 17s\tremaining: 3h 46m 31s\n",
      "215:\ttest: 0.9704314\tbest: 0.9712445 (210)\ttotal: 27m 24s\tremaining: 3h 46m 23s\n",
      "216:\ttest: 0.9704314\tbest: 0.9712445 (210)\ttotal: 27m 32s\tremaining: 3h 46m 16s\n",
      "217:\ttest: 0.9708695\tbest: 0.9712445 (210)\ttotal: 27m 39s\tremaining: 3h 46m 9s\n",
      "218:\ttest: 0.9706820\tbest: 0.9712445 (210)\ttotal: 27m 47s\tremaining: 3h 46m 1s\n",
      "219:\ttest: 0.9706820\tbest: 0.9712445 (210)\ttotal: 27m 55s\tremaining: 3h 45m 53s\n",
      "220:\ttest: 0.9706820\tbest: 0.9712445 (210)\ttotal: 28m 2s\tremaining: 3h 45m 47s\n",
      "221:\ttest: 0.9706820\tbest: 0.9712445 (210)\ttotal: 28m 10s\tremaining: 3h 45m 39s\n",
      "222:\ttest: 0.9702039\tbest: 0.9712445 (210)\ttotal: 28m 18s\tremaining: 3h 45m 32s\n",
      "223:\ttest: 0.9701237\tbest: 0.9712445 (210)\ttotal: 28m 26s\tremaining: 3h 45m 26s\n",
      "224:\ttest: 0.9701237\tbest: 0.9712445 (210)\ttotal: 28m 33s\tremaining: 3h 45m 18s\n",
      "225:\ttest: 0.9700606\tbest: 0.9712445 (210)\ttotal: 28m 41s\tremaining: 3h 45m 11s\n",
      "226:\ttest: 0.9700606\tbest: 0.9712445 (210)\ttotal: 28m 49s\tremaining: 3h 45m 5s\n",
      "227:\ttest: 0.9697658\tbest: 0.9712445 (210)\ttotal: 28m 56s\tremaining: 3h 44m 57s\n",
      "228:\ttest: 0.9701502\tbest: 0.9712445 (210)\ttotal: 29m 4s\tremaining: 3h 44m 50s\n",
      "229:\ttest: 0.9701502\tbest: 0.9712445 (210)\ttotal: 29m 12s\tremaining: 3h 44m 43s\n",
      "230:\ttest: 0.9701783\tbest: 0.9712445 (210)\ttotal: 29m 19s\tremaining: 3h 44m 36s\n",
      "231:\ttest: 0.9703112\tbest: 0.9712445 (210)\ttotal: 29m 27s\tremaining: 3h 44m 28s\n",
      "232:\ttest: 0.9706581\tbest: 0.9712445 (210)\ttotal: 29m 35s\tremaining: 3h 44m 21s\n",
      "233:\ttest: 0.9706581\tbest: 0.9712445 (210)\ttotal: 29m 42s\tremaining: 3h 44m 13s\n",
      "234:\ttest: 0.9709607\tbest: 0.9712445 (210)\ttotal: 29m 50s\tremaining: 3h 44m 7s\n",
      "235:\ttest: 0.9706754\tbest: 0.9712445 (210)\ttotal: 29m 58s\tremaining: 3h 44m\n",
      "236:\ttest: 0.9704008\tbest: 0.9712445 (210)\ttotal: 30m 5s\tremaining: 3h 43m 52s\n",
      "237:\ttest: 0.9702133\tbest: 0.9712445 (210)\ttotal: 30m 13s\tremaining: 3h 43m 45s\n",
      "238:\ttest: 0.9704008\tbest: 0.9712445 (210)\ttotal: 30m 21s\tremaining: 3h 43m 38s\n",
      "239:\ttest: 0.9702133\tbest: 0.9712445 (210)\ttotal: 30m 28s\tremaining: 3h 43m 31s\n",
      "240:\ttest: 0.9702133\tbest: 0.9712445 (210)\ttotal: 30m 36s\tremaining: 3h 43m 24s\n",
      "241:\ttest: 0.9703727\tbest: 0.9712445 (210)\ttotal: 30m 44s\tremaining: 3h 43m 17s\n",
      "242:\ttest: 0.9699627\tbest: 0.9712445 (210)\ttotal: 30m 51s\tremaining: 3h 43m 10s\n",
      "243:\ttest: 0.9699627\tbest: 0.9712445 (210)\ttotal: 30m 59s\tremaining: 3h 43m 3s\n",
      "244:\ttest: 0.9701502\tbest: 0.9712445 (210)\ttotal: 31m 7s\tremaining: 3h 42m 56s\n",
      "245:\ttest: 0.9710033\tbest: 0.9712445 (210)\ttotal: 31m 15s\tremaining: 3h 42m 49s\n",
      "246:\ttest: 0.9710033\tbest: 0.9712445 (210)\ttotal: 31m 22s\tremaining: 3h 42m 41s\n",
      "247:\ttest: 0.9708440\tbest: 0.9712445 (210)\ttotal: 31m 30s\tremaining: 3h 42m 35s\n",
      "248:\ttest: 0.9708440\tbest: 0.9712445 (210)\ttotal: 31m 38s\tremaining: 3h 42m 27s\n",
      "249:\ttest: 0.9701783\tbest: 0.9712445 (210)\ttotal: 31m 45s\tremaining: 3h 42m 19s\n",
      "250:\ttest: 0.9699908\tbest: 0.9712445 (210)\ttotal: 31m 53s\tremaining: 3h 42m 12s\n",
      "251:\ttest: 0.9701783\tbest: 0.9712445 (210)\ttotal: 32m\tremaining: 3h 42m 4s\n",
      "252:\ttest: 0.9699908\tbest: 0.9712445 (210)\ttotal: 32m 8s\tremaining: 3h 41m 57s\n",
      "253:\ttest: 0.9699908\tbest: 0.9712445 (210)\ttotal: 32m 16s\tremaining: 3h 41m 50s\n",
      "254:\ttest: 0.9701783\tbest: 0.9712445 (210)\ttotal: 32m 24s\tremaining: 3h 41m 43s\n",
      "255:\ttest: 0.9700632\tbest: 0.9712445 (210)\ttotal: 32m 31s\tremaining: 3h 41m 36s\n",
      "256:\ttest: 0.9704382\tbest: 0.9712445 (210)\ttotal: 32m 39s\tremaining: 3h 41m 28s\n",
      "257:\ttest: 0.9702226\tbest: 0.9712445 (210)\ttotal: 32m 46s\tremaining: 3h 41m 20s\n",
      "258:\ttest: 0.9702226\tbest: 0.9712445 (210)\ttotal: 32m 54s\tremaining: 3h 41m 13s\n",
      "259:\ttest: 0.9702226\tbest: 0.9712445 (210)\ttotal: 33m 2s\tremaining: 3h 41m 6s\n",
      "260:\ttest: 0.9700351\tbest: 0.9712445 (210)\ttotal: 33m 9s\tremaining: 3h 40m 58s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.9712444948\n",
      "bestIteration = 210\n",
      "\n",
      "Shrink model to first 211 iterations.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-10-2860974315.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top1 = grouped.apply(lambda g: g.loc[g[\"score\"].idxmax()]).reset_index(drop=True)\n",
      "/tmp/ipython-input-10-2860974315.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  topk = grouped.apply(lambda g: g.nlargest(k, \"score\")).reset_index(drop=True)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluation Metrics (Validation Set):\n",
      "Accuracy@1 : 0.8550\n",
      "Recall@3   : 0.9850\n",
      "MRR        : 0.9203\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-10-2860974315.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mrr = grouped.apply(reciprocal_rank).mean()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is significant improvement from the LightGBM model. Both our own metrics and the Catboost properitary metrics indicate a significant perfect with near perfect ranking quality on the evalualtion set.\n",
    "\n",
    "YetiRank Loss: 0.9712444948 - near perfect ranking quality.\n",
    "\n",
    "Accuracy@1: 0.8550, Recall@3: 0.9850 - indicating the same thing, most of the time our top three ranking predictions contains the correct template.\n",
    "\n",
    "The catboost model seems to provide a much better result than the LightGBM variant. I think the final thing to do be resolving to one of the two is to do hyperparameter training on both models and do a final comparission on both datasets."
   ],
   "metadata": {
    "id": "_QsdKq8qjZ4z"
   }
  }
 ]
}