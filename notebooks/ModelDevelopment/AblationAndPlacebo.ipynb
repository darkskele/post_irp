{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20dd1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from db.db import init_db, read_table\n",
    "from db.models import TableName\n",
    "from etl.load import load_raw_data, load_clean_data\n",
    "from etl.pipeline import _process_table\n",
    "from pattern_mining.pipeline import _process_and_load_cleaned_data\n",
    "from email_prediction.feature_engineering.pipeline import _build_profile_and_pad, build_feature_matrix\n",
    "from email_prediction.feature_engineering.features.feature_builder import _build_firm_stats_lookup, _build_firm_template_lookup, _build_rows_for_investor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRanker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8c874",
   "metadata": {},
   "source": [
    "# Placebo and Ablation Testing\n",
    "\n",
    "Following feedback from Antony I will do some furhter testing to confirm importance of feature types and synthetic investor placebo testing.\n",
    "\n",
    "Lets start with placebo testing of synthetic investors. We will do this by generating a modest dataset of synthetic investors, then we will match the data size in a separate set by randomly selecting duplicate investors. We will no do segmentation and use a small portion of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1323a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables created successfully.\n",
      "Read from LP Contact Data\n",
      "Write to LP complete!\n",
      "Written 165268 records to LP\n",
      "Read LP table from database!\n",
      "Table Normalisation Complete!\n",
      "Table Regex-Cleaning Complete!\n",
      "Removed 18 bad investor rows!\n",
      "Table Standardisation Complete!\n",
      "87 invalid email lengths found!\n",
      "73142 missing emails!\n",
      "31 invalid email formats found!\n",
      "396 invalid LinkedIns found!\n",
      "47184 missing LinkedIns!\n",
      "2181 invalid firm lengths found!\n",
      "0 missing firms!\n",
      "8897 invalid investor lengths found!\n",
      "0 missing investors!\n",
      "22564 missing linkedin and email pairs!\n",
      "Removed 30 emails with invalid local-part characters.\n",
      "Wrote 92078 rows to LP_CLEAN\n",
      "Also wrote 92078 rows to COMBINED_CLEAN\n"
     ]
    }
   ],
   "source": [
    "# Setup DB and load raw data\n",
    "init_db()\n",
    "load_raw_data(TableName.LP)\n",
    "\n",
    "# Process LP table\n",
    "clean_lp = _process_table(TableName.LP)\n",
    "\n",
    "# Write results\n",
    "load_clean_data(TableName.LP_CLEAN, clean_lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243bb1c",
   "metadata": {},
   "source": [
    "Now to run our pattern mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a38b5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read COMBINED_CLEAN table from database!\n",
      "Email Token Encoder Initialised!\n",
      "Tokenised 5000 out of 92078\n",
      "Tokenised 10000 out of 92078\n",
      "Tokenised 15000 out of 92078\n",
      "Tokenised 20000 out of 92078\n",
      "Tokenised 25000 out of 92078\n",
      "Tokenised 30000 out of 92078\n",
      "Tokenised 35000 out of 92078\n",
      "Tokenised 40000 out of 92078\n",
      "Tokenised 45000 out of 92078\n",
      "Tokenised 50000 out of 92078\n",
      "Tokenised 55000 out of 92078\n",
      "Tokenised 60000 out of 92078\n",
      "Tokenised 65000 out of 92078\n",
      "Tokenised 70000 out of 92078\n",
      "Tokenised 75000 out of 92078\n",
      "Tokenised 80000 out of 92078\n",
      "Tokenised 85000 out of 92078\n",
      "Tokenised 90000 out of 92078\n",
      "Finished tokenizing: 83504 valid rows, 0 failures.\n",
      "8574 unknown sequences out of 92078\n",
      "297 unique templates!\n",
      ">/home/d_bowman/documents/IRP/irp-db1724/spmf.jar\n",
      "=============  TRULEGROWTH - STATS =============\n",
      "Sequential rules count: 12\n",
      "Total time : 2317 ms\n",
      "Max memory (mb)424.197998046875\n",
      "=====================================\n",
      "\n",
      "12 rules mined!\n",
      "[{'lhs_tokens': ['first_original_0'], 'rhs_tokens': ['.'], 'support': 33360, 'confidence': 0.7062410026251165}, {'lhs_tokens': ['first_original_0'], 'rhs_tokens': ['.', 'last_original_0'], 'support': 32805, 'confidence': 0.6944914895418749}, {'lhs_tokens': ['first_original_0'], 'rhs_tokens': ['last_original_0'], 'support': 35139, 'confidence': 0.7439029553730205}, {'lhs_tokens': ['first_original_0', '.'], 'rhs_tokens': ['last_original_0'], 'support': 32804, 'confidence': 0.9755256192940197}, {'lhs_tokens': ['first_original_0'], 'rhs_tokens': ['l_0'], 'support': 1247, 'confidence': 0.026399356423067153}, {'lhs_tokens': ['.'], 'rhs_tokens': ['last_original_0'], 'support': 34661, 'confidence': 0.9828725365092869}, {'lhs_tokens': ['.', 'f_0'], 'rhs_tokens': ['last_original_0'], 'support': 1531, 'confidence': 0.967150979153506}, {'lhs_tokens': ['f_0'], 'rhs_tokens': ['.'], 'support': 1546, 'confidence': 0.04435009610143722}, {'lhs_tokens': ['f_0'], 'rhs_tokens': ['.', 'last_original_0'], 'support': 1531, 'confidence': 0.04391979115866777}, {'lhs_tokens': ['last_original_0'], 'rhs_tokens': ['last_original_1'], 'support': 961, 'confidence': 0.013494916587092063}, {'lhs_tokens': ['f_0'], 'rhs_tokens': ['last_original_0'], 'support': 32814, 'confidence': 0.9413350928024327}, {'lhs_tokens': ['f_0'], 'rhs_tokens': ['l_0'], 'support': 980, 'confidence': 0.028113256260936918}]\n",
      "Also wrote 20844 rows to COMBINED_CLEAN\n"
     ]
    }
   ],
   "source": [
    "# Run pattern mining\n",
    "clean_data = _process_and_load_cleaned_data()\n",
    "\n",
    "# Fix a random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Take a random 25% sample of rows as the fullset will take too long to train\n",
    "clean_data = clean_data.sample(frac=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "# Replace in table\n",
    "load_clean_data(TableName.COMBINED_CLEAN, clean_data, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8695749",
   "metadata": {},
   "source": [
    "Now we will run a modest padding campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e389dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read COMBINED_CLEAN table from database!\n",
      "Read CANDIDATE_TEMPLATES table from database!\n",
      "Read FIRM_TEMPLATE_MAP table from database!\n",
      "Generated 35700 new investors!\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "clean_data = read_table(TableName.COMBINED_CLEAN)\n",
    "\n",
    "# Get data\n",
    "cand_temps = read_table(TableName.CANDIDATE_TEMPLATES)\n",
    "\n",
    "# Get firm template map\n",
    "firm_template_map = read_table(TableName.FIRM_TEMPLATE_MAP)\n",
    "\n",
    "# Find low investor firms\n",
    "low_investor_firms = firm_template_map[\n",
    "    firm_template_map[\"num_investors\"] < 5\n",
    "][\"firm\"].tolist()\n",
    "\n",
    "# Build firm profiles for each data set\n",
    "augmented = _build_profile_and_pad(\n",
    "    clean_data, firm_template_map, cand_temps, low_investor_firms, 5\n",
    ")\n",
    "print(f\"Generated {len(augmented) - len(clean_data)} new investors!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a522df",
   "metadata": {},
   "source": [
    "Next we will randomly select investors to duplicate and generate randomly padded set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba31475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 35700 random investors!\n"
     ]
    }
   ],
   "source": [
    "# Number of random investors to pad\n",
    "n_length_to_pad = len(augmented) - len(clean_data)\n",
    "\n",
    "# Randomly sample investors to duplicate\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "placebo_rows = clean_data.sample(\n",
    "    n=n_length_to_pad, \n",
    "    replace=True, \n",
    "    random_state=RANDOM_SEED\n",
    ").copy()\n",
    "\n",
    "# Randomly reassign firm from global pool to break any true structure signal from random assignment\n",
    "firms = clean_data[\"firm\"].unique()\n",
    "placebo_rows[\"firm\"] = rng.choice(firms, size=len(placebo_rows))\n",
    "\n",
    "# Combine with original clean_data\n",
    "random_augment = pd.concat([clean_data, placebo_rows], ignore_index=True)\n",
    "\n",
    "print(f\"Generated {len(random_augment) - len(clean_data)} random investors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529d666",
   "metadata": {},
   "source": [
    "Now we can generate our feature matrices. We will use the complex table for the randomly augmented so we don't have to define another table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1603383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 features built out of 56544\n",
      "5000 features built out of 56544\n",
      "10000 features built out of 56544\n",
      "15000 features built out of 56544\n",
      "20000 features built out of 56544\n",
      "25000 features built out of 56544\n",
      "30000 features built out of 56544\n",
      "35000 features built out of 56544\n",
      "40000 features built out of 56544\n",
      "45000 features built out of 56544\n",
      "50000 features built out of 56544\n",
      "55000 features built out of 56544\n",
      "0 features built out of 56544\n",
      "5000 features built out of 56544\n",
      "10000 features built out of 56544\n",
      "15000 features built out of 56544\n",
      "20000 features built out of 56544\n",
      "25000 features built out of 56544\n",
      "30000 features built out of 56544\n",
      "35000 features built out of 56544\n",
      "40000 features built out of 56544\n",
      "45000 features built out of 56544\n",
      "50000 features built out of 56544\n",
      "55000 features built out of 56544\n"
     ]
    }
   ],
   "source": [
    "# Save both to separate tables\n",
    "build_feature_matrix(\n",
    "    augmented,\n",
    "    cand_temps,\n",
    "    firm_template_map,\n",
    "    table=TableName.FEATURE_MATRIX,\n",
    ")\n",
    "build_feature_matrix(\n",
    "    random_augment,\n",
    "    cand_temps,\n",
    "    firm_template_map,\n",
    "    table=TableName.FEATURE_MATRIX_COMPLEX,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35e42f",
   "metadata": {},
   "source": [
    "Ok now we will define our splits. Let's start with a simple firm level split. We'll define a test set of 5000 investors. We will isolate those firms and derive a training/validation set from the remaining investors that are not in those firms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b19b9471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read COMBINED_CLEAN table from database!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d_bowman/miniconda3/envs/irp-db1724/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Load clean data\n",
    "clean_data = read_table(TableName.COMBINED_CLEAN)\n",
    "\n",
    "# Seed random\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "TEST_ROWS_N = 5000\n",
    "TRAIN_VAL_ROWS_N = 15000\n",
    "\n",
    "# Pick test set\n",
    "shuffled_firms = rng.permutation(clean_data[\"firm\"].unique())\n",
    "test_firms, test_rows = [], 0\n",
    "for f in shuffled_firms:\n",
    "    rows = clean_data[clean_data[\"firm\"] == f]\n",
    "    test_rows += len(rows)\n",
    "    test_firms.append(f)\n",
    "    if test_rows >= TEST_ROWS_N:\n",
    "        break\n",
    "\n",
    "# Remaining firms\n",
    "remaining_firms = [f for f in clean_data[\"firm\"].unique() if f not in test_firms]\n",
    "remaining_df = clean_data[clean_data[\"firm\"].isin(remaining_firms)]    \n",
    "\n",
    "# Sample train/val rows from remaining firms\n",
    "sampled = remaining_df.sample(n=TRAIN_VAL_ROWS_N, random_state=RANDOM_SEED)\n",
    "firm_held_train_ids, firm_held_val_ids = np.split(sampled[\"id\"].sample(frac=1, random_state=RANDOM_SEED), [int(.8*len(sampled))])\n",
    "\n",
    "# Build splits\n",
    "firm_held_test_ids = clean_data[clean_data[\"firm\"].isin(test_firms)][\"id\"]\n",
    "\n",
    "# Save to csv\n",
    "pd.DataFrame({\"train_ids\": firm_held_train_ids}).to_csv(\"splits/firm_held_train_ids.csv\", index=False)\n",
    "pd.DataFrame({\"val_ids\": firm_held_val_ids}).to_csv(\"splits/firm_held_val_ids.csv\", index=False)\n",
    "pd.DataFrame({\"test_ids\": firm_held_test_ids}).to_csv(\"splits/firm_held_test_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521c538",
   "metadata": {},
   "source": [
    "Next we will isolate a complex and standard test set. We will have 2000 of both in the test set and a healthy mix in the training and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6420362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = [\n",
    "    \"has_nfkd_normalized\",\n",
    "    \"has_multiple_last_names\",\n",
    "    \"has_german_char\",\n",
    "    \"has_nickname\",\n",
    "    \"has_multiple_first_names\",\n",
    "    \"has_middle_name\",\n",
    "]\n",
    "\n",
    "N_COMPLEX = 2000\n",
    "N_STANDARD = 2000\n",
    "\n",
    "# Build 1 row per investor with flags\n",
    "flag_view = clean_data[['id'] + FLAGS].drop_duplicates(subset=['id'])\n",
    "\n",
    "# Complex vs standard investors\n",
    "complex_mask = (flag_view[FLAGS].sum(axis=1) > 0)\n",
    "complex_ids_all  = flag_view.loc[complex_mask,  'id'].to_numpy()\n",
    "standard_ids_all = flag_view.loc[~complex_mask, 'id'].to_numpy()\n",
    "\n",
    "# Sample test sets\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "complex_sample  = rng.choice(complex_ids_all,  size=min(N_COMPLEX,  len(complex_ids_all)),  replace=False)\n",
    "standard_sample = rng.choice(standard_ids_all, size=min(N_STANDARD, len(standard_ids_all)), replace=False)\n",
    "test_ids = np.concatenate([complex_sample, standard_sample])\n",
    "\n",
    "# Build train/val from the remaining pool (80/20)\n",
    "pool = flag_view.loc[~flag_view['id'].isin(test_ids), 'id'].sample(frac=1.0, random_state=RANDOM_SEED)\n",
    "n_val = int(0.2 * len(pool))\n",
    "val_ids   = pool.iloc[:n_val].to_frame(name='val_ids')\n",
    "train_ids = pool.iloc[n_val:].to_frame(name='train_ids')\n",
    "\n",
    "# Save cultural bias TEST ids \n",
    "pd.DataFrame({'test_ids_complex': complex_sample}).to_csv('splits/test_ids_complex_only.csv', index=False)\n",
    "pd.DataFrame({'test_ids_standard': standard_sample}).to_csv('splits/test_ids_standard_only.csv', index=False)\n",
    "train_ids.to_csv('splits/train_ids_cult_bias.csv', index=False)\n",
    "val_ids.to_csv('splits/val_ids_cult_bias.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fdd6db",
   "metadata": {},
   "source": [
    "Finally we need a simple split for the placebo testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "077755e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = clean_data[\"id\"].astype(str).unique()\n",
    "\n",
    "# 70/15/15 split\n",
    "train_ids, test_ids = train_test_split(all_ids, test_size=0.15, random_state=42)\n",
    "train_ids, val_ids  = train_test_split(train_ids, test_size=0.1765, random_state=42)  \n",
    "\n",
    "# Save to CSVs\n",
    "pd.DataFrame({\"train_ids\": train_ids}).to_csv(\"splits/train_ids.csv\", index=False)\n",
    "pd.DataFrame({\"val_ids\": val_ids}).to_csv(\"splits/val_ids.csv\", index=False)\n",
    "pd.DataFrame({\"test_ids\": test_ids}).to_csv(\"splits/test_ids.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irp-db1724",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
